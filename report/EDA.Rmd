# EDA

## Data Acquisition

To download the different speaches, we scrap the speaches from two different websites.
The speaches from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson's speaches, they come from the official website of the governement of the United-Kingdom.

### Emmanuel Macron

We choose the 3 first speaches from Macron about the corona virus dating from the:
- 12 March (text1)
- 16 March (text2)
- 13 April (text3)

```{r }
# Data Acquisition Macron 

macron12march <- read_html("https://franceintheus.org/spip.php?article9654") %>%
  html_nodes("div.texte") %>%
  html_text()
macron12march <- str_replace_all(macron12march,"[\r\n\t]", ".")
macron12march <- substr(macron12march, 178, 20197)

macron16march <- read_html("https://franceintheus.org/spip.php?article9659#1") %>%
  html_nodes("div.texte") %>%
  html_text()
macron16march <- macron16march <- str_replace_all(macron16march,"[\r\n\t]", ".")
macron16march <- substr(macron16march, 131, 15719)

macron13april <- read_html("https://franceintheus.org/spip.php?article9710") %>%
  html_nodes("div.texte") %>%
  html_text() 
macron13april <- macron13april <- str_replace_all(macron13april,"[\r\n\t]", ".")
macron13april <- substr(macron13april, 117, 20000)


macron <- corpus(c(macron12march,macron16march,macron13april))
```

```{r, summary-macron-speeches}
kable(summary(macron), caption = "Macron's speeches characteristics" ,align = "lccrr",digits = 4)
```

The first speach of Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words.

### Boris Johnson

We choose the 7 first speaches of the Prime Minister Johnson about the corona virus dating from the:
- 09 March (text1)
- 12 March (text2)
- 16 March (text3)
- 18 March (text5)
- 19 March (text6)
- 20 March (text7)
- 22 March (text8)

```{r include=FALSE}

# Data Acquisition Boris

boris16mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-16-march-2020") %>%
                html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>% 
                html_text()
boris16mars <- str_replace_all(boris16mars,"[\r\n\t]", ".")

boris12mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-12-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris12mars <- str_replace_all(boris12mars,"[\r\n\t]", ".")


boris18mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-18-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris18mars <- str_replace_all(boris18mars,"[\r\n\t]", ".")

boris9mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-9-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris9mars <- str_replace_all(boris9mars,"[\r\n\t]", ".")


boris19mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-19-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris19mars <- str_replace_all(boris19mars,"[\r\n\t]", ".")


boris20mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-20-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris20mars <- str_replace_all(boris20mars,"[\r\n\t]", ".")



boris22mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-22-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris22mars <- str_replace_all(boris22mars,"[\r\n\t]", ".")


boris<-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars))
```

\@ref(tab:summary-boris-speeches)

```{r summary-boris-speeches}
kable(summary(boris), caption = "Johnson's speeches characteristics" ,align = "lccrr",digits = 4)
```

Johnson made more speeches but shorter. His first speech was 609 words, then the following ones ranged from 793 to 1222 words.

## Tokenisation, Lemmatization & Cleaning

Numbers, punctuation, symbols and separators are removed, as well as unimportant words. Moreover, we cast all letters to their corresponding lower case version.

We use lexicon to replace each token by its lemma.

Here you can see the kept words words for each speech. 

### Emmanuel Macron

```{r}

## Tokenization
corpus_macron <- corpus(macron)
corpus_macron <- tokens(corpus_macron, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

## Lemmatization
corpus_macron <- tokens_replace(corpus_macron, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
corpus_macron=corpus_macron %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) 

corpus_macron

```

### Boris Johnson

```{r}

## Tokenization
corpus_boris <- corpus(boris)
corpus_boris <- tokens(corpus_boris, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

## Lemmatization
corpus_boris <- tokens_replace(corpus_boris, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
corpus_boris = corpus_boris %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) 

corpus_boris

```

## Document-Term Matrix DTM

Now let's compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. 

### Table

*Emmanuel Macron*

```{r table-macrontidy}
## Document-Term Matrix DTM
corpus_macron.dfm <- dfm(corpus_macron)

macron_dtm <- VectorSource(corpus_macron) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
macron_tidy <- tidy(macron_dtm)

datatable(macron_tidy, class = "cell-border stripe")

```

*Boris Johnson*

```{r table-boristidy}

## Document-Term Matrix DTM
corpus_boris.dfm <- dfm(corpus_boris)

boris_dtm <- VectorSource(corpus_boris) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
boris_tidy <- tidy(boris_dtm)

datatable(boris_tidy, class = "cell-border stripe")
```

### Most frequent words

We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document.

#### All text confused

«will» is the word the most used for Macron and Johnson.

*Emmanuel Macron*
In the figure \@ref(fig:frequent-macron) 

```{r frequent-macron,fig.cap="15 most common words in Macron's speeches"}
#top 15 
macron_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`)) %>%
  top_n(15)  %>%
  ggplot(aes(x=count, y=term, fill = term)) +
  theme(legend.position = "none") +
  xlab("Frequency") + ylab("Words") +
  geom_bar(stat = "identity") +
  scale_y_reordered() 
```

```{r topppp-macron,fig.cap="Details of the more frequent words for Macron's speeches"}
#top 16 mots plus utilisés par texte
macron_count  = macron_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`))

macron_index = top_n(macron_count, 15)

macron_tidy %>% filter(term %in% macron_index$term) %>%
  ggplot(aes(x=term, y = count, fill =term)) +
  geom_col()+
  xlab("Words") + ylab("Frequency") +
  coord_flip()+
  facet_wrap(~document, ncol=2) + 
   guides(fill=FALSE, color=FALSE) 
```

*Boris Johnson*

```{r frequent-johnson, fig.cap="15 most common words in Johnson's speeches"}
#top 15
boris_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`)) %>%
  top_n(15)  %>%
  ggplot(aes(x=count, y=term, fill =term)) +
  theme(legend.position = "none") +
  xlab("Frequency") + ylab("Words") +
  geom_bar(stat = "identity") +
  scale_y_reordered() 

```

```{r topppp-Johnson, fig.cap="Details of the more frequent words for Johnson's speeches"}
#top 16 mots plus utilisés par texte
boris_count  = boris_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`))

boris_index = top_n(boris_count, 15)

boris_tidy %>% filter(term %in% boris_index$term) %>%
   ggplot(aes(x=term, y = count, fill=term)) +
  geom_col()+
  xlab("Words") + ylab("Frequency") +
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)
```

We see that the list of the 15 most frequent terms is due to doc 2 ,4 and 3. 

#### Per text 

Now we want to know which are the most frequent terms for each speach.

*Emmanuel Macron*

```{r top-Macron, fig.cap="15 most common words in each speach of Macron"}
#top 15 par texte 
macron_tidy %>%
  group_by(document) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  xlab("Words") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free")
```

*Boris Johnson*

```{r top-Johnson, fig.cap="15 most common words in each speach of Johnson"}
#top 15 par texte
boris_tidy %>%
  group_by(document) %>%
  top_n(15) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  xlab("Words") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  facet_wrap(~ document, scales = "free") 
```


## TF-IDF

Now we repeat the same analysis using the TF-IDF.

### Emmanuel Macron

```{r tfidf-Macron, fig.cap="Most specific word for each Emmanuel Macron's speeches"}
## TFIDF no point when just on document, maybe add when combining texts
corpus_macron.tfidf <- dfm_tfidf(corpus_macron.dfm)

#tfidf
macron_index_tfidf = tidy(corpus_macron.tfidf) %>% group_by(document) %>% top_n(1)

tidy(corpus_macron.tfidf) %>% filter(term %in% macron_index_tfidf$term) %>%
  ggplot( aes(term, count, fill=term)) +
  xlab("Words") + ylab("Frequency") +
  geom_col()+
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)

```

### Boris Johnson

```{r tfidf-Johnson, fig.cap="Most specific word for each Emmanuel Johnson's speeches"}
## TFIDF no point when just on document, maybe add when combining texts
corpus_boris.tfidf <- dfm_tfidf(corpus_boris.dfm)

#tfidf
boris_index_tfidf = tidy(corpus_boris.tfidf) %>% group_by(document) %>% top_n(1)

tidy(corpus_boris.tfidf) %>% filter(term %in% boris_index_tfidf$term) %>%
  ggplot( aes(term, count,fill=term)) +
  xlab("Words") + ylab("Frequency") +
  geom_col()+
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE) 
```

## Cloud of Words

It is another method to see the most used words. The larger the word, the more frequently they are used.

### Usind DFM

*Emmanuel macron*

```{r cloud-macron, fig.cap="Cloud od Words of Macron's speach with DFM"}
textplot_wordcloud(corpus_macron.dfm, color=brewer.pal(8, "Dark2")) 
```

*Boris Johnson*

```{r cloud-Johnson, fig.cap="Cloud od Words of Johnson's speach with DFM"}
textplot_wordcloud(corpus_boris.dfm, color=brewer.pal(8, "Dark2")) 
```

### Using  TF-IDF

The tfidf matrix helps us to see the frequency of the tokens independently of the legnght of the documents. We then observe that some tokens are more specific to some documents. For example we see that for Macron speeches, the token war is specific to his 2nd speech and the token test is specific to his 3rd speech. ( see \@ref(fig:tfidf-Macron) )

*Emmanuel macron*

```{r cloud-Macrontfidf, fig.cap="Cloud od Words of Johnson's speach with TI-IDF"}
textplot_wordcloud(corpus_macron.tfidf, color=brewer.pal(8, "Dark2"))
```

*Boris Johnson*

```{r cloud-Johnsontfidf, fig.cap="Cloud od Words of Johnson's speach with TI-IDF"}
textplot_wordcloud(corpus_boris.tfidf, color=brewer.pal(8, "Dark2")) 
```

## Lexical Divesity Token Type Ratio TTR


A TTR is comprised beetween 0 and 1. When equal to 1, it corresponds to a rich lexical diversity, this is to say that each token is from a different type. In opposite, if equal to 0, it mean that the corpus presents a poor lexical diversity (if he would use one word only).


### Emmanuel Macron 

```{r}
N.macron <- ntoken(corpus_macron)
V.macron <- ntype(corpus_macron)
TTR.macron <- V.macron/N.macron
kable(TTR.macron, caption = "Lexical diversity of Macron." ,align = "lccrr",digits = 4) ###the text is quite poor, as TTR is of 0.4

```

Macron has a mean TTR of 0,45, which is quite poor.

### Boris Johnson

```{r}
## Lexical Divesity Token Type Ratio TTR
N.boris <- ntoken(corpus_boris)
V.boris <- ntype(corpus_boris)
TTR.boris <- V.boris/N.boris
kable(TTR.boris, caption = "Lexical diversity of Johnson." ,align = "lccrr",digits = 4) ###the text is quite rich, as TTR is of 0.6
```

Johnson has quite a richer vocabulary, an average of 0,6 over the different corpuses.

## Zipf's Law

Now, we illustrate the Zipf's law on the discourses. The terms are ranked by their corresponding frequency (rank=1 for the most frequent), then plotted versus tehir rank. This is easily obtained using quanteda.

Using a log-log relation, this gives us a linear regression.

### Emmanuel Macron 

```{r zip-Macron, fig.cap="Zipf's law for Macron"}
corpus_macron_freq <- textstat_frequency(corpus_macron.dfm)


ggplot(corpus_macron_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) 
```

```{r log-Macron, fig.cap="log-log relation"}
plot(log(frequency)~log(rank), data=corpus_macron_freq, pch=20)  
```

### Boris Johnson

```{r zip-Johnson, fig.cap="Zipf's law for Johnson"}
corpus_boris_freq <- textstat_frequency(corpus_boris.dfm)

ggplot(corpus_boris_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20)
```

```{r log-Johnson, fig.cap="log-log relation"}
plot(log(frequency)~log(rank), data=corpus_boris_freq, pch=20)
```

## Yule's index

A larger index means more diversity.

### Emmanuel Macron 

```{r yules-Macron, fig.cap="Yule's index for Macron"}
textstat_lexdiv(corpus_macron.dfm, measure = "I") %>% 
  ggplot(aes(x=reorder(document,I), y=I))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("Yule's index") 
```

### Boris Johnson

```{r yules-Johnson, fig.cap="Yule's index for Johnson"}
textstat_lexdiv(corpus_boris.dfm, measure = "I") %>% 
  ggplot(aes(x=reorder(document,I), y=I))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("Yule's index") 
```


## MATTR

It is the Moving Average Type-Token Ratio. 

### Emmanuel Macron 


```{r mattr-macron, fig.cap="MATTR for Macron"}
textstat_lexdiv(corpus_macron, measure = "MATTR", MATTR_window = 10)  %>%
  ggplot(aes(x=reorder(document,MATTR), y=MATTR))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("MATTR") 
```

### Boris Johnson

```{r mattr-Johnson, fig.cap="MATTR for Johnson"}
textstat_lexdiv(corpus_boris, measure = "MATTR", MATTR_window = 10)  %>%
  ggplot(aes(x=reorder(document,MATTR), y=MATTR))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("MATTR") 
```
