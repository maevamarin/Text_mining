# Topic Modelling    

For this chapter, we analyzed the different topics used in the speeches. We used :
- the Latent Semantic Analysis, which is going to decompose the DTM Matrix into the document-topic similarity, the topic strength and the term-to-topic similarity
- and the Latent Dirichlet Allocation, that is a typical dimension reduction technique, which uses dirichlet priors for the document-topic and the word-topic distribution
We have first analyzed the speeches of Johnson and Macron separately and we have then combined them together.

## Boris Johnson

### LSA

First, we make the DTM matrix. We are goin to use 3 dimensions, it means 3 differents topics.

```{r,warning=FALSE}
bmod<-textmodel_lsa(corpus_boris.dfm,nd=3)
```

To inspect the results, we can extract the matrices involved in the LSA decomposition.
In the firs table, each components measures the link between the document and the topic.\@ref(tab:table-docs-boris)
In the second table, each component measure the link between the document and the term. \@ref(tab:table-features-boris)

LSA is typical a reduction technique. Instead of have N documents or M term, it is represented by K documents.

```{r table-docs-boris,warning=FALSE}
lsa_docs_boris<-head(bmod$docs)
lsa_docs_boris<-data.frame(lsa_docs_boris)

lsa_docs_boris%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")
```

```{r table-features-boris,warning=FALSE}
lsa_features_boris<-head(bmod$features)
lsa_features_boris<-data.frame(lsa_features_boris)

lsa_features_boris%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")
```

Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1.
As we observe in the figure \@ref(fig:lsaboris), the dimension 1 is negatively correlated with the document lenght.
Therefore the dimension 1 bring us not a lot of informations that we have already.

```{r lsaboris,fig.cap="First dimension of the LSA - Boris Johnson"}
ns<-apply(corpus_boris.dfm,1,sum) 
plot(ns~bmod$docs [,1])
```
We clearly observe that the dimension 1 is negatively correlated with the document lenght.

Now in order to make the link between the topics and the documents and the topics with term, we use biplot. We represent the dimension 2 and 3, beacause often the first component bring often little information. 

Reminders:

The seven speech are class by chronological order:
  - 09 March (text1)
  - 12 March (text2)
  - 16 March (text3)
  - 18 March (text5)
  - 19 March (text6)
  - 20 March (text7)
  - 22 March (text8)
  
It is  noticeable that the texts that are closer in time are grouped together. And that the first speeches go in the opposite direction of the one of the last speeches, as we observe in the figure \@ref(fig:biplotboris).

```{r biplotboris,fig.cap="Biplot - Boris Johnson"}
biplot(y=bmod$docs[,2:3],x=bmod$features[,2:3],
       col=c("grey","red"),
       xlab = "Dimension 2",
       ylab="Dimension 3")
```

We repeat the same analysis with TF-IDF. The influence of small weighted-frequent tokens is reduced.

In the firs table \@ref(tab:table-docs-boris-2), each components measures the link between the document and the topic.
In the second table \@ref(tab:table-features-boris-2) each component measure the link between the document and the term.


```{r table-docs-boris-2,warning=FALSE}
bmod_2<- textmodel_lsa(corpus_boris.tfidf, nd=3)

lsa_docs_boris_2<-head(bmod_2$docs)
lsa_docs_boris_2<-data.frame(lsa_docs_boris_2)

lsa_docs_boris_2%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")
```

```{r table-features-boris-2,warning=FALSE}

lsa_features_boris_2<-head(bmod_2$features)
lsa_features_boris_2<-data.frame(lsa_features_boris_2)

lsa_features_boris_2%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")
```

### LDA

We now turn to the LDA. For illustration, we will make K=3 topis. 
```{r,warning=FALSE}
K<-3
corpus_boris.dtm<- convert(corpus_boris.dfm, to="topicmodels")
lda_boris<- LDA(corpus_boris.dtm ,k=K)

```

In the table \@ref(tab:table-term-boris), it is the list of the six most frequent term in each topic
```{r table-term-boris,warning=FALSE}
terms<-terms(lda_boris,6)
terms<-data.frame(terms)

terms %>%
  kable(caption="List of the terms present in each topic") %>%
  kable_styling(bootstrap_options = "striped")

```

In the table \@ref(tab:table-topic-boris), you can observe which text is related to which topic.

```{r table-topic-boris,warning=FALSE}
## To see the topics related to each document

topics<-(topics(lda_boris,1))
topics<-data.frame(topics)

topics%>%
  kable(caption="Topics") %>%
  kable_styling(bootstrap_options = "striped")


```

We now build the bar plot to inspect the per-topic-per-word probabilities (beta's). We take the 10 top terms and rearrange the betas per topic according to this order. We observe in the figure \@ref(fig:betaboris) that topic 1 is correlated to the tokens "will", "go" and "want", topic 2 to the token "school" and topic 3 to the tokens "now", "can" and "will".

```{r betaboris,fig.cap="Beta - Boris Johnson"}
beta.td.boris<-tidy(lda_boris,matrix="beta")

beta.top.term.boris<-beta.td.boris %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


beta.top.term.boris %>%
  mutate(term=reorder_within(term, beta, topic)) %>%
  ggplot(aes(term,beta,fill=factor(topic))) +
   geom_col(show.legend = FALSE)+
  facet_wrap(~topic, scales = "free") +
  coord_flip()+
  scale_x_reordered()

```

Now, we compute the gamma, it shows the proportion of each topic within each document, as you can observe in the figure \@ref(fig:gammaboris). We see that the first speeches are related to topic 3 ("now", "can", "will"). This makes sense as those speeches were made in the begining of the pandemic and are linked to a sense of urgency. The speeches in the middle correspond to the second topic and all the measures implented for schools. The three last speeches are represented by the first topic only, and are correlated to the tokens "will", "go" and "want". We can conlude from this that the British Prime Minster wanted his citizens to keep up the efforts.


```{r gammaboris,fig.cap="Gamma - Boris Johnson"}
gamma.td.boris<- tidy(lda_boris,matrix="gamma")


gamma.td.boris %>%
  ggplot(aes(document,gamma,fill=factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic,scales = "free")+
  coord_flip()+
  scale_x_reordered()
```

## Macron

### LSA

```{r,warning=FALSE}
mmod<-textmodel_lsa(corpus_macron.dfm,nd=3)

```

To inspect the results, we can extract the matrices involved in the LSA decomposition


\@ref(tab:table-document-macron)
\@ref(tab:table-features-macron)

```{r table-document-macron,warning=FALSE}

lsa_docs_macron<-head(mmod$docs)
lsa_docs_macron<-data.frame(lsa_docs_macron)

lsa_docs_macron%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")

```

```{r table-features-macron,warning=FALSE}

lsa_features_macron<-head(mmod$features)
lsa_features_macron<-data.frame(lsa_features_macron)

lsa_features_macron%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")


```

Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1.

```{r lsamacron,fig.cap="First dimension of the LSA - Emmanuel Macron"}
ns_macron<-apply(corpus_macron.dfm,1,sum) 
plot(ns_macron~mmod$docs [,1])
```

We clearly observe that the dimension 1 is negatively correlated with the document lenght, as we observe in the figure \@ref(fig:lsamacron).

Now in order to make the link between the topics and the documents and the topics with term, we use biplot \@ref(fig:biplotmacron). We clearly observe in this figure that each text correspond to a specific topic, they don't do in the same direction.

```{r biplotmacron,fig.cap="Biplot - Emmanuel Macron"}
biplot(y=mmod$docs[,2:3],x=mmod$features[,2:3],
       col=c("grey","red"),
       xlab = "Dimension 2",
       ylab="Dimension 3")
```
We repeat the same analysis with TF-IDF, \@ref(tab:table-document-macron-2),\@ref(tab:table-term-macron-2)


```{r table-document-macron-2}
mmod_2<- textmodel_lsa(corpus_macron.tfidf, nd=3)

lsa_docs_macron_2<-head(mmod_2$docs)
lsa_docs_macron_2<-data.frame(lsa_docs_macron_2)

lsa_docs_macron_2%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")
```


```{r table-term-macron-2}
lsa_features_macron_2<-head(mmod_2$features)
lsa_features_macron_2<-data.frame(lsa_features_macron_2)

lsa_features_macron_2%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")
```


## Combine

We combine the speeches of both authors in the same dataframe, that will have a total of ten speeches.

```{r,warning=FALSE}
##Boris Johnson
boris_2<-as_tibble(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) %>%
  rename(
  text=value)
author="Boris Johnson"
boris_2<- cbind(boris_2, author)

##Emmanuel Macron
Macron_2<-as_tibble(c(macron12march,macron16march,macron13april)) %>% 
  rename(
    text = value)

author="Macron"
macron_2<- cbind(Macron_2, author)

##Combine the 2 dataframes
combine <- rbind(boris_2, macron_2)


## Tokenization
combine_corpus<-corpus(combine)
combine_tokens<- tokens(combine_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

##combi Lemmatization

combine_tokens <- tokens_replace(combine_tokens, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
combine_tokens = combine_tokens %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english"))
```

### LSA


```{r,warning=FALSE}
combine_corpus.dfm <- dfm(combine_tokens)
cmod<-textmodel_lsa(combine_corpus.dfm,nd=6)
```

Often the first dimension in the LSA is associated with the document length. To see if it is true, we build a scatter-plot between the document length and Dimension 1. We clearly observe that the three last documents are the longest ones and they all correspond to Macron's speeches.

```{r,warning=FALSE}
ns_combine<-apply(combine_corpus.dfm,1,sum) 
plot(ns_combine~cmod$docs [,1])
```

We then decide to proceed to the analysis using the second and third dimensions,figure \@ref(fig:biplot-combine), as we have observed that the first dimension is negatively correlated to the document length and does not therefore bring us a lot of information. We see that the speeches of Macron and Johnson are represented by different dimensions. Macron's speeches correspond better to the second dimension (documents 8, 9 and 10), whereas the third dimension is better represented by Johnson. This tells us that without building any further model and just by looking at the biplot corresponding to the second and third dimensions, we can guess whether a new corpus would belong to Johnson or to Macron.


```{r biplot-combine,warning=FALSE}
biplot(y=cmod$docs[,2:3],x=cmod$features[,2:3],
       col=c("grey","red"),
       xlab = "Dimension 2",
       ylab="Dimension 3")
```

