# EDA

## Data Acquisition

About the data collection: to download the different speeches, we scrap the speeches from two different websites. The one’s from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson’s ones, they come from the official website of the government of the United-Kingdom. We took 7 speeches for Johnson and 3 for Macron, dating from the 9th march to the 13th April.

### Emmanuel Macron

We choose the 3 first speaches from Macron about the corona virus dating from the:
- 12 March (text1)
- 16 March (text2)
- 13 April (text3)

```{r }
# Data Acquisition Macron 

macron12march <- read_html("https://franceintheus.org/spip.php?article9654") %>%
  html_nodes("div.texte") %>%
  html_text()
macron12march <- str_replace_all(macron12march,"[\r\n\t]", ".")
macron12march <- substr(macron12march, 178, 20197)

macron16march <- read_html("https://franceintheus.org/spip.php?article9659#1") %>%
  html_nodes("div.texte") %>%
  html_text()
macron16march <- macron16march <- str_replace_all(macron16march,"[\r\n\t]", ".")
macron16march <- substr(macron16march, 131, 15719)

macron13april <- read_html("https://franceintheus.org/spip.php?article9710") %>%
  html_nodes("div.texte") %>%
  html_text() 
macron13april <- macron13april <- str_replace_all(macron13april,"[\r\n\t]", ".")
macron13april <- substr(macron13april, 117, 20000)


macron <- corpus(c(macron12march,macron16march,macron13april))
```

```{r, summary-macron-speeches}
kable(summary(macron), caption = "Macron's speeches characteristics" ,align = "lccrr",digits = 4)
```

The first speach of Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words.

### Boris Johnson

We choose the 7 first speaches of the Prime Minister Johnson, table \@ref(tab:summary-boris-speeches), about the corona virus dating from the:
- 09 March (text1)
- 12 March (text2)
- 16 March (text3)
- 18 March (text5)
- 19 March (text6)
- 20 March (text7)
- 22 March (text8)

```{r include=FALSE}

# Data Acquisition Boris

boris16mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-16-march-2020") %>%
                html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>% 
                html_text()
boris16mars <- str_replace_all(boris16mars,"[\r\n\t]", ".")

boris12mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-12-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris12mars <- str_replace_all(boris12mars,"[\r\n\t]", ".")


boris18mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-18-march-2020") %>%   
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris18mars <- str_replace_all(boris18mars,"[\r\n\t]", ".")

boris9mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-9-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()

boris9mars <- str_replace_all(boris9mars,"[\r\n\t]", ".")


boris19mars<- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-19-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris19mars <- str_replace_all(boris19mars,"[\r\n\t]", ".")


boris20mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-20-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris20mars <- str_replace_all(boris20mars,"[\r\n\t]", ".")



boris22mars <- read_html("https://www.gov.uk/government/speeches/pm-statement-on-coronavirus-22-march-2020") %>% 
              html_nodes(xpath="//*[@id='content']/div[3]/div[1]/div[1]/div[2]/div") %>%
              html_text()
boris22mars <- str_replace_all(boris22mars,"[\r\n\t]", ".")


boris<-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars))
```



```{r summary-boris-speeches}
kable(summary(boris), caption = "Johnson's speeches characteristics" ,align = "lccrr",digits = 4)
```

Johnson made more speeches but shorter. His first speech was 609 words, then the following ones ranged from 793 to 1222 words.

## Tokenisation, Lemmatization & Cleaning

Numbers, punctuation, symbols and separators are removed, as well as unimportant words. Moreover, we cast all letters to their corresponding lower case version.

We use lexicon to replace each token by its lemma.

Here you can see the kept words words for each speech. 

### Emmanuel Macron

```{r}

## Tokenization
corpus_macron <- corpus(macron)
corpus_macron <- tokens(corpus_macron, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

## Lemmatization
corpus_macron <- tokens_replace(corpus_macron, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
corpus_macron=corpus_macron %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) 

corpus_macron

```

### Boris Johnson

```{r}

## Tokenization
corpus_boris <- corpus(boris)
corpus_boris <- tokens(corpus_boris, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

## Lemmatization
corpus_boris <- tokens_replace(corpus_boris, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
corpus_boris = corpus_boris %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english")) 

corpus_boris

```

## Document-Term Matrix DTM

Now let's compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. 

### Table

*Emmanuel Macron*

```{r table-macrontidy}
## Document-Term Matrix DTM
corpus_macron.dfm <- dfm(corpus_macron)

macron_dtm <- VectorSource(corpus_macron) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
macron_tidy <- tidy(macron_dtm)

datatable(macron_tidy, class = "cell-border stripe")

```

*Boris Johnson*

```{r table-boristidy}

## Document-Term Matrix DTM
corpus_boris.dfm <- dfm(corpus_boris)

boris_dtm <- VectorSource(corpus_boris) %>% VCorpus() %>%  DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE))
boris_tidy <- tidy(boris_dtm)

datatable(boris_tidy, class = "cell-border stripe")
```

### Most frequent words

#### All text confused

We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document.

«will» is the word the most used for Macron and Johnson.

*Emmanuel Macron*

```{r topppp-macron,fig.cap="Details of the more frequent words for Macron's speeches"}
#top 16 mots plus utilisés par texte
macron_count  = macron_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`))

macron_index = top_n(macron_count, 15)

macron_tidy %>% filter(term %in% macron_index$term) %>%
  ggplot(aes(x=term, y = count, fill =term)) +
  geom_col()+
  xlab("Words") + ylab("Frequency") +
  coord_flip()+
  facet_wrap(~document, ncol=2) + 
   guides(fill=FALSE, color=FALSE) 
```

*Boris Johnson*


```{r topppp-Johnson, fig.cap="Details of the more frequent words for Johnson's speeches"}
#top 16 mots plus utilisés par texte
boris_count  = boris_tidy %>%
  group_by(term) %>%
  summarise("count"=sum(`count`))

boris_index = top_n(boris_count, 15)

boris_tidy %>% filter(term %in% boris_index$term) %>%
   ggplot(aes(x=term, y = count, fill=term)) +
  geom_col()+
  xlab("Words") + ylab("Frequency") +
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)
```

We see that the list of the 15 most frequent terms is due to doc 2 ,4 and 3. 

#### Per text 

Now we want to know which are the most frequent terms for each speach. The 5 more present words in each speech.


*Emmanuel Macron*

```{r top-Macron, fig.cap="15 most common words in each speach of Macron"}
#top 5 per text 
macron_tidy %>%
  group_by(document) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  xlab("Words") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free")
```

*Boris Johnson*

```{r top-Johnson, fig.cap="15 most common words in each speach of Johnson"}
#top 5 per text
boris_tidy %>%
  group_by(document) %>%
  top_n(5) %>%
  ungroup() %>%
  mutate(document = factor(as.numeric(document), levels = 1:17)) %>%
  ggplot(aes(reorder_within(term, count, document), count, fill =term)) +
  theme(legend.position = "none") +
  xlab("Words") + ylab("Frequency") +
  geom_bar(stat = "identity") +
  scale_x_reordered() +
  coord_flip() +
  facet_wrap(~ document, scales = "free") 
```


## TF-IDF

Now we repeat the same analysis using the TF-IDF.

The tfidf matrix helps us to see the frequency of the tokens independently of the legnght of the documents. We then observe that some tokens are more specific to some documents. 

### Emmanuel Macron

Using TF-IDF, we see the most specific word of each speech.Regarding Macron`s speeches, figure \@ref(fig:tfidf-Macron), the first one had the word «count» for a specificity, it can be explained because it was the beginning of the corona and the number of patient was the main subject. For the second text it was the word «war», it‘s the famous speech where macron compared the corona virus crisis to a war against an invisible enemy. For it last speech, it’s the word “test”, dating of April and can be explained by the fact that he responds to the debate about the necessity of testing people or not.

```{r tfidf-Macron, fig.cap="Most specific word for each Emmanuel Macron's speeches"}
## TFIDF no point when just on document, maybe add when combining texts
corpus_macron.tfidf <- dfm_tfidf(corpus_macron.dfm)

#tfidf
macron_index_tfidf = tidy(corpus_macron.tfidf) %>% group_by(document) %>% top_n(1)

tidy(corpus_macron.tfidf) %>% filter(term %in% macron_index_tfidf$term) %>%
  ggplot( aes(term, count, fill=term)) +
  xlab("Words") + ylab("Frequency") +
  geom_col()+
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE)

```

### Boris Johnson

For Johnson,figure \@ref(fig:tfidf-Johnson), the word “outbreak” was specific for the 1st speech. The word “dangerous” for the second speech. The two first speeches seems to be more informative.Than the word “progress” and “school” for the following speeches, it was to announced that the number of case increased and that that measures were implement.The last next is characterized by the word “Jenrick” and “Robert”; it makes reference to Robert Jenrick a political man who explained the new protection measures.

```{r tfidf-Johnson, fig.cap="Most specific word for each Emmanuel Johnson's speeches"}
## TFIDF no point when just on document, maybe add when combining texts
corpus_boris.tfidf <- dfm_tfidf(corpus_boris.dfm)

#tfidf
boris_index_tfidf = tidy(corpus_boris.tfidf) %>% group_by(document) %>% top_n(1)

tidy(corpus_boris.tfidf) %>% filter(term %in% boris_index_tfidf$term) %>%
  ggplot( aes(term, count,fill=term)) +
  xlab("Words") + ylab("Frequency") +
  geom_col()+
  coord_flip()+
  facet_wrap(~document, ncol=2)+ 
   guides(fill=FALSE, color=FALSE) 
```

## Cloud of Words


It is another method to see the most used words. The larger the word, the more frequently they are used.

### Usind DFM

*Emmanuel macron*

```{r cloud-macron, fig.cap="Cloud od Words of Macron's speach with DFM"}
textplot_wordcloud(corpus_macron.dfm, color=brewer.pal(8, "Dark2")) 
```

*Boris Johnson*

```{r cloud-Johnson, fig.cap="Cloud od Words of Johnson's speach with DFM"}
textplot_wordcloud(corpus_boris.dfm, color=brewer.pal(8, "Dark2")) 
```

### Using  TF-IDF

We made a word of cloud to see other specific words containing in each text.

Child, public, contact… other words which can be very specific to a subject. 
The specific vocabulary for Johnson, as we see in the figure \@ref(fig:cloud-Johnson), it seems to be lighter than Macron’s one,\@ref(fig:cloud-macron) . School, child against war, count, test…

*Emmanuel macron*

```{r cloud-Macrontfidf, fig.cap="Cloud od Words of Johnson's speach with TI-IDF"}
textplot_wordcloud(corpus_macron.tfidf, color=brewer.pal(8, "Dark2"))
```

*Boris Johnson*

```{r cloud-Johnsontfidf, fig.cap="Cloud od Words of Johnson's speach with TI-IDF"}
textplot_wordcloud(corpus_boris.tfidf, color=brewer.pal(8, "Dark2")) 
```


Thanks to the TF-IDF , without reading all the text we can have a clear idea about the speeches and see a relation between the advance in time and the specific word.

## Lexical Divesity Token Type Ratio TTR


A TTR is comprised beetween 0 and 1. When equal to 1, it corresponds to a rich lexical diversity, this is to say that each token is from a different type. In opposite, if equal to 0, it mean that the corpus presents a poor lexical diversity (if he would use one word only).


### Emmanuel Macron 

```{r}
N.macron <- ntoken(corpus_macron)
V.macron <- ntype(corpus_macron)
TTR.macron <- V.macron/N.macron
kable(TTR.macron, caption = "Lexical diversity of Macron." ,align = "lccrr",digits = 4) ###the text is quite poor, as TTR is of 0.4

```

Macron has a mean TTR of 0,45, which is quite poor.

### Boris Johnson

```{r}
## Lexical Divesity Token Type Ratio TTR
N.boris <- ntoken(corpus_boris)
V.boris <- ntype(corpus_boris)
TTR.boris <- V.boris/N.boris
kable(TTR.boris, caption = "Lexical diversity of Johnson." ,align = "lccrr",digits = 4) ###the text is quite rich, as TTR is of 0.6
```

Johnson has quite a richer vocabulary, an average of 0,6 over the different corpuses.

## Zipf's Law

Now, we illustrate the Zipf's law on the discourses. The terms are ranked by their corresponding frequency (rank=1 for the most frequent), then plotted versus tehir rank. This is easily obtained using quanteda.

Using a log-log relation, this gives us a linear regression.

### Emmanuel Macron 

```{r zip-Macron, fig.cap="Zipf's law for Macron"}
corpus_macron_freq <- textstat_frequency(corpus_macron.dfm)


ggplot(corpus_macron_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) 
```

```{r log-Macron, fig.cap="log-log relation"}
plot(log(frequency)~log(rank), data=corpus_macron_freq, pch=20)  
```

### Boris Johnson

```{r zip-Johnson, fig.cap="Zipf's law for Johnson"}
corpus_boris_freq <- textstat_frequency(corpus_boris.dfm)

ggplot(corpus_boris_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20)
```

```{r log-Johnson, fig.cap="log-log relation"}
plot(log(frequency)~log(rank), data=corpus_boris_freq, pch=20)
```

## Yule's index

A larger index means more diversity.

Regarding the yule’s index, which is a speech diversity index. For macron speeches \@ref(fig:yules-Macron), more it is long, less it is diverse. This is quite normal because longer is a text, more is it possible to have a repetition in term of type of tokens. It is not the case for Johnson as we can observe in the figure \@ref(fig:yules-Johnson). The first speech of Johnson is the shortest and the most diverse one but after it doesn’t follow any patterns. 

### Emmanuel Macron 

```{r yules-Macron, fig.cap="Yule's index for Macron"}
textstat_lexdiv(corpus_macron.dfm, measure = "I") %>% 
  ggplot(aes(x=reorder(document,I), y=I))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("Yule's index") 
```

### Boris Johnson

```{r yules-Johnson, fig.cap="Yule's index for Johnson"}
textstat_lexdiv(corpus_boris.dfm, measure = "I") %>% 
  ggplot(aes(x=reorder(document,I), y=I))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("Yule's index") 
```


## MATTR

It is the Moving Average Type-Token Ratio. MATTR is less dependent on the lenght of the document.

The rank of the speeches completely change comparing to the Yule's index. 
Speeches which had a rich vocabulary diversification seems to had less vocabulary diversification with a windows of 10 words. 

### Emmanuel Macron 


```{r mattr-macron, fig.cap="MATTR for Macron"}
textstat_lexdiv(corpus_macron, measure = "MATTR", MATTR_window = 10)  %>%
  ggplot(aes(x=reorder(document,MATTR), y=MATTR))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("MATTR") 
```

### Boris Johnson

```{r mattr-Johnson, fig.cap="MATTR for Johnson"}
textstat_lexdiv(corpus_boris, measure = "MATTR", MATTR_window = 10)  %>%
  ggplot(aes(x=reorder(document,MATTR), y=MATTR))+
  geom_point()+
  coord_flip()+
  xlab("Text")+
  ylab("MATTR") 
```
