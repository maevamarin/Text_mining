[
["introduction.html", "Dicours Analysis Chapter 1 Introduction 1.1 Overview and Motivation", " Dicours Analysis Eugénie Mathieu, Maeva Marin, Hadrien Renger, Wajma Nazim 17 décembre, 2020 Chapter 1 Introduction 1.1 Overview and Motivation Since the pandemic, the politicians’ adresses to their citizens have multiplied and our goal was to run a comparative analysis based on the speeches of the UK Prime Minister Boris Johnson and the French President Emmanuel Macron, translated in English. Knowing that politician discourses are always cautiously constructed to have the expected impact of the population, we assumed some specificities and discriminatory features among their speeches. Here, we would neglect the effect of a potential poor translation. Our motivation was to find significant differences in the sentence structure and in sentiments conveyed to confirm our basis hypothesis and to construct a classifier of speeches via a supervised learner. The aim of our project in to analyse the discourses of politician Boris Johnson and Emmanuel Macron and to see if there is any major differences in terms of the vocabulary used, the lengths of the speeches and the sentiments related to the various speeches. We proceeded in the following order, we first cleaned the data, using tokenisation and cleaning methods, we then tried to find statistical pattern using frequencies and scoring models and we then tried to interpret the results using a sentiment analysis. We need to highlight here that we used several methods, such as dimension reduction techniques and machine learning algorithms. "],
["eda.html", "Chapter 2 EDA 2.1 Data Acquisition 2.2 Tokenisation, Lemmatization &amp; Cleaning 2.3 Document-Term Matrix DTM 2.4 TF-IDF 2.5 Cloud of Words 2.6 Lexical Divesity Token Type Ratio TTR 2.7 Zipf’s Law 2.8 Yule’s index 2.9 MATTR", " Chapter 2 EDA 2.1 Data Acquisition About the data collection: to download the different speeches, we scrap the speeches from two different websites. The one’s from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson’s ones, they come from the official website of the government of the United-Kingdom. We took 7 speeches for Johnson and 3 for Macron, dating from the 9th march to the 13th April. 2.1.1 Emmanuel Macron We choose the 3 first speaches from Macron about the corona virus dating from the: - 12 March (text1) - 16 March (text2) - 13 April (text3) # Data Acquisition Macron macron12march &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9654&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() macron12march &lt;- str_replace_all(macron12march,&quot;[\\r\\n\\t]&quot;, &quot;.&quot;) macron12march &lt;- substr(macron12march, 178, 20197) macron16march &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9659#1&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() macron16march &lt;- macron16march &lt;- str_replace_all(macron16march,&quot;[\\r\\n\\t]&quot;, &quot;.&quot;) macron16march &lt;- substr(macron16march, 131, 15719) macron13april &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9710&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() macron13april &lt;- macron13april &lt;- str_replace_all(macron13april,&quot;[\\r\\n\\t]&quot;, &quot;.&quot;) macron13april &lt;- substr(macron13april, 117, 20000) macron &lt;- corpus(c(macron12march,macron16march,macron13april)) kable(summary(macron), caption = &quot;Macron&#39;s speeches characteristics&quot; ,align = &quot;lccrr&quot;,digits = 4) Table 2.1: Macron’s speeches characteristics Text Types Tokens Sentences text1 988 3849 190 text2 909 3030 137 text3 1080 3945 184 The first speach of Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words. 2.1.2 Boris Johnson We choose the 7 first speaches of the Prime Minister Johnson, table 2.2, about the corona virus dating from the: - 09 March (text1) - 12 March (text2) - 16 March (text3) - 18 March (text5) - 19 March (text6) - 20 March (text7) - 22 March (text8) kable(summary(boris), caption = &quot;Johnson&#39;s speeches characteristics&quot; ,align = &quot;lccrr&quot;,digits = 4) Table 2.2: Johnson’s speeches characteristics Text Types Tokens Sentences text1 266 609 23 text2 409 1222 50 text3 405 1231 43 text4 406 1230 52 text5 321 994 37 text6 357 1030 47 text7 300 793 35 Johnson made more speeches but shorter. His first speech was 609 words, then the following ones ranged from 793 to 1222 words. 2.2 Tokenisation, Lemmatization &amp; Cleaning Numbers, punctuation, symbols and separators are removed, as well as unimportant words. Moreover, we cast all letters to their corresponding lower case version. We use lexicon to replace each token by its lemma. Here you can see the kept words words for each speech. 2.2.1 Emmanuel Macron ## Tokenization corpus_macron &lt;- corpus(macron) corpus_macron &lt;- tokens(corpus_macron, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ## Lemmatization corpus_macron &lt;- tokens_replace(corpus_macron, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning corpus_macron=corpus_macron %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) corpus_macron #&gt; Tokens consisting of 3 documents. #&gt; text1 : #&gt; [1] &quot;check&quot; &quot;delivery&quot; &quot;man&quot; &quot;woman&quot; &quot;france&quot; #&gt; [6] &quot;dear&quot; &quot;compatriot&quot; &quot;past&quot; &quot;week&quot; &quot;country&quot; #&gt; [11] &quot;confront&quot; &quot;spread&quot; #&gt; [ ... and 1,714 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;paris&quot; &quot;march&quot; &quot;woman&quot; &quot;man&quot; &quot;france&quot; #&gt; [6] &quot;thursday&quot; &quot;night&quot; &quot;speak&quot; &quot;health&quot; &quot;crisis&quot; #&gt; [11] &quot;country&quot; &quot;confront&quot; #&gt; [ ... and 1,312 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;paris&quot; &quot;april&quot; &quot;frenchwoman&quot; &quot;frenchman&quot; #&gt; [5] &quot;dear&quot; &quot;compatriot&quot; &quot;live&quot; &quot;difficult&quot; #&gt; [9] &quot;time&quot; &quot;feel&quot; &quot;fear&quot; &quot;distress&quot; #&gt; [ ... and 1,690 more ] 2.2.2 Boris Johnson ## Tokenization corpus_boris &lt;- corpus(boris) corpus_boris &lt;- tokens(corpus_boris, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ## Lemmatization corpus_boris &lt;- tokens_replace(corpus_boris, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning corpus_boris = corpus_boris %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) corpus_boris #&gt; Tokens consisting of 7 documents. #&gt; text1 : #&gt; [1] &quot;morning&quot; &quot;chair&quot; &quot;meet&quot; &quot;government&#39;s&quot; #&gt; [5] &quot;cobr&quot; &quot;emergency&quot; &quot;committee&quot; &quot;coronavirus&quot; #&gt; [9] &quot;outbreak&quot; &quot;first&quot; &quot;minister&quot; &quot;scotland&quot; #&gt; [ ... and 246 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; #&gt; [5] &quot;much&quot; &quot;come&quot; &quot;just&quot; &quot;chair&quot; #&gt; [9] &quot;meet&quot; &quot;government&#39;s&quot; &quot;emergency&quot; &quot;committee&quot; #&gt; [ ... and 501 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; &quot;much&quot; #&gt; [6] &quot;come&quot; &quot;want&quot; &quot;bring&quot; &quot;everyone&quot; &quot;date&quot; #&gt; [11] &quot;national&quot; &quot;fight&quot; #&gt; [ ... and 477 more ] #&gt; #&gt; text4 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;indeed&quot; #&gt; [6] &quot;tune&quot; &quot;daily&quot; &quot;update&quot; &quot;want&quot; &quot;introduce&quot; #&gt; [11] &quot;sure&quot; &quot;know&quot; #&gt; [ ... and 532 more ] #&gt; #&gt; text5 : #&gt; [1] &quot;want&quot; &quot;begin&quot; &quot;thank&quot; &quot;everyone&quot; &quot;thank&quot; #&gt; [6] &quot;medium&quot; &quot;also&quot; &quot;thank&quot; &quot;everyone&quot; &quot;huge&quot; #&gt; [11] &quot;effort&quot; &quot;country&quot; #&gt; [ ... and 356 more ] #&gt; #&gt; text6 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;today&quot; #&gt; [6] &quot;join&quot; &quot;chancellor&quot; &quot;exchequer&quot; &quot;rishi&quot; &quot;sunak&quot; #&gt; [11] &quot;jennie&quot; &quot;harry&quot; #&gt; [ ... and 392 more ] #&gt; #&gt; [ reached max_ndoc ... 1 more document ] 2.3 Document-Term Matrix DTM Now let’s compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. 2.3.1 Table Emmanuel Macron ## Document-Term Matrix DTM corpus_macron.dfm &lt;- dfm(corpus_macron) macron_dtm &lt;- VectorSource(corpus_macron) %&gt;% VCorpus() %&gt;% DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE)) macron_tidy &lt;- tidy(macron_dtm) datatable(macron_tidy, class = &quot;cell-border stripe&quot;) Boris Johnson ## Document-Term Matrix DTM corpus_boris.dfm &lt;- dfm(corpus_boris) boris_dtm &lt;- VectorSource(corpus_boris) %&gt;% VCorpus() %&gt;% DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE)) boris_tidy &lt;- tidy(boris_dtm) datatable(boris_tidy, class = &quot;cell-border stripe&quot;) 2.3.2 Most frequent words 2.3.2.1 All text confused We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document. «will» is the word the most used for Macron and Johnson. Emmanuel Macron #top 16 mots plus utilisés par texte macron_count = macron_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) macron_index = top_n(macron_count, 15) macron_tidy %&gt;% filter(term %in% macron_index$term) %&gt;% ggplot(aes(x=term, y = count, fill =term)) + geom_col()+ xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + coord_flip()+ facet_wrap(~document, ncol=2) + guides(fill=FALSE, color=FALSE) Figure 2.1: Details of the more frequent words for Macron’s speeches Boris Johnson #top 16 mots plus utilisés par texte boris_count = boris_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) boris_index = top_n(boris_count, 15) boris_tidy %&gt;% filter(term %in% boris_index$term) %&gt;% ggplot(aes(x=term, y = count, fill=term)) + geom_col()+ xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + coord_flip()+ facet_wrap(~document, ncol=2)+ guides(fill=FALSE, color=FALSE) Figure 2.2: Details of the more frequent words for Johnson’s speeches We see that the list of the 15 most frequent terms is due to doc 2 ,4 and 3. 2.3.2.2 Per text Now we want to know which are the most frequent terms for each speach. The 5 more present words in each speech. Emmanuel Macron #top 5 per text macron_tidy %&gt;% group_by(document) %&gt;% top_n(5) %&gt;% ungroup() %&gt;% mutate(document = factor(as.numeric(document), levels = 1:17)) %&gt;% ggplot(aes(reorder_within(term, count, document), count, fill =term)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + coord_flip() + facet_wrap(~ document, scales = &quot;free&quot;) Figure 2.3: 15 most common words in each speach of Macron Boris Johnson #top 5 per text boris_tidy %&gt;% group_by(document) %&gt;% top_n(5) %&gt;% ungroup() %&gt;% mutate(document = factor(as.numeric(document), levels = 1:17)) %&gt;% ggplot(aes(reorder_within(term, count, document), count, fill =term)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + coord_flip() + facet_wrap(~ document, scales = &quot;free&quot;) Figure 2.4: 15 most common words in each speach of Johnson 2.4 TF-IDF Now we repeat the same analysis using the TF-IDF. The tfidf matrix helps us to see the frequency of the tokens independently of the legnght of the documents. We then observe that some tokens are more specific to some documents. 2.4.1 Emmanuel Macron Using TF-IDF, we see the most specific word of each speech.Regarding Macron`s speeches, figure 2.5, the first one had the word «count» for a specificity, it can be explained because it was the beginning of the corona and the number of patient was the main subject. For the second text it was the word «war», it‘s the famous speech where macron compared the corona virus crisis to a war against an invisible enemy. For it last speech, it’s the word “test”, dating of April and can be explained by the fact that he responds to the debate about the necessity of testing people or not. ## TFIDF no point when just on document, maybe add when combining texts corpus_macron.tfidf &lt;- dfm_tfidf(corpus_macron.dfm) #tfidf macron_index_tfidf = tidy(corpus_macron.tfidf) %&gt;% group_by(document) %&gt;% top_n(1) tidy(corpus_macron.tfidf) %&gt;% filter(term %in% macron_index_tfidf$term) %&gt;% ggplot( aes(term, count, fill=term)) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2)+ guides(fill=FALSE, color=FALSE) Figure 2.5: Most specific word for each Emmanuel Macron’s speeches 2.4.2 Boris Johnson For Johnson,figure 2.6, the word “outbreak” was specific for the 1st speech. The word “dangerous” for the second speech. The two first speeches seems to be more informative.Than the word “progress” and “school” for the following speeches, it was to announced that the number of case increased and that that measures were implement.The last next is characterized by the word “Jenrick” and “Robert”; it makes reference to Robert Jenrick a political man who explained the new protection measures. ## TFIDF no point when just on document, maybe add when combining texts corpus_boris.tfidf &lt;- dfm_tfidf(corpus_boris.dfm) #tfidf boris_index_tfidf = tidy(corpus_boris.tfidf) %&gt;% group_by(document) %&gt;% top_n(1) tidy(corpus_boris.tfidf) %&gt;% filter(term %in% boris_index_tfidf$term) %&gt;% ggplot( aes(term, count,fill=term)) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2)+ guides(fill=FALSE, color=FALSE) Figure 2.6: Most specific word for each Emmanuel Johnson’s speeches 2.5 Cloud of Words It is another method to see the most used words. The larger the word, the more frequently they are used. 2.5.1 Usind DFM Emmanuel macron textplot_wordcloud(corpus_macron.dfm, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 2.7: Cloud od Words of Macron’s speach with DFM Boris Johnson textplot_wordcloud(corpus_boris.dfm, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 2.8: Cloud od Words of Johnson’s speach with DFM 2.5.2 Using TF-IDF We made a word of cloud to see other specific words containing in each text. Child, public, contact… other words which can be very specific to a subject. The specific vocabulary for Johnson, as we see in the figure 2.8, it seems to be lighter than Macron’s one,2.7 . School, child against war, count, test… Emmanuel macron textplot_wordcloud(corpus_macron.tfidf, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 2.9: Cloud od Words of Johnson’s speach with TI-IDF Boris Johnson textplot_wordcloud(corpus_boris.tfidf, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 2.10: Cloud od Words of Johnson’s speach with TI-IDF Thanks to the TF-IDF , without reading all the text we can have a clear idea about the speeches and see a relation between the advance in time and the specific word. 2.6 Lexical Divesity Token Type Ratio TTR A TTR is comprised beetween 0 and 1. When equal to 1, it corresponds to a rich lexical diversity, this is to say that each token is from a different type. In opposite, if equal to 0, it mean that the corpus presents a poor lexical diversity (if he would use one word only). 2.6.1 Emmanuel Macron N.macron &lt;- ntoken(corpus_macron) V.macron &lt;- ntype(corpus_macron) TTR.macron &lt;- V.macron/N.macron kable(TTR.macron, caption = &quot;Lexical diversity of Macron.&quot; ,align = &quot;lccrr&quot;,digits = 4) ###the text is quite poor, as TTR is of 0.4 Table 2.3: Lexical diversity of Macron. x text1 0.402 text2 0.480 text3 0.463 Macron has a mean TTR of 0,45, which is quite poor. 2.6.2 Boris Johnson ## Lexical Divesity Token Type Ratio TTR N.boris &lt;- ntoken(corpus_boris) V.boris &lt;- ntype(corpus_boris) TTR.boris &lt;- V.boris/N.boris kable(TTR.boris, caption = &quot;Lexical diversity of Johnson.&quot; ,align = &quot;lccrr&quot;,digits = 4) ###the text is quite rich, as TTR is of 0.6 Table 2.4: Lexical diversity of Johnson. x text1 0.674 text2 0.548 text3 0.558 text4 0.509 text5 0.576 text6 0.611 text7 0.611 Johnson has quite a richer vocabulary, an average of 0,6 over the different corpuses. 2.7 Zipf’s Law Now, we illustrate the Zipf’s law on the discourses. The terms are ranked by their corresponding frequency (rank=1 for the most frequent), then plotted versus tehir rank. This is easily obtained using quanteda. Using a log-log relation, this gives us a linear regression. 2.7.1 Emmanuel Macron corpus_macron_freq &lt;- textstat_frequency(corpus_macron.dfm) ggplot(corpus_macron_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) Figure 2.11: Zipf’s law for Macron plot(log(frequency)~log(rank), data=corpus_macron_freq, pch=20) Figure 2.12: log-log relation 2.7.2 Boris Johnson corpus_boris_freq &lt;- textstat_frequency(corpus_boris.dfm) ggplot(corpus_boris_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) Figure 2.13: Zipf’s law for Johnson plot(log(frequency)~log(rank), data=corpus_boris_freq, pch=20) Figure 2.14: log-log relation 2.8 Yule’s index A larger index means more diversity. Regarding the yule’s index, which is a speech diversity index. For macron speeches 2.15, more it is long, less it is diverse. This is quite normal because longer is a text, more is it possible to have a repetition in term of type of tokens. It is not the case for Johnson as we can observe in the figure 2.16. The first speech of Johnson is the shortest and the most diverse one but after it doesn’t follow any patterns. 2.8.1 Emmanuel Macron textstat_lexdiv(corpus_macron.dfm, measure = &quot;I&quot;) %&gt;% ggplot(aes(x=reorder(document,I), y=I))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;Yule&#39;s index&quot;) Figure 2.15: Yule’s index for Macron 2.8.2 Boris Johnson textstat_lexdiv(corpus_boris.dfm, measure = &quot;I&quot;) %&gt;% ggplot(aes(x=reorder(document,I), y=I))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;Yule&#39;s index&quot;) Figure 2.16: Yule’s index for Johnson 2.9 MATTR It is the Moving Average Type-Token Ratio. MATTR is less dependent on the lenght of the document. The rank of the speeches completely change comparing to the Yule’s index. Speeches which had a rich vocabulary diversification seems to had less vocabulary diversification with a windows of 10 words. 2.9.1 Emmanuel Macron textstat_lexdiv(corpus_macron, measure = &quot;MATTR&quot;, MATTR_window = 10) %&gt;% ggplot(aes(x=reorder(document,MATTR), y=MATTR))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;MATTR&quot;) Figure 2.17: MATTR for Macron 2.9.2 Boris Johnson textstat_lexdiv(corpus_boris, measure = &quot;MATTR&quot;, MATTR_window = 10) %&gt;% ggplot(aes(x=reorder(document,MATTR), y=MATTR))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;MATTR&quot;) Figure 2.18: MATTR for Johnson "],
["sentiment-analysis.html", "Chapter 3 Sentiment Analysis 3.1 Analysis with the “nrc” library 3.2 Analysis with the LSD2015 dictionnary 3.3 Analysis with the “afinn” dictionnary", " Chapter 3 Sentiment Analysis 3.1 Analysis with the “nrc” library We use the “nrc” dictionary to start our sentiment analysis on the discourses of our two politicians. It is a dictionnary qualifying tokens by specific sentiments and by labelling them “negative” or “positive”. To do so, we will match the tokens of both corpuses with the dictionnary by applying an inner join. However, to use the inner_join function, we need a table object, what our objects are not primarily. We reload the data to create objects specific to this stage, boris_2 and macron_2, which are registered as tibble and allowing the use of the inner_join function. From the token list per document boris.tok, we join the corresponding qualifier in nrc using an inner_joint: ##################################################################################################################################### ########################################## Let&#39;s start with the Boris Johnson&#39;s discourses ###################################### ##################################################################################################################################### boris_2&lt;-as.tibble( c(boris9mars, boris12mars, boris16mars, boris18mars, boris19mars, boris20mars, boris22mars)) # trick to get a &quot;tbl_df&quot;,&quot;tbl&quot;,&quot;data.frame&quot; compatible with the inner_join function DocumentB &lt;- c(&quot;Text1&quot;,&quot;Text2&quot;,&quot;Text3&quot;,&quot;Text4&quot;,&quot;Text5&quot;,&quot;Text6&quot;,&quot;Text7&quot;) # adding a column &quot;Document&quot; to have a landmark for the tokens boris_2$Document &lt;- DocumentB boris_2 &lt;- boris_2[,c(2,1)] boris_2.tok &lt;- unnest_tokens(boris_2, output=&quot;word&quot;, input=&quot;value&quot;, to_lower=TRUE, strip_punct=TRUE, strip_numeric=TRUE) # unnest tokens of the table boris_2.sent&lt;- boris_2.tok %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) # do the inner join to merge the two tables ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron_2&lt;-as.tibble( c(macron12march, macron13april, macron16march)) # trick to get a &quot;tbl_df&quot;,&quot;tbl&quot;,&quot;data.frame&quot; compatible with the inner_join function DocumentM &lt;- c(&quot;Text1&quot;,&quot;Text2&quot;,&quot;Text3&quot;) # adding a column &quot;Document&quot; to have a landmark for the tokens macron_2$Document &lt;- DocumentM macron_2 &lt;- macron_2[,c(2,1)] macron_2.tok &lt;- unnest_tokens(macron_2, output=&quot;word&quot;, input=&quot;value&quot;, to_lower=TRUE, strip_punct=TRUE, strip_numeric=TRUE) # unnest tokens of the table macron_2.sent&lt;- macron_2.tok %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) # do the inner join to merge the two tables After creating our objects, we investigate which sentiment are present in Boris Johnson’s discourses. To do so, we use a numerical and a graphical method. The numerical method is simply a matrix of the frequency of tokens identified to a certain sentiment. Remind that a word can have more than 1 sentiment, which can lead to slight an overestimation of the sentiment. The table below presents positive discourses from the UK’s First Minister which are mainly weighted by the sentiment “trust”. We could have assumed that the discourse would be reassuring in order to avoid any panic due to the inedite circumstances of the covid. “Anticipation” is as well high, for the same reason (annoucement of the futures measures ans aaniticpations of the consequences, e.g.) However, postive sentiments are balanced by the relative high score of the fear, followed by disgust and anger. In absolute values, the most sentimental discourse was the second public word, on March 12th 2020. The graphical representations of the sentiments among discourses enables us a quick glimpse on these results. The discourses of the Prime Minister are animated by the current need. As said before, our data sets include 7 speeches in 14 days: the Prime Minister speaks more often in order to monitor a close relationship with UK citizen while given the daily progress in the covid-19 handling. table(boris_2.sent$Document,boris_2.sent$sentiment) %&gt;% kable() %&gt;% kable_styling() # sentiment terms per document anger anticipation disgust fear joy negative positive sadness surprise trust Text1 7 14 5 15 4 14 33 7 4 24 Text2 16 33 17 33 8 35 63 22 7 45 Text3 12 16 14 25 10 23 46 15 8 31 Text4 11 24 8 14 9 20 50 11 8 35 Text5 11 18 10 19 8 24 30 12 5 19 Text6 8 17 6 14 9 22 45 8 2 35 Text7 3 13 4 12 8 13 26 4 3 23 The graphical representation of the sentiments among discourses enables us a quick glimpse on these results. boris_2.sent %&gt;% group_by(Document,sentiment) %&gt;% summarize(n=n())%&gt;% mutate(freq=n/sum(n)) %&gt;% ggplot(aes(x=sentiment,y=freq,fill=sentiment)) + geom_bar(stat=&quot;identity&quot;,alpha=0.8) + facet_wrap(~ Document) + coord_flip() + ggtitle(&quot;Boris Johnson: Graphical representation of the sentiment per text&quot;) + xlab(&quot;Frequencies of the sentiments&quot;) + ylab(&quot;Sentiment&quot;) + geom_text(aes(label = n), size = 3, hjust = 1, vjust = 0, position = &quot;stack&quot;) Figure 3.1: Boris Johnson: Graphical representation of the sentiment per text By looking now at the discourses of Macron using the same method, we note the seemingly same frequencies of sentiments in the table below. His words appear to be carefully built and the variance is very low, perhaps to give the impression to have a stable and coherent speeches overtime. The categories of sentiments are quite similarly distributed: the speeches are positive marked by trust and anticipation. The proportions in the barplots are the same for speech 1, 2, and 3. We might assume they are based on a specific inner structure. The construction of text The French President would read is cautiously studied. This point, combined with the observation on the length, is interesting: Emmanuel Macron speaks more during less public orations, in order to have more powerful public interventions table(macron_2.sent$Document,macron_2.sent$sentiment) %&gt;% kable() %&gt;% kable_styling() # sentiment terms per document anger anticipation disgust fear joy negative positive sadness surprise trust Text1 26 88 16 73 39 109 196 43 33 122 Text2 29 96 21 63 46 105 188 44 31 107 Text3 25 63 10 52 25 92 149 30 22 102 macron_2.sent %&gt;% group_by(Document,sentiment) %&gt;% summarize(n=n())%&gt;% mutate(freq=n/sum(n)) %&gt;% ggplot(aes(x=sentiment,y=freq,fill=sentiment)) + geom_bar(stat=&quot;identity&quot;,alpha=0.8) + facet_wrap(~ Document) + coord_flip() + ggtitle(&quot;Graphical representation of the sentiment per text&quot;) + xlab(&quot;Frequencies of the sentiments&quot;) + ylab(&quot;Sentiment&quot;) + geom_text(aes(label = n), size =3, hjust = 1, vjust = 0, position = &quot;stack&quot;) 3.2 Analysis with the LSD2015 dictionnary In order to better analyse those results and fortify those insights, we double check with the dictionnary LSD2015. It is another dictionnary assigning a qualifier to terms. The difference with tidytext is essentially in the manipulation of the objects: it handles the “tokens” class and for this reason we have to recode our objects. The results are impacted by this different treatment and we do expect slight changes. As the figure 3.2 shows, the results of Text 2 as the most positive and all the discourses as majoritarily positive are confirmed. For Macron’s speeches, the proportions of sentiments differs: the first discourse is associated with more postive sentiment (224) relatively to proportion computed with the “nrc” dictionnary (193). However, the trend remains the same. We keep in mind that difference in length of texts explains the varying size of the bar, as previously. Anyway both findings converge to the same points. ###################################################################################################################################### ########################################## Let&#39;s start with the Johnson&#39;s discourses ########################################### ###################################################################################################################################### boris.cp&lt;-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) summary(boris.cp) Text Types Tokens Sentences text1 266 609 23 text2 409 1222 50 text3 405 1231 43 text4 406 1230 52 text5 321 994 37 text6 357 1030 47 text7 300 793 35 boris.tk&lt;-tokens(boris.cp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) boris.tk&lt;-tokens_tolower(boris.tk) boris.tk&lt;- tokens_replace(boris.tk,pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) boris.tk&lt;-boris.tk %&gt;% tokens_remove(stopwords(&quot;english&quot;)) boris.sent&lt;- tokens_lookup(boris.tk,dictionary = data_dictionary_LSD2015) %&gt;% dfm() %&gt;% tidy boris.plot.quanteda &lt;- ggplot(boris.sent, aes( x = document, y = count, fill = term)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + ggtitle(&quot;Johnson: Proportion of sentiment using the dictionnary LSD2015&quot;) + xlab(&quot;Document&quot;) + ylab(&quot;Number of terms attributed to negative and positive sentiments&quot;) + geom_text(aes(label = count), size =3, hjust = 5, vjust = 0, position = &quot;stack&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron.cp&lt;-corpus(c(macron12march,macron13april,macron16march)) macron.tk&lt;-tokens(macron.cp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) macron.tk&lt;-tokens_tolower(macron.tk) macron.tk&lt;- tokens_replace(macron.tk,pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) macron.tk&lt;-macron.tk %&gt;% tokens_remove(stopwords(&quot;english&quot;)) macron.sent&lt;- tokens_lookup(macron.tk,dictionary = data_dictionary_LSD2015) %&gt;% dfm() %&gt;% tidy macron.plot.quanteda&lt;- ggplot(macron.sent, aes( x = document, y = count, fill = term)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + ggtitle(&quot;Macron: Proportion of sentiment using the dictionnary LSD2015&quot;) + xlab(&quot;Document&quot;) + ylab(&quot;Number of terms attributed to negative and positive sentiments&quot;) + geom_text(aes(label = count), size =3, hjust = 5, vjust = 0, position = &quot;stack&quot;) grid.arrange(boris.plot.quanteda,macron.plot.quanteda) Figure 3.2: Proportion of sentiment using the dictionnary LSD2015 3.3 Analysis with the “afinn” dictionnary We now use a different approach: a quantitative way to assess the sentiment analysis. To do so, we use the “afinn” dictionnary which attributes a value to the word, taking into account the power conveyed by the term (value between 0 and 1) and its qualitative classification (positive or neagtive sign). The classification of the words was encoded differently than for the “nrc” dictionnary. Again, we do expect different scores, but hopefully in the same direction. The results displayed by the figure 3.3 are derived from an average score of sentiment per document. For the Boris Johnson’s speeches, the second one has a contradictory score with the previous results: it belongs to the lowest scored text in positive sentiments. We can thus observe the difference in encodage among dictionnaries used in Text Mining. The “afinn” dictionnarywas encoded by a Danish Professor, LSD2015 by two American professors, and “nrc” by a Canadian professors working in an Commision on Ethics. The categorization is subjective and lead to substantial differences. Those results need to be mitigate by a last approach: the use of valence shifters. ###################################################################################################################################### ########################################## Let&#39;s continue with the Johnson&#39;s discourses ########################################### ###################################################################################################################################### boris_2.sent &lt;- boris_2.tok %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) boris.plot.afinn&lt;-aggregate(value~Document, data =boris_2.sent,FUN=mean) %&gt;% ggplot(aes(x=Document,y=value, fill = Document)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + ggtitle(&quot;Johnson: Sentiment score per text by using afinn dictionnary&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Score value&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron_2.sent &lt;- macron_2.tok %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) macron.plot.afinn &lt;- aggregate(value~Document, data =macron_2.sent,FUN=mean) %&gt;% ggplot(aes(x=Document,y=value, fill = Document)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + ggtitle(&quot;Macron: Sentiment score per text by using afinn dictionnary&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Score value&quot;) grid.arrange(boris.plot.afinn,macron.plot.afinn) Figure 3.3: Graphical representation: Sentiment score per text by using afinn dictionnary 3.3.1 Analysis using “nrc”&quot; dictionnary and valence shifters The sentimentr library offers some function to compute sentiments integrating valence shiters. There are specific words which amplify or reduce the power of a word, even turn it into the reverse sentiment. We evaluate sentences here, and one important aspect is that it cannot be applied to a Bag Of Word model, since the word order is necessary. The valence shifters extract more acurately the sentiment ouf of the text, since it consider the sentiment conveyed by a sentence and not only words without context. We should take the insights given by valence shifters as the ultimate confirmation test for the previous results. The results are displayed by the plot 3.4. The second discourse of the Prime Minister here is one of the longest: it contains about 50 sentences and is only preceded by the fourth public word, on March 18th. It is the one varying the most in sentiment, what might explains the dffference in values observed between LSD2015&amp;nrc with afinn dictionnary. Why? Simply because it was the first speech speaking openly about the covid and the serious measures to take. The sixth one shows one positive peak, what correspond to the moment whn Boris Johnson strengthened the measures and made a speech to reassure UK citizen. At this time, the measures taken weremore rigid: thre spread of the virus had intensifyied. He brought information with positive sentiments to unify the population under the new circumstances and to push the to obey by showing them the positive impact of respecting stonger measures. Macron’s speeches are well longer, three times more than Boris Johnson’s ones, as we observe in the second plot. His words are generally more neutral but convey sometimes strong negative sentiments, like the 4th sentence. Actually, this sentence is not strongly negative but uses terms which are: “In the vast majority of cases, COVID-19 does not pose a threat, but the virus can have very serious consequences, especially for those of our fellow citizens who are elderly or suffer from chronic illnesses such as diabetes, obesity or cancer”. The variation of the sentiments is greater in the last allocution, on April 13th 2020. Serious + consequences, chronic + illness, diabete+ obesity + cancer aside amplify the seriousness of the sentence. We observe as well a greater variation in sentiments the last allocution of Emmanuel Macon, on April 13th 2020. It corresponds to the moment when the situation became critical in France. The President emphasizes the serious danger of the covid, the sanitary situation being worse than in UK. boris.text&lt;-get_sentences(boris) boris.senti&lt;-sentiment(boris.text) boris.senti&lt;-as_tibble(boris.senti) boris.senti%&gt;% group_by(element_id) %&gt;% ggplot(aes(x=sentence_id,y=sentiment, col = element_id )) + geom_line() + facet_wrap(~element_id) + ggtitle(&quot;Johnson: Evolution of the sentiments \\nwithin speeches using valence shifters&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Sentences in speeches&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron.text&lt;-get_sentences(macron) macron.senti&lt;-sentiment(macron.text) macron.senti&lt;-as_tibble(macron.senti) macron.senti%&gt;% group_by(element_id) %&gt;% ggplot(aes(x=sentence_id,y=sentiment, col = element_id )) + geom_line() + facet_wrap(~element_id) + ggtitle(&quot;Macron: Evolution of the sentiments \\nwithin speeches using valence shifters&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Sentences in speeches&quot;) Figure 3.4: Evolution of the sentiments within speeches using valence shifters "],
["similarities.html", "Chapter 4 Similarities 4.1 Boris 4.2 Macron", " Chapter 4 Similarities The aim of this part of the project is to compute the similarities and dissimilarities between the different doscuments for both Johnson and Macron. We will use here the previously cleaned and tokenised corpuses and the two TF-IDF matrices computed when performing the exploratory data analysis. The three metrics used are the following; the Jaccard Similarity (similarity measure) 4.1, the Cosine Similarity (similarity measure) 4.2 and the Euclidean Distance 4.3 (dissimilarity measure, bounded by the largest distance that is present in the corpus, can therefore be rescaled to a similarity measure between 0 and 1, 1 being the largest distance in the corpus). In order to get a better visualisation of the three metrics, we used a heatmap representation (similarity = 0 –&gt; yellow and similarity = 1 –&gt; red). Actually, when looking at the various heatmaps drawn when running the code, all those similarity measures show the same results, there is not any large similarity between the different documents for Boris Johnson. The only cases on the heatmap that are red are the ones that are on the diagonal, which corresponds to the similarity of a given document and itself, which is equal to 1. 4.1 Boris ## Jaccard Similarity boris.jac &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity boris.cos &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance boris.euc &lt;- textstat_dist(corpus_boris.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix boris.jac.mat &lt;- melt(as.matrix(boris.jac)) ggplot(data=boris.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() Figure 4.1: Jaccard Similarity - Boris Johnson ## Cosine Matrix boris.cos.mat &lt;- melt(as.matrix(boris.cos)) ggplot(data=boris.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() Figure 4.2: Cosine Similarity - Boris Johnson ## Euclidean Matrix boris.euc.mat &lt;- melt(as.matrix(boris.euc)) M &lt;- max(boris.euc.mat$value) boris.euc.mat$value.std &lt;- (M-boris.euc.mat$value)/M ggplot(data=boris.euc.mat, aes(x=Var1, y=Var2, fill=boris.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() Figure 4.3: Euclidean Distance - Boris Johnson We then used two different clustering methods, hierarchical clustering (dendrogram) 4.4 and partitioning (K-means method) ??. We see that the results are quite similar. When looking at the 10 most common words per cluster, there are some words that appear when using the first method and the second one. ## Clustering ## Jaccard Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.jac)) plot(boris.hc) ## Cosine Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.cos)) plot(boris.hc) ## Dendrogram = Hierarchical Clustering boris.clust &lt;- cutree(boris.hc, k=3) boris.clust #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 1 2 2 2 3 3 3 Figure 4.4: Dendrogram - Hierarchical Clustering ## K-means Method = Partitionning boris.km &lt;- kmeans(corpus_boris.tfidf, centers=3) boris.km$cluster #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 1 1 3 2 1 1 1 ### Extracting the 10 most used words - Dendrogram data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.clust==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.clust==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.clust==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 outbreak school already chris child see four mass progress manage parent robert tackle period jenrick minister public behind phase ensure thousand thing important bite patrick dangerous virus delay london huge ### Extracting the 10 most used words - K-Means data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 outbreak school mass already child london see parent contact chris pupil ensure dangerous teacher gathering progress already fight robert fightback without jenrick update stop virus judgment non-essential public downward rather When computing document similarities for Macron, we also observe that the only elements on the heatmap that are represented by the red colour are situated on the digonal. However, we can notice a slight difference here compared to Johnson. We do observe that there is a small similarity between document 1 and document 2 for Macron. This is to say that he used the same tokens both in the first and the second document. 4.2 Macron ## Jaccard Similarity macron.jac &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity macron.cos &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance macron.euc &lt;- textstat_dist(corpus_macron.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix macron.jac.mat &lt;- melt(as.matrix(macron.jac)) ggplot(data=macron.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() Figure 4.5: Jaccard Similarity - Emmanuel Macron ## Cosine Matrix macron.cos.mat &lt;- melt(as.matrix(macron.cos)) ggplot(data=macron.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() Figure 4.6: Cosine Similarity - Emmanuel Macron ## Euclidean Matrix macron.euc.mat &lt;- melt(as.matrix(macron.euc)) M &lt;- max(macron.euc.mat$value) macron.euc.mat$value.std &lt;- (M-macron.euc.mat$value)/M ggplot(data=macron.euc.mat, aes(x=Var1, y=Var2, fill=macron.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() Figure 4.7: Euclidean Distance - Emmanuel Macron "],
["topic-modelling.html", "Chapter 5 Topic Modelling 5.1 Boris Johnson 5.2 Macron 5.3 Combine", " Chapter 5 Topic Modelling For this chapter, we analyzed the different topics used in the speeches. We used : - the Latent Semantic Analysis, which is going to decompose the DTM Matrix into the document-topic similarity, the topic strength and the term-to-topic similarity - and the Latent Dirichlet Allocation, that is a typical dimension reduction technique, which uses dirichlet priors for the document-topic and the word-topic distribution We have first analyzed the speeches of Johnson and Macron separately and we have then combined them together. 5.1 Boris Johnson 5.1.1 LSA First, we make the DTM matrix. We are goin to use 3 dimensions, it means 3 differents topics. bmod&lt;-textmodel_lsa(corpus_boris.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition. In the firs table, each components measures the link between the document and the topic.5.1 In the second table, each component measure the link between the document and the term. 5.2 LSA is typical a reduction technique. Instead of have N documents or M term, it is represented by K documents. lsa_docs_boris&lt;-head(bmod$docs) lsa_docs_boris&lt;-data.frame(lsa_docs_boris) lsa_docs_boris%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.1: Link between document and topic X1 X2 X3 text1 -0.175 0.053 -0.148 text2 -0.519 0.052 -0.813 text3 -0.358 -0.737 0.101 text4 -0.525 0.626 0.342 text5 -0.299 -0.210 0.314 text6 -0.356 -0.121 0.226 lsa_features_boris&lt;-head(bmod$features) lsa_features_boris&lt;-data.frame(lsa_features_boris) lsa_features_boris%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.2: Link between document and terms X1 X2 X3 morning -0.002 0.002 -0.005 government’s -0.011 0.005 -0.039 cobr -0.007 -0.023 -0.002 emergency -0.030 -0.050 -0.011 committee -0.009 0.004 -0.034 coronavirus -0.047 0.015 -0.026 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. As we observe in the figure 5.1, the dimension 1 is negatively correlated with the document lenght. Therefore the dimension 1 bring us not a lot of informations that we have already. ns&lt;-apply(corpus_boris.dfm,1,sum) plot(ns~bmod$docs [,1]) Figure 5.1: First dimension of the LSA - Boris Johnson We clearly observe that the dimension 1 is negatively correlated with the document lenght. Now in order to make the link between the topics and the documents and the topics with term, we use biplot. We represent the dimension 2 and 3, beacause often the first component bring often little information. Reminders: The seven speech are class by chronological order: - 09 March (text1) - 12 March (text2) - 16 March (text3) - 18 March (text5) - 19 March (text6) - 20 March (text7) - 22 March (text8) It is noticeable that the texts that are closer in time are grouped together. And that the first speeches go in the opposite direction of the one of the last speeches, as we observe in the figure 5.2. biplot(y=bmod$docs[,2:3],x=bmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) Figure 5.2: Biplot - Boris Johnson We repeat the same analysis with TF-IDF. The influence of small weighted-frequent tokens is reduced. In the firs table 5.3, each components measures the link between the document and the topic. In the second table 5.4 each component measure the link between the document and the term. bmod_2&lt;- textmodel_lsa(corpus_boris.tfidf, nd=3) lsa_docs_boris_2&lt;-head(bmod_2$docs) lsa_docs_boris_2&lt;-data.frame(lsa_docs_boris_2) lsa_docs_boris_2%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.3: Link between document and topic X1 X2 X3 text1 -0.147 -0.063 -0.318 text2 -0.380 -0.187 -0.752 text3 -0.444 -0.750 0.463 text4 -0.735 0.615 0.256 text5 -0.180 -0.132 -0.173 text6 -0.200 -0.040 -0.153 lsa_features_boris_2&lt;-head(bmod_2$features) lsa_features_boris_2&lt;-data.frame(lsa_features_boris_2) lsa_features_boris_2%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.4: Link between document and terms X1 X2 X3 morning -0.008 -0.004 -0.025 government’s -0.024 -0.014 -0.070 cobr -0.021 -0.037 0.007 emergency -0.042 -0.038 0.013 committee -0.019 -0.011 -0.054 coronavirus -0.016 -0.001 -0.011 5.1.2 LDA We now turn to the LDA. For illustration, we will make K=3 topis. K&lt;-3 corpus_boris.dtm&lt;- convert(corpus_boris.dfm, to=&quot;topicmodels&quot;) lda_boris&lt;- LDA(corpus_boris.dtm ,k=K) In the table 5.5, it is the list of the six most frequent term in each topic terms&lt;-terms(lda_boris,6) terms&lt;-data.frame(terms) terms %&gt;% kable(caption=&quot;List of the terms present in each topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.5: List of the terms present in each topic Topic.1 Topic.2 Topic.3 will will will want much go school now can much disease people spread people know make can want In the table 5.6, you can observe which text is related to which topic. ## To see the topics related to each document topics&lt;-(topics(lda_boris,1)) topics&lt;-data.frame(topics) topics%&gt;% kable(caption=&quot;Topics&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.6: Topics topics text1 1 text2 2 text3 2 text4 1 text5 3 text6 3 text7 1 We now build the bar plot to inspect the per-topic-per-word probabilities (beta’s). We take the 10 top terms and rearrange the betas per topic according to this order. We observe in the figure 5.3 that topic 1 is correlated to the tokens “will”, “go” and “want”, topic 2 to the token “school” and topic 3 to the tokens “now”, “can” and “will”. beta.td.boris&lt;-tidy(lda_boris,matrix=&quot;beta&quot;) beta.top.term.boris&lt;-beta.td.boris %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.boris %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + geom_col(show.legend = FALSE)+ facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() Figure 5.3: Beta - Boris Johnson Now, we compute the gamma, it shows the proportion of each topic within each document, as you can observe in the figure 5.4. We see that the first speeches are related to topic 3 (“now”, “can”, “will”). This makes sense as those speeches were made in the begining of the pandemic and are linked to a sense of urgency. The speeches in the middle correspond to the second topic and all the measures implented for schools. The three last speeches are represented by the first topic only, and are correlated to the tokens “will”, “go” and “want”. We can conlude from this that the British Prime Minster wanted his citizens to keep up the efforts. gamma.td.boris&lt;- tidy(lda_boris,matrix=&quot;gamma&quot;) gamma.td.boris %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() Figure 5.4: Gamma - Boris Johnson 5.2 Macron 5.2.1 LSA mmod&lt;-textmodel_lsa(corpus_macron.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition 5.7 5.8 lsa_docs_macron&lt;-head(mmod$docs) lsa_docs_macron&lt;-data.frame(lsa_docs_macron) lsa_docs_macron%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.7: Link between document and topic X1 X2 X3 text1 -0.662 0.495 0.563 text2 -0.426 0.370 -0.825 text3 -0.617 -0.786 -0.034 lsa_features_macron&lt;-head(mmod$features) lsa_features_macron&lt;-data.frame(lsa_features_macron) lsa_features_macron%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.8: Link between document and terms X1 X2 X3 check -0.004 0.010 0.013 delivery -0.011 -0.021 0.012 france -0.069 0.031 0.017 dear -0.054 0.018 -0.021 past -0.025 -0.010 0.018 country -0.084 -0.077 -0.065 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. ns_macron&lt;-apply(corpus_macron.dfm,1,sum) plot(ns_macron~mmod$docs [,1]) Figure 5.5: First dimension of the LSA - Emmanuel Macron We clearly observe that the dimension 1 is negatively correlated with the document lenght, as we observe in the figure 5.5. Now in order to make the link between the topics and the documents and the topics with term, we use biplot 5.6. We clearly observe in this figure that each text correspond to a specific topic, they don’t do in the same direction. biplot(y=mmod$docs[,2:3],x=mmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) Figure 5.6: Biplot - Emmanuel Macron We repeat the same analysis with TF-IDF, 5.9,5.10 mmod_2&lt;- textmodel_lsa(corpus_macron.tfidf, nd=3) lsa_docs_macron_2&lt;-head(mmod_2$docs) lsa_docs_macron_2&lt;-data.frame(lsa_docs_macron_2) lsa_docs_macron_2%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.9: Link between document and topic X1 X2 X3 text1 -0.318 0.930 0.187 text2 -0.115 0.158 -0.981 text3 -0.941 -0.333 0.056 lsa_features_macron_2&lt;-head(mmod_2$features) lsa_features_macron_2&lt;-data.frame(lsa_features_macron_2) lsa_features_macron_2%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 5.10: Link between document and terms X1 X2 X3 check -0.010 0.035 0.008 delivery -0.026 0.004 0.005 france 0.000 0.000 0.000 dear 0.000 0.000 0.000 past 0.000 0.000 0.000 country 0.000 0.000 0.000 5.3 Combine We combine the speeches of both authors in the same dataframe, that will have a total of ten speeches. ##Boris Johnson boris_2&lt;-as_tibble(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) %&gt;% rename( text=value) author=&quot;Boris Johnson&quot; boris_2&lt;- cbind(boris_2, author) ##Emmanuel Macron Macron_2&lt;-as_tibble(c(macron12march,macron16march,macron13april)) %&gt;% rename( text = value) author=&quot;Macron&quot; macron_2&lt;- cbind(Macron_2, author) ##Combine the 2 dataframes combine &lt;- rbind(boris_2, macron_2) ## Tokenization combine_corpus&lt;-corpus(combine) combine_tokens&lt;- tokens(combine_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ##combi Lemmatization combine_tokens &lt;- tokens_replace(combine_tokens, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning combine_tokens = combine_tokens %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) 5.3.1 LSA combine_corpus.dfm &lt;- dfm(combine_tokens) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=6) Often the first dimension in the LSA is associated with the document length. To see if it is true, we build a scatter-plot between the document length and Dimension 1. We clearly observe that the three last documents are the longest ones and they all correspond to Macron’s speeches. ns_combine&lt;-apply(combine_corpus.dfm,1,sum) plot(ns_combine~cmod$docs [,1]) We then decide to proceed to the analysis using the second and third dimensions,figure ??, as we have observed that the first dimension is negatively correlated to the document length and does not therefore bring us a lot of information. We see that the speeches of Macron and Johnson are represented by different dimensions. Macron’s speeches correspond better to the second dimension (documents 8, 9 and 10), whereas the third dimension is better represented by Johnson. This tells us that without building any further model and just by looking at the biplot corresponding to the second and third dimensions, we can guess whether a new corpus would belong to Johnson or to Macron. biplot(y=cmod$docs[,2:3],x=cmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) "],
["word-embedding.html", "Chapter 6 Word Embedding 6.1 Boris Johnson 6.2 Macron", " Chapter 6 Word Embedding A word embedding is a learned representation for text where words that have the same meaning have a similar representation. This is a method of learning a representation of words used in particular in automatic language processing. The term should rather be rendered by vectorisation of words in order to correspond more neatly to this method. 6.1 Boris Johnson Here, we compute the co-occurence matrix. We use the fcm function from quanteda. We use a window lenght 5. speech.coo.boris&lt;-fcm(corpus_boris,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.boris&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.boris&lt;-speech.glove.boris$fit_transform(speech.coo.boris) #&gt; INFO [12:20:26.253] epoch 1, loss 0.0342 #&gt; INFO [12:20:26.268] epoch 2, loss 0.0244 #&gt; INFO [12:20:26.277] epoch 3, loss 0.0225 #&gt; INFO [12:20:26.283] epoch 4, loss 0.0215 #&gt; INFO [12:20:26.289] epoch 5, loss 0.0208 #&gt; INFO [12:20:26.295] epoch 6, loss 0.0203 #&gt; INFO [12:20:26.301] epoch 7, loss 0.0198 #&gt; INFO [12:20:26.306] epoch 8, loss 0.0193 #&gt; INFO [12:20:26.312] epoch 9, loss 0.0189 #&gt; INFO [12:20:26.318] epoch 10, loss 0.0184 For illustration purpose, we now plot the 50 most used terms as you can observe in the figure 6.1. More the words are close, more they are similar. Two word are similar if they are often use in the same context. n.w.boris&lt;-apply(corpus_boris.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.boris,decreasing = TRUE)[1:50] plot(speech.weC.boris[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.boris[index,],labels = rownames(speech.weC.boris[index,])) Figure 6.1: The 50 most used terms In the figure 6.2 speech.dtm &lt;-corpus_boris.dfm speech.rwmd.model.boris&lt;-RelaxedWordMoversDistance$new(corpus_boris.dfm,speech.weC.boris) speech.rwms.boris&lt;-speech.rwmd.model.boris$sim2(corpus_boris.dfm) speech.rwmd.boris&lt;-speech.rwmd.model.boris$dist2(corpus_boris.dfm) speech.hc.boris&lt;-hclust(as.dist(speech.rwmd.boris)) plot(speech.hc.boris,cex=0.8) Figure 6.2: Cluster Dendogram We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.boris&lt;- cutree(speech.hc.boris,k=4) corpus_boris.dfm[speech.cl.boris==1,] #&gt; Document-feature matrix of: 2 documents, 797 features (72.0% sparse). #&gt; features #&gt; docs morning government&#39;s cobr emergency committee coronavirus #&gt; text1 1 2 1 1 1 3 #&gt; text3 0 0 1 3 0 1 #&gt; features #&gt; docs outbreak first scotland minister #&gt; text1 5 4 1 3 #&gt; text3 0 1 0 0 #&gt; [ reached max_nfeat ... 787 more features ] 6.2 Macron speech.coo.macron&lt;-fcm(corpus_macron,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.macron&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.macron&lt;-speech.glove.macron$fit_transform(speech.coo.macron) #&gt; INFO [12:20:26.610] epoch 1, loss 0.0247 #&gt; INFO [12:20:26.619] epoch 2, loss 0.0179 #&gt; INFO [12:20:26.628] epoch 3, loss 0.0163 #&gt; INFO [12:20:26.639] epoch 4, loss 0.0152 #&gt; INFO [12:20:26.648] epoch 5, loss 0.0145 #&gt; INFO [12:20:26.658] epoch 6, loss 0.0136 #&gt; INFO [12:20:26.672] epoch 7, loss 0.0130 #&gt; INFO [12:20:26.680] epoch 8, loss 0.0123 #&gt; INFO [12:20:26.689] epoch 9, loss 0.0118 #&gt; INFO [12:20:26.704] epoch 10, loss 0.0114 For illustration purpose, we now plot the 50 most used terms 6.3 n.w.macron&lt;-apply(corpus_macron.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.macron,decreasing = TRUE)[1:50] plot(speech.weC.macron[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.macron[index,],labels = rownames(speech.weC.macron[index,])) Figure 6.3: The 50 most used terms 6.4 speech.dtm.macron &lt;- corpus_macron.dfm speech.rwmd.model.macron&lt;-RelaxedWordMoversDistance$new(corpus_macron.dfm,speech.weC.macron) speech.rwms.macron&lt;-speech.rwmd.model.macron$sim2(corpus_macron.dfm) speech.rwmd.macron&lt;-speech.rwmd.model.macron$dist2(corpus_macron.dfm) speech.hc.macron&lt;-hclust(as.dist(speech.rwmd.macron)) plot(speech.hc.macron,cex=0.8) Figure 6.4: Cluser Dendogram We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.macron&lt;- cutree(speech.hc.macron,k=2) corpus_macron.dfm[speech.cl.macron==1,] #&gt; Document-feature matrix of: 2 documents, 1,366 features (45.8% sparse). #&gt; features #&gt; docs check delivery france dear past country spread virus #&gt; text1 1 1 9 6 3 6 8 13 #&gt; text3 0 2 6 5 3 12 2 12 #&gt; features #&gt; docs covid-19 several #&gt; text1 4 5 #&gt; text3 1 5 #&gt; [ reached max_nfeat ... 1,356 more features ] "],
["supervised-learning.html", "Chapter 7 Supervised learning 7.1 LSA 7.2 Random forest 7.3 Improving the features", " Chapter 7 Supervised learning In this section, we use a supervised learner to develop a classifier of the Politicans’ speeches. The aim of this section is to have a classification model able to correctly attribute a random speech to Boris Johnson or Emmanuel Macron. To do so, we first combine the dataframe of Boris Johnson with the dataframe of Emmanuel Macron. Since those dataframes differ in number of speechs and in length, we divide the speeches into sentences, which would smooth difference between our two dependent outcome possibilities. ##Boris Johnson boris_2&lt;-as_tibble(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) %&gt;% rename( text=value) author=&quot;Boris Johnson&quot; boris_supervised&lt;- cbind(boris_2, author) boris_2_sentence&lt;-get_sentences(boris_supervised) ##Emmanuel Macron Macron_2&lt;-as_tibble(c(macron12march,macron16march,macron13april)) %&gt;% rename( text = value) author=&quot;Macron&quot; macron_supervised&lt;- cbind(Macron_2, author) macron_2_sentence&lt;-get_sentences(macron_supervised) ##Combine the 2 dataframes combine &lt;- rbind(boris_2_sentence, macron_2_sentence) ## Tokenization combine_corpus&lt;-corpus(combine) combine_tokens&lt;- tokens(combine_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ##combi Lemmatization combine_tokens &lt;- tokens_replace(combine_tokens, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning combine_tokens = combine_tokens %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) y&lt;-factor(docvars(combine_tokens,&quot;author&quot;)) Then, we build the featues. To this aim, we first compute the DTM matrix. combine.dfm&lt;-dfm(combine_tokens) combine.dfm #&gt; Document-feature matrix of: 771 documents, 1,691 features (99.4% sparse) and 3 docvars. #&gt; features #&gt; docs morning meeting government&#39;s cobr emergency committee #&gt; text1 0 0 0 0 0 0 #&gt; text2 1 0 1 1 1 1 #&gt; text3 0 0 0 0 0 0 #&gt; text4 0 0 0 0 0 0 #&gt; text5 0 0 0 0 0 0 #&gt; text6 0 0 0 0 0 0 #&gt; features #&gt; docs coronavirus outbreak first scotland #&gt; text1 0 0 0 0 #&gt; text2 1 1 0 0 #&gt; text3 0 0 3 1 #&gt; text4 0 0 0 0 #&gt; text5 0 0 0 0 #&gt; text6 1 0 0 0 #&gt; [ reached max_ndoc ... 765 more documents, reached max_nfeat ... 1,681 more features ] 7.1 LSA Because of the huge number of tokens, the feature matrix obtained may be too big to train a model in a reasonable amount of time. We thus apply a reduction dimension technque in order to obtain less features while keeping the relevant information. LSA is the perfect technique to achieve this. We target 30 dimensions (30 subjects) combine_corpus.dfm &lt;- dfm(combine_corpus) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=30) 7.2 Random forest After preparing our data to be used by the learner, we decide to run a random forest, which is a robust method to find the best classification model by computing a large set of classification tree to obtain the most pertinent values of classification criterias. After building our model, we create a training and a test sets. In this simple context, in order to illustrate the concepts without too long computation times, we will limit ourselves to just one training set and one test set by applying the Pareto law 80-20. set.seed(782) df&lt;-data.frame(Class=y, x=cmod$docs) index.tr&lt;-sample(size = round(0.8*length(y)),x=c(1:length(y)),replace = FALSE) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(Class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) In order to see the prediction quality of the model, we call the confusionMatrix function in the caret package: confusionMatrix&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$Class) confusionMatrix #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Boris Johnson Macron #&gt; Boris Johnson 23 6 #&gt; Macron 29 96 #&gt; #&gt; Accuracy : 0.773 #&gt; 95% CI : (0.698, 0.836) #&gt; No Information Rate : 0.662 #&gt; P-Value [Acc &gt; NIR] : 0.00191 #&gt; #&gt; Kappa : 0.43 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00020 #&gt; #&gt; Sensitivity : 0.442 #&gt; Specificity : 0.941 #&gt; Pos Pred Value : 0.793 #&gt; Neg Pred Value : 0.768 #&gt; Prevalence : 0.338 #&gt; Detection Rate : 0.149 #&gt; Detection Prevalence : 0.188 #&gt; Balanced Accuracy : 0.692 #&gt; #&gt; &#39;Positive&#39; Class : Boris Johnson #&gt; The model has an accuracy of 77.3%, is is a quite poor accuracy, but enough good for a first try. But… if we have a look one the sensitivity ( which is the % of predict the positive class, here this is Boris johnson) it is only 44.2%. So, we have a good model in order to predict the speech of Macon, about 94%, but much less to predict Boris Johnson. 7.3 Improving the features In order to improve the accuracy, we look to improve the features construction. We may consider the elements fo feature construction as hyperparameters to be optimized. Therefore we compare differents dimensions with their corresponding accuracy. In the figure 7.1, we notice that with a dimension of 100, the accuracy will get higher than 84% ! nd.vec&lt;-c(2,5,25,50,100,500,1000) acc.vec&lt;-numeric(length(nd.vec)) for (j in 1:length(nd.vec)) { cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=nd.vec[j]) df&lt;-data.frame(class=y,x=cmod$docs) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) acc.vec[j]&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$class)$overall[1] } acc.vec #&gt; [1] 0.740 0.779 0.779 0.805 0.786 0.727 0.708 plot(acc.vec~nd.vec,type=&quot;b&quot;) Figure 7.1: Accuracy set.seed(788) combine_corpus.dfm &lt;- dfm(combine_corpus) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=100) df&lt;-data.frame(class=y, x=cmod$docs) index.tr&lt;-sample(size = round(0.8*length(y)),x=c(1:length(y)),replace = FALSE) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) confusionmatrix_2&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$class) confusionmatrix_2 #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Boris Johnson Macron #&gt; Boris Johnson 32 4 #&gt; Macron 19 99 #&gt; #&gt; Accuracy : 0.851 #&gt; 95% CI : (0.784, 0.903) #&gt; No Information Rate : 0.669 #&gt; P-Value [Acc &gt; NIR] : 2.57e-07 #&gt; #&gt; Kappa : 0.636 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00351 #&gt; #&gt; Sensitivity : 0.627 #&gt; Specificity : 0.961 #&gt; Pos Pred Value : 0.889 #&gt; Neg Pred Value : 0.839 #&gt; Prevalence : 0.331 #&gt; Detection Rate : 0.208 #&gt; Detection Prevalence : 0.234 #&gt; Balanced Accuracy : 0.794 #&gt; #&gt; &#39;Positive&#39; Class : Boris Johnson #&gt; By re-running the same model but by increasing dimensions rather than weighting the sentences, we observe an increase in accuracy,now the accuracy is 85.1%. Sensitivity has improved by 18.50% but the learner has a still a poor prediction ability, in spite of the increase of the dimensions.The improvement by weighting the independent variables would be potentially more efficient. We would keep it as being part of further investigations. "],
["conclusion.html", "Chapter 8 Conclusion", " Chapter 8 Conclusion We focused on the beginning of pandemic period by focusing on the speeches of both politicians in March. After having downloaded 3 speeches of Emmanuel Macron and 7 speeches of Boris Johnson, using Xpath, our Exploratory Data Analysis points out specificities among the UK and French supervised. In spite of the shortness of the Boris Johnson’s words, the vocabulary used by the Brititish Price Minister is richer than the allocutions of the French President. The French President speaks more about the future, by using at a high frequency the auxiliary «will», whereas the Prime Minister is more balanced in the words used, linking present and future and integrating social terms like family and friends. The Sentiment Analysis outlines more positive discourses coming from across the Channel, the speech on March 12th being the more positive than the others. The specificity of Emmanuel Macron’s structure is its stability: his allocutions follow an almost similar distribution of the sentiment, and this trend is not affected by the length of his speeches. By using the «nrc», «afinn» and «LSD2015», the qualitative ad quantitative-based results differs, but 3 stages out of the 4 are convergent, fortifying those results. The Similarity analysis was not significant: the results highlight no similarities among speeches of the same president. Some investigations might be needed in order to understand the roots of this outcome. The Word Embedding presents clusters in a seemingly chronological order for Boris Johnson, grouping step by step the 1rst speech to the later, while the respective Macron’s dendogram groups the 1rst and 3rd discourses together, joining ultimately the second one. Topic Modelling’s insights for the texts per politician has no added value: speaking of the same subject and in the same way, the discourses overlap each other by sharing certain similar topics. However, when combining French and British discourses, the Topic Modelling gives a clearer vision on the difference of topics mentioned. We note that the joint analysis leads to better quality results. Finally, after identifying some discriminatory features among speeches, we built a supervised learner to categorize random speeches as belonging to Emmanuel Macron ou Boris Johnson, using a random forest model. It is based on the division of the speeches by sentence. Our classification model is biased as predicting too much False Negative (sentences erroneously categorized as belonging to Emmanuel Macron). An attempt for improvement by increasing the number of features was satisfactory, increasing slightly the accuracy while keeping the overestimation of President Macron’s sentences. Further improvements might be operated by weighting the independent variables in order to reduce the bias. To conclude, our basis assumption of similarity of the construction of sentences and of the goal of such discourses might be correct. In addition to the similar subject (pandemic measures), the basis assumption could be a cause of the lukewarm results in Topic Modeling, Word Embedding and Similarity, limiting the potentiality of discriminatory features. The speeches differ greatly in length, vocabulary and sentiments, but those differences are not as strong as needed for building a satisfactory random forest classification learner. The project might be leveled up by adding new speeches (taking a larger period of time) and rethinking our supervised learner to be based on more accurate independent variables, assessed by a statistical analysis and investigation on the p-values. "],
["references.html", "References", " References "]
]
