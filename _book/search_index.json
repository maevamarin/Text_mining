[
["introduction.html", "Dicours Analysis Chapter 1 Introduction 1.1 Overview and Motivation", " Dicours Analysis Eugénie Mathieu, Maeva Marin, Hadrien Renger, Wajma Nazim 14 décembre, 2020 Chapter 1 Introduction 1.1 Overview and Motivation "],
["introduction-1.html", "Chapter 2 Introduction", " Chapter 2 Introduction Since the pandemic, the politicians’ adresses to their citizens have multiplied and our goal was to run a comparative analysis based on the speeches of the UK Prime Minister Boris Johnson and the French President Emmanuel Macron, translated in English. Knowing that politician discourses are always cautiously constructed to have the expected impact of the population, we assumed some specificities and discriminatory features among their speeches. Here, we would neglect the effect of a potential poor translation. Our motivation was to find significant differences in the sentence structure and in sentiments conveyed to confirm our basis hypothesis and to construct a classifier of speeches via a supervised learner. The aim of our project in to analyse the discourses of politician Boris Johnson and Emmanuel Macron and to see if there is any major differences in terms of the vocabulary used, the lengths of the speeches and the sentiments related to the various speeches. We proceeded in the following order, we first cleaned the data, using tokenisation and cleaning methods, we then tried to find statistical pattern using frequencies and scoring models and we then tried to interpret the results using a sentiment analysis. We need to highlight here that we used several methods, such as dimension reduction techniques and machine learning algorithms. "],
["eda.html", "Chapter 3 EDA 3.1 Data Acquisition 3.2 Tokenisation, Lemmatization &amp; Cleaning 3.3 Document-Term Matrix DTM 3.4 TF-IDF 3.5 Cloud of Words 3.6 Lexical Divesity Token Type Ratio TTR 3.7 Zipf’s Law 3.8 Yule’s index 3.9 MATTR", " Chapter 3 EDA 3.1 Data Acquisition To download the different speaches, we scrap the speaches from two different websites. The speaches from Macron come from the website of the Embassy of France in Washington DC (USA) and regarding Boris Johnson’s speaches, they come from the official website of the governement of the United-Kingdom. 3.1.1 Emmanuel Macron We choose the 3 first speaches from Macron about the corona virus dating from the: - 12 March (text1) - 16 March (text2) - 13 April (text3) # Data Acquisition Macron macron12march &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9654&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() macron12march &lt;- str_replace_all(macron12march,&quot;[\\r\\n\\t]&quot;, &quot;.&quot;) macron12march &lt;- substr(macron12march, 178, 20197) macron16march &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9659#1&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() macron16march &lt;- macron16march &lt;- str_replace_all(macron16march,&quot;[\\r\\n\\t]&quot;, &quot;.&quot;) macron16march &lt;- substr(macron16march, 131, 15719) macron13april &lt;- read_html(&quot;https://franceintheus.org/spip.php?article9710&quot;) %&gt;% html_nodes(&quot;div.texte&quot;) %&gt;% html_text() macron13april &lt;- macron13april &lt;- str_replace_all(macron13april,&quot;[\\r\\n\\t]&quot;, &quot;.&quot;) macron13april &lt;- substr(macron13april, 117, 20000) macron &lt;- corpus(c(macron12march,macron16march,macron13april)) kable(summary(macron), caption = &quot;Macron&#39;s speeches characteristics&quot; ,align = &quot;lccrr&quot;,digits = 4) Table 3.1: Macron’s speeches characteristics Text Types Tokens Sentences text1 988 3849 190 text2 909 3030 137 text3 1080 3945 184 The first speach of Macron was quite long , 162 sentences and the two next were shorter: 107 sentences. Each speech consists approximatly of an average of 3200 words. 3.1.2 Boris Johnson We choose the 7 first speaches of the Prime Minister Johnson about the corona virus dating from the: - 09 March (text1) - 12 March (text2) - 16 March (text3) - 18 March (text5) - 19 March (text6) - 20 March (text7) - 22 March (text8) 3.2 kable(summary(boris), caption = &quot;Johnson&#39;s speeches characteristics&quot; ,align = &quot;lccrr&quot;,digits = 4) Table 3.2: Johnson’s speeches characteristics Text Types Tokens Sentences text1 266 609 23 text2 409 1222 50 text3 405 1231 43 text4 406 1230 52 text5 321 994 37 text6 357 1030 47 text7 300 793 35 Johnson made more speeches but shorter. His first speech was 609 words, then the following ones ranged from 793 to 1222 words. 3.2 Tokenisation, Lemmatization &amp; Cleaning Numbers, punctuation, symbols and separators are removed, as well as unimportant words. Moreover, we cast all letters to their corresponding lower case version. We use lexicon to replace each token by its lemma. Here you can see the kept words words for each speech. 3.2.1 Emmanuel Macron ## Tokenization corpus_macron &lt;- corpus(macron) corpus_macron &lt;- tokens(corpus_macron, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ## Lemmatization corpus_macron &lt;- tokens_replace(corpus_macron, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning corpus_macron=corpus_macron %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) corpus_macron #&gt; Tokens consisting of 3 documents. #&gt; text1 : #&gt; [1] &quot;check&quot; &quot;delivery&quot; &quot;man&quot; &quot;woman&quot; &quot;france&quot; #&gt; [6] &quot;dear&quot; &quot;compatriot&quot; &quot;past&quot; &quot;week&quot; &quot;country&quot; #&gt; [11] &quot;confront&quot; &quot;spread&quot; #&gt; [ ... and 1,714 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;paris&quot; &quot;march&quot; &quot;woman&quot; &quot;man&quot; &quot;france&quot; #&gt; [6] &quot;thursday&quot; &quot;night&quot; &quot;speak&quot; &quot;health&quot; &quot;crisis&quot; #&gt; [11] &quot;country&quot; &quot;confront&quot; #&gt; [ ... and 1,312 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;paris&quot; &quot;april&quot; &quot;frenchwoman&quot; &quot;frenchman&quot; #&gt; [5] &quot;dear&quot; &quot;compatriot&quot; &quot;live&quot; &quot;difficult&quot; #&gt; [9] &quot;time&quot; &quot;feel&quot; &quot;fear&quot; &quot;distress&quot; #&gt; [ ... and 1,690 more ] 3.2.2 Boris Johnson ## Tokenization corpus_boris &lt;- corpus(boris) corpus_boris &lt;- tokens(corpus_boris, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ## Lemmatization corpus_boris &lt;- tokens_replace(corpus_boris, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning corpus_boris = corpus_boris %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) corpus_boris #&gt; Tokens consisting of 7 documents. #&gt; text1 : #&gt; [1] &quot;morning&quot; &quot;chair&quot; &quot;meet&quot; &quot;government&#39;s&quot; #&gt; [5] &quot;cobr&quot; &quot;emergency&quot; &quot;committee&quot; &quot;coronavirus&quot; #&gt; [9] &quot;outbreak&quot; &quot;first&quot; &quot;minister&quot; &quot;scotland&quot; #&gt; [ ... and 246 more ] #&gt; #&gt; text2 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; #&gt; [5] &quot;much&quot; &quot;come&quot; &quot;just&quot; &quot;chair&quot; #&gt; [9] &quot;meet&quot; &quot;government&#39;s&quot; &quot;emergency&quot; &quot;committee&quot; #&gt; [ ... and 501 more ] #&gt; #&gt; text3 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;everybody&quot; &quot;thank&quot; &quot;much&quot; #&gt; [6] &quot;come&quot; &quot;want&quot; &quot;bring&quot; &quot;everyone&quot; &quot;date&quot; #&gt; [11] &quot;national&quot; &quot;fight&quot; #&gt; [ ... and 477 more ] #&gt; #&gt; text4 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;indeed&quot; #&gt; [6] &quot;tune&quot; &quot;daily&quot; &quot;update&quot; &quot;want&quot; &quot;introduce&quot; #&gt; [11] &quot;sure&quot; &quot;know&quot; #&gt; [ ... and 532 more ] #&gt; #&gt; text5 : #&gt; [1] &quot;want&quot; &quot;begin&quot; &quot;thank&quot; &quot;everyone&quot; &quot;thank&quot; #&gt; [6] &quot;medium&quot; &quot;also&quot; &quot;thank&quot; &quot;everyone&quot; &quot;huge&quot; #&gt; [11] &quot;effort&quot; &quot;country&quot; #&gt; [ ... and 356 more ] #&gt; #&gt; text6 : #&gt; [1] &quot;good&quot; &quot;afternoon&quot; &quot;thank&quot; &quot;come&quot; &quot;today&quot; #&gt; [6] &quot;join&quot; &quot;chancellor&quot; &quot;exchequer&quot; &quot;rishi&quot; &quot;sunak&quot; #&gt; [11] &quot;jennie&quot; &quot;harry&quot; #&gt; [ ... and 392 more ] #&gt; #&gt; [ reached max_ndoc ... 1 more document ] 3.3 Document-Term Matrix DTM Now let’s compute the word frequencies (TF) by documents. First, the tokens are grouped by the indicator Document, which allows to count the words by documents. 3.3.1 Table Emmanuel Macron ## Document-Term Matrix DTM corpus_macron.dfm &lt;- dfm(corpus_macron) macron_dtm &lt;- VectorSource(corpus_macron) %&gt;% VCorpus() %&gt;% DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE)) macron_tidy &lt;- tidy(macron_dtm) datatable(macron_tidy, class = &quot;cell-border stripe&quot;) Boris Johnson ## Document-Term Matrix DTM corpus_boris.dfm &lt;- dfm(corpus_boris) boris_dtm &lt;- VectorSource(corpus_boris) %&gt;% VCorpus() %&gt;% DocumentTermMatrix(control=list(removePunctuation=TRUE, removeNumbers=TRUE, stopwords=TRUE)) boris_tidy &lt;- tidy(boris_dtm) datatable(boris_tidy, class = &quot;cell-border stripe&quot;) 3.3.2 Most frequent words We only keep the 15 most frequent words for redability purpose and create barplots using ggplot and geom_col. The facetwrap function split the barplots per Document. 3.3.2.1 All text confused «will» is the word the most used for Macron and Johnson. Emmanuel Macron In the figure 3.1 #top 15 macron_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) %&gt;% top_n(15) %&gt;% ggplot(aes(x=count, y=term, fill = term)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Frequency&quot;) + ylab(&quot;Words&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_y_reordered() Figure 3.1: 15 most common words in Macron’s speeches #top 16 mots plus utilisés par texte macron_count = macron_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) macron_index = top_n(macron_count, 15) macron_tidy %&gt;% filter(term %in% macron_index$term) %&gt;% ggplot(aes(x=term, y = count, fill =term)) + geom_col()+ xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + coord_flip()+ facet_wrap(~document, ncol=2) + guides(fill=FALSE, color=FALSE) Figure 3.2: Details of the more frequent words for Macron’s speeches Boris Johnson #top 15 boris_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) %&gt;% top_n(15) %&gt;% ggplot(aes(x=count, y=term, fill =term)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Frequency&quot;) + ylab(&quot;Words&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_y_reordered() Figure 3.3: 15 most common words in Johnson’s speeches #top 16 mots plus utilisés par texte boris_count = boris_tidy %&gt;% group_by(term) %&gt;% summarise(&quot;count&quot;=sum(`count`)) boris_index = top_n(boris_count, 15) boris_tidy %&gt;% filter(term %in% boris_index$term) %&gt;% ggplot(aes(x=term, y = count, fill=term)) + geom_col()+ xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + coord_flip()+ facet_wrap(~document, ncol=2)+ guides(fill=FALSE, color=FALSE) Figure 3.4: Details of the more frequent words for Johnson’s speeches We see that the list of the 15 most frequent terms is due to doc 2 ,4 and 3. 3.3.2.2 Per text Now we want to know which are the most frequent terms for each speach. Emmanuel Macron #top 15 par texte macron_tidy %&gt;% group_by(document) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(document = factor(as.numeric(document), levels = 1:17)) %&gt;% ggplot(aes(reorder_within(term, count, document), count, fill =term)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + coord_flip() + facet_wrap(~ document, scales = &quot;free&quot;) Figure 3.5: 15 most common words in each speach of Macron Boris Johnson #top 15 par texte boris_tidy %&gt;% group_by(document) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% mutate(document = factor(as.numeric(document), levels = 1:17)) %&gt;% ggplot(aes(reorder_within(term, count, document), count, fill =term)) + theme(legend.position = &quot;none&quot;) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_bar(stat = &quot;identity&quot;) + scale_x_reordered() + facet_wrap(~ document, scales = &quot;free&quot;) Figure 3.6: 15 most common words in each speach of Johnson 3.4 TF-IDF Now we repeat the same analysis using the TF-IDF. 3.4.1 Emmanuel Macron ## TFIDF no point when just on document, maybe add when combining texts corpus_macron.tfidf &lt;- dfm_tfidf(corpus_macron.dfm) #tfidf macron_index_tfidf = tidy(corpus_macron.tfidf) %&gt;% group_by(document) %&gt;% top_n(1) tidy(corpus_macron.tfidf) %&gt;% filter(term %in% macron_index_tfidf$term) %&gt;% ggplot( aes(term, count, fill=term)) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2)+ guides(fill=FALSE, color=FALSE) Figure 3.7: Most specific word for each Emmanuel Macron’s speeches 3.4.2 Boris Johnson ## TFIDF no point when just on document, maybe add when combining texts corpus_boris.tfidf &lt;- dfm_tfidf(corpus_boris.dfm) #tfidf boris_index_tfidf = tidy(corpus_boris.tfidf) %&gt;% group_by(document) %&gt;% top_n(1) tidy(corpus_boris.tfidf) %&gt;% filter(term %in% boris_index_tfidf$term) %&gt;% ggplot( aes(term, count,fill=term)) + xlab(&quot;Words&quot;) + ylab(&quot;Frequency&quot;) + geom_col()+ coord_flip()+ facet_wrap(~document, ncol=2)+ guides(fill=FALSE, color=FALSE) Figure 3.8: Most specific word for each Emmanuel Johnson’s speeches 3.5 Cloud of Words It is another method to see the most used words. The larger the word, the more frequently they are used. 3.5.1 Usind DFM Emmanuel macron textplot_wordcloud(corpus_macron.dfm, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 3.9: Cloud od Words of Macron’s speach with DFM Boris Johnson textplot_wordcloud(corpus_boris.dfm, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 3.10: Cloud od Words of Johnson’s speach with DFM 3.5.2 Using TF-IDF The tfidf matrix helps us to see the frequency of the tokens independently of the legnght of the documents. We then observe that some tokens are more specific to some documents. For example we see that for Macron speeches, the token war is specific to his 2nd speech and the token test is specific to his 3rd speech. ( see 3.7 ) Emmanuel macron textplot_wordcloud(corpus_macron.tfidf, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 3.11: Cloud od Words of Johnson’s speach with TI-IDF Boris Johnson textplot_wordcloud(corpus_boris.tfidf, color=brewer.pal(8, &quot;Dark2&quot;)) Figure 3.12: Cloud od Words of Johnson’s speach with TI-IDF 3.6 Lexical Divesity Token Type Ratio TTR A TTR is comprised beetween 0 and 1. When equal to 1, it corresponds to a rich lexical diversity, this is to say that each token is from a different type. In opposite, if equal to 0, it mean that the corpus presents a poor lexical diversity (if he would use one word only). 3.6.1 Emmanuel Macron N.macron &lt;- ntoken(corpus_macron) V.macron &lt;- ntype(corpus_macron) TTR.macron &lt;- V.macron/N.macron kable(TTR.macron, caption = &quot;Lexical diversity of Macron.&quot; ,align = &quot;lccrr&quot;,digits = 4) ###the text is quite poor, as TTR is of 0.4 Table 3.3: Lexical diversity of Macron. x text1 0.402 text2 0.480 text3 0.463 Macron has a mean TTR of 0,45, which is quite poor. 3.6.2 Boris Johnson ## Lexical Divesity Token Type Ratio TTR N.boris &lt;- ntoken(corpus_boris) V.boris &lt;- ntype(corpus_boris) TTR.boris &lt;- V.boris/N.boris kable(TTR.boris, caption = &quot;Lexical diversity of Johnson.&quot; ,align = &quot;lccrr&quot;,digits = 4) ###the text is quite rich, as TTR is of 0.6 Table 3.4: Lexical diversity of Johnson. x text1 0.674 text2 0.548 text3 0.558 text4 0.509 text5 0.576 text6 0.611 text7 0.611 Johnson has quite a richer vocabulary, an average of 0,6 over the different corpuses. 3.7 Zipf’s Law Now, we illustrate the Zipf’s law on the discourses. The terms are ranked by their corresponding frequency (rank=1 for the most frequent), then plotted versus tehir rank. This is easily obtained using quanteda. Using a log-log relation, this gives us a linear regression. 3.7.1 Emmanuel Macron corpus_macron_freq &lt;- textstat_frequency(corpus_macron.dfm) ggplot(corpus_macron_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) Figure 3.13: Zipf’s law for Macron plot(log(frequency)~log(rank), data=corpus_macron_freq, pch=20) Figure 3.14: log-log relation 3.7.2 Boris Johnson corpus_boris_freq &lt;- textstat_frequency(corpus_boris.dfm) ggplot(corpus_boris_freq,aes(x = rank, y = frequency, label=feature)) + geom_point(size=2, alpha =1) + theme_bw() + geom_text(aes(label=feature),hjust=0, vjust=0) + xlim(0,20) Figure 3.15: Zipf’s law for Johnson plot(log(frequency)~log(rank), data=corpus_boris_freq, pch=20) Figure 3.16: log-log relation 3.8 Yule’s index A larger index means more diversity. 3.8.1 Emmanuel Macron textstat_lexdiv(corpus_macron.dfm, measure = &quot;I&quot;) %&gt;% ggplot(aes(x=reorder(document,I), y=I))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;Yule&#39;s index&quot;) Figure 3.17: Yule’s index for Macron 3.8.2 Boris Johnson textstat_lexdiv(corpus_boris.dfm, measure = &quot;I&quot;) %&gt;% ggplot(aes(x=reorder(document,I), y=I))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;Yule&#39;s index&quot;) Figure 3.18: Yule’s index for Johnson 3.9 MATTR It is the Moving Average Type-Token Ratio. 3.9.1 Emmanuel Macron textstat_lexdiv(corpus_macron, measure = &quot;MATTR&quot;, MATTR_window = 10) %&gt;% ggplot(aes(x=reorder(document,MATTR), y=MATTR))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;MATTR&quot;) Figure 3.19: MATTR for Macron 3.9.2 Boris Johnson textstat_lexdiv(corpus_boris, measure = &quot;MATTR&quot;, MATTR_window = 10) %&gt;% ggplot(aes(x=reorder(document,MATTR), y=MATTR))+ geom_point()+ coord_flip()+ xlab(&quot;Text&quot;)+ ylab(&quot;MATTR&quot;) Figure 3.20: MATTR for Johnson "],
["sentiment-analysis.html", "Chapter 4 Sentiment Analysis 4.1 Analysis with the “nrc” library 4.2 Analysis with the LSD2015 dictionnary 4.3 Analysis with the “afinn” dictionnary", " Chapter 4 Sentiment Analysis 4.1 Analysis with the “nrc” library We use the “nrc” dictionary to start our sentiment analysis on the discourses of our two politicians. It is a dictionnary qualifying tokens by specific sentiments and by labelling them “negative” or “positive”. To do so, we will match the tokens of both corpuses with the dictionnary by applying an inner join. However, to use the inner_join function, we need a table object, what our objects are not primarily. We reload the data to create objects specific to this stage, boris_2 and macron_2, which are registered as tibble and allowing the use of the inner_join function. From the token list per document boris.tok, we join the corresponding qualifier in nrc using an inner_joint: ##################################################################################################################################### ########################################## Let&#39;s start with the Boris Johnson&#39;s discourses ###################################### ##################################################################################################################################### boris_2&lt;-as.tibble( c(boris9mars, boris12mars, boris16mars, boris18mars, boris19mars, boris20mars, boris22mars)) # trick to get a &quot;tbl_df&quot;,&quot;tbl&quot;,&quot;data.frame&quot; compatible with the inner_join function DocumentB &lt;- c(&quot;Text1&quot;,&quot;Text2&quot;,&quot;Text3&quot;,&quot;Text4&quot;,&quot;Text5&quot;,&quot;Text6&quot;,&quot;Text7&quot;) # adding a column &quot;Document&quot; to have a landmark for the tokens boris_2$Document &lt;- DocumentB boris_2 &lt;- boris_2[,c(2,1)] boris_2.tok &lt;- unnest_tokens(boris_2, output=&quot;word&quot;, input=&quot;value&quot;, to_lower=TRUE, strip_punct=TRUE, strip_numeric=TRUE) # unnest tokens of the table boris_2.sent&lt;- boris_2.tok %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) # do the inner join to merge the two tables ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron_2&lt;-as.tibble( c(macron12march, macron13april, macron16march)) # trick to get a &quot;tbl_df&quot;,&quot;tbl&quot;,&quot;data.frame&quot; compatible with the inner_join function DocumentM &lt;- c(&quot;Text1&quot;,&quot;Text2&quot;,&quot;Text3&quot;) # adding a column &quot;Document&quot; to have a landmark for the tokens macron_2$Document &lt;- DocumentM macron_2 &lt;- macron_2[,c(2,1)] macron_2.tok &lt;- unnest_tokens(macron_2, output=&quot;word&quot;, input=&quot;value&quot;, to_lower=TRUE, strip_punct=TRUE, strip_numeric=TRUE) # unnest tokens of the table macron_2.sent&lt;- macron_2.tok %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) # do the inner join to merge the two tables After creating our objects, we investigate which sentiment are present in Boris Johnson’s discourses. To do so, we use a numerical and a graphical method. The numerical method is simply a matrix of the frequency of tokens identified to a certain sentiment. Remind that a word can have more than 1 sentiment, which can lead to slight an overestimation of the sentiment. Below, the table presents positive discourses from the UK’s First Minister which are mainly weighted by the sentiment “trust”. We could have assumed that the discourse would be reassuring in order to avoid any panic due to the inedite circumstances of the covid. “Anticipation” is as well high, for the same reason (annoucement of the futures measures ans aaniticpations of the consequences, e.g.) However, postive sentiments are balanced by the relative high score of the fear, followed by disgust and anger. In absolute values, the most sentimental discourse was the second public word, on March 12th 2020. The graphical representations of the sentiments among discourses enables us a quick glimpse on these results. table(boris_2.sent$Document,boris_2.sent$sentiment) %&gt;% kable() %&gt;% kable_styling() # sentiment terms per document anger anticipation disgust fear joy negative positive sadness surprise trust Text1 7 14 5 15 4 14 33 7 4 24 Text2 16 33 17 33 8 35 63 22 7 45 Text3 12 16 14 25 10 23 46 15 8 31 Text4 11 24 8 14 9 20 50 11 8 35 Text5 11 18 10 19 8 24 30 12 5 19 Text6 8 17 6 14 9 22 45 8 2 35 Text7 3 13 4 12 8 13 26 4 3 23 The graphical representation of the sentiments among discourses enables us a quick glimpse on these results. boris_2.sent %&gt;% group_by(Document,sentiment) %&gt;% summarize(n=n())%&gt;% mutate(freq=n/sum(n)) %&gt;% ggplot(aes(x=sentiment,y=freq,fill=sentiment)) + geom_bar(stat=&quot;identity&quot;,alpha=0.8) + facet_wrap(~ Document) + coord_flip() + ggtitle(&quot;Boris Johnson: Graphical representation of the sentiment per text&quot;) + xlab(&quot;Frequencies of the sentiments&quot;) + ylab(&quot;Sentiment&quot;) + geom_text(aes(label = n), size = 3, hjust = 1, vjust = 0, position = &quot;stack&quot;) Figure 4.1: Boris Johnson: Graphical representation of the sentiment per text By looking now at the discourses of Macron using the same method, we note the seemingly same frequencies of sentiments in the below table. His words appear to be carefully built and the variance is very low, perhaps to give the impression to have a stable and coherent speeches overtime. The categories of sentiments are quite similarly distributed: the speeches are positive marked by trust and anticipation. table(macron_2.sent$Document,macron_2.sent$sentiment) %&gt;% kable() %&gt;% kable_styling() # sentiment terms per document anger anticipation disgust fear joy negative positive sadness surprise trust Text1 26 88 16 73 39 109 196 43 33 122 Text2 29 96 21 63 46 105 188 44 31 107 Text3 25 63 10 52 25 92 149 30 22 102 macron_2.sent %&gt;% group_by(Document,sentiment) %&gt;% summarize(n=n())%&gt;% mutate(freq=n/sum(n)) %&gt;% ggplot(aes(x=sentiment,y=freq,fill=sentiment)) + geom_bar(stat=&quot;identity&quot;,alpha=0.8) + facet_wrap(~ Document) + coord_flip() + ggtitle(&quot;Graphical representation of the sentiment per text&quot;) + xlab(&quot;Frequencies of the sentiments&quot;) + ylab(&quot;Sentiment&quot;) + geom_text(aes(label = n), size =3, hjust = 1, vjust = 0, position = &quot;stack&quot;) 4.2 Analysis with the LSD2015 dictionnary In order to better analyse those results and fortify those insights, we double check with the dictionnary LSD2015. It is another dictionnary assigning a qualifier to terms. The difference with tidytext is essentially in the manipulation of the objects: it handles the “tokens” class and for this reason we have to recode our objects. The results are impacted by this different treatment and we do expect slight changes. As the figure 4.2 shows, the results of Text 2 as the most positive and all the discourses as majoritarily positive are confirmed. For Macron’s speeches, the proportions of sentiments differs: the first discourse is associated with more postive sentiment (224) relatively to proportion computed with the “nrc” dictionnary (193). However, the trend remains the same. We keep in mind that difference in length of texts explains the varying size of the bar, as previously. Anyway both findings converge to the same points. ###################################################################################################################################### ########################################## Let&#39;s start with the Johnson&#39;s discourses ########################################### ###################################################################################################################################### boris.cp&lt;-corpus(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) summary(boris.cp) Text Types Tokens Sentences text1 266 609 23 text2 409 1222 50 text3 405 1231 43 text4 406 1230 52 text5 321 994 37 text6 357 1030 47 text7 300 793 35 boris.tk&lt;-tokens(boris.cp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) boris.tk&lt;-tokens_tolower(boris.tk) boris.tk&lt;- tokens_replace(boris.tk,pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) boris.tk&lt;-boris.tk %&gt;% tokens_remove(stopwords(&quot;english&quot;)) boris.sent&lt;- tokens_lookup(boris.tk,dictionary = data_dictionary_LSD2015) %&gt;% dfm() %&gt;% tidy boris.plot.quanteda &lt;- ggplot(boris.sent, aes( x = document, y = count, fill = term)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + ggtitle(&quot;Johnson: Proportion of sentiment using the dictionnary LSD2015&quot;) + xlab(&quot;Document&quot;) + ylab(&quot;Number of terms attributed to negative and positive sentiments&quot;) + geom_text(aes(label = count), size =3, hjust = 5, vjust = 0, position = &quot;stack&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron.cp&lt;-corpus(c(macron12march,macron13april,macron16march)) macron.tk&lt;-tokens(macron.cp, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) macron.tk&lt;-tokens_tolower(macron.tk) macron.tk&lt;- tokens_replace(macron.tk,pattern = hash_lemmas$token, replacement = hash_lemmas$lemma) macron.tk&lt;-macron.tk %&gt;% tokens_remove(stopwords(&quot;english&quot;)) macron.sent&lt;- tokens_lookup(macron.tk,dictionary = data_dictionary_LSD2015) %&gt;% dfm() %&gt;% tidy macron.plot.quanteda&lt;- ggplot(macron.sent, aes( x = document, y = count, fill = term)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + ggtitle(&quot;Macron: Proportion of sentiment using the dictionnary LSD2015&quot;) + xlab(&quot;Document&quot;) + ylab(&quot;Number of terms attributed to negative and positive sentiments&quot;) + geom_text(aes(label = count), size =3, hjust = 5, vjust = 0, position = &quot;stack&quot;) grid.arrange(boris.plot.quanteda,macron.plot.quanteda) Figure 4.2: Proportion of sentiment using the dictionnary LSD2015 4.3 Analysis with the “afinn” dictionnary We now use a different approach: a quantitative way to assess the sentiment analysis. To do so, we use the “afinn” dictionnary which attributes a value to the word, taking into account the power conveyed by the term (value between 0 and 1) and its qualitative classification (positive or neagtive sign). The classification of the words was encoded differently than for the “nrc” dictionnary. Again, we do expect different scores, but hopefully in the same direction. The results displayed by the figure 4.3 are derived from an average score of sentiment per document. For the Boris Johnson’s speeches, the second one has a contradictory score with the previous results: it belongs to the lowest scored text in positive sentiments. We can thus observe the difference in encodage among dictionnaries used in Text Mining. The “afinn” dictionnarywas encoded by a Danish Professor, LSD2015 by two American professors, and “nrc” by a Canadian professors working in an Commision on Ethics. The categorization is subjective and lead to substantial differences. Those results need to be mitigate by a last approach: the use of valence shifters. ###################################################################################################################################### ########################################## Let&#39;s continue with the Johnson&#39;s discourses ########################################### ###################################################################################################################################### boris_2.sent &lt;- boris_2.tok %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) boris.plot.afinn&lt;-aggregate(value~Document, data =boris_2.sent,FUN=mean) %&gt;% ggplot(aes(x=Document,y=value, fill = Document)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + ggtitle(&quot;Johnson: Sentiment score per text by using afinn dictionnary&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Score value&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron_2.sent &lt;- macron_2.tok %&gt;% inner_join(get_sentiments(&quot;afinn&quot;)) macron.plot.afinn &lt;- aggregate(value~Document, data =macron_2.sent,FUN=mean) %&gt;% ggplot(aes(x=Document,y=value, fill = Document)) + geom_bar(stat=&quot;identity&quot;) + coord_flip() + ggtitle(&quot;Macron: Sentiment score per text by using afinn dictionnary&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Score value&quot;) grid.arrange(boris.plot.afinn,macron.plot.afinn) Figure 4.3: Graphical representation: Sentiment score per text by using afinn dictionnary 4.3.1 Analysis using “nrc”&quot; dictionnary and valence shifters The sentimentr library offers some function to compute sentiments integrating valence shiters. There are specific words which amplify or reduce the power of a word, even turn it into the reverse sentiment. We evaluate sentences here, and one important aspect is that it cannot be applied to a Bag Of Word model, since the word order is necessary. The valence shifters extract more acurately the sentiment ouf of the text, since it consider the sentiment conveyed by a sentence and not only words without context. We should take the insights given by valence shifters as the ultimate confirmation test for the previous results. The results are displayed by the plot 4.4. The second discourse of the Prime Minister here is one of the longest: it contains about 50 sentences and is only preceded by the fourth public word, on March 18th. It is the one varying the most in sentiment, what might explains the dffference in values observed between LSD2015&amp;nrc with afinn dictionnary. The sixth one shows one positive peak, what correspond to the moment whn Boris Johnson strengthened the measures and made a speech to reassure UK citizen. He brought information with positive sentiments to unify the population under the new circumstances and to push the to obey by showing them the positive impact of respecting those stonger measures. Macron’s speeches are well longer, three times more than Boris Johnson’s ones, as we observe in the second plot. His words are generally more neutral but convey sometimes strong negative sentiments, like the 4th sentence. Actually, this sentence is not strongly negative but uses terms which are: “In the vast majority of cases, COVID-19 does not pose a threat, but the virus can have very serious consequences, especially for those of our fellow citizens who are elderly or suffer from chronic illnesses such as diabetes, obesity or cancer”. The variation of the sentiments is greater in the last allocution, on April 13th 2020. boris.text&lt;-get_sentences(boris) boris.senti&lt;-sentiment(boris.text) boris.senti&lt;-as_tibble(boris.senti) boris.senti element_id sentence_id word_count sentiment 1 1 NA 0.000 1 2 17 -0.497 1 3 34 0.334 1 4 15 0.258 1 5 12 0.217 1 6 22 0.000 1 7 23 -0.209 1 8 45 -0.224 1 9 23 -0.386 1 10 22 -0.032 1 11 32 0.101 1 12 27 -0.048 1 13 22 0.000 1 14 57 0.479 1 15 47 0.153 1 16 35 0.122 1 17 22 0.213 1 18 25 -0.198 1 19 32 0.424 1 20 20 0.000 2 1 NA 0.000 2 2 30 0.475 2 3 24 0.184 2 4 18 -0.118 2 5 7 -0.038 2 6 56 0.000 2 7 7 -0.132 2 8 5 -0.537 2 9 11 -1.116 2 10 30 0.228 2 11 18 0.448 2 12 21 0.164 2 13 12 0.000 2 14 26 -0.968 2 15 44 0.232 2 16 24 -0.041 2 17 11 0.000 2 18 13 0.222 2 19 39 -0.304 2 20 18 -0.109 2 21 9 0.200 2 22 39 -0.244 2 23 22 0.341 2 24 35 0.135 2 25 25 0.050 2 26 14 0.107 2 27 25 0.080 2 28 17 -0.728 2 29 31 0.045 2 30 21 0.349 2 31 8 0.000 2 32 15 0.103 2 33 18 -0.236 2 34 12 0.000 2 35 5 0.000 2 36 19 -0.252 2 37 16 0.038 2 38 13 0.333 2 39 55 -0.302 2 40 12 0.000 2 41 33 -0.736 2 42 34 -0.026 2 43 16 0.125 2 44 25 -0.100 2 45 35 0.617 2 46 31 0.636 2 47 35 0.536 2 48 51 0.070 3 1 NA 0.000 3 2 9 0.550 3 3 33 0.052 3 4 35 0.101 3 5 12 0.260 3 6 58 0.276 3 7 42 0.131 3 8 30 0.210 3 9 13 0.000 3 10 26 -0.020 3 11 48 -0.144 3 12 12 0.072 3 13 12 -0.144 3 14 16 0.000 3 15 33 0.374 3 16 17 0.000 3 17 7 -0.340 3 18 19 -0.229 3 19 44 -0.077 3 20 14 -0.241 3 21 38 0.143 3 22 43 -0.195 3 23 22 0.128 3 24 23 0.073 3 25 98 0.203 3 26 27 0.520 3 27 29 -0.056 3 28 21 0.000 3 29 10 -0.079 3 30 31 0.081 3 31 38 -0.162 3 32 69 -0.066 3 33 48 0.180 3 34 11 0.000 3 35 17 0.303 3 36 46 0.258 4 1 NA 0.000 4 2 2 0.530 4 3 13 0.166 4 4 26 0.127 4 5 16 0.000 4 6 33 -0.061 4 7 12 0.000 4 8 26 0.446 4 9 14 -0.107 4 10 12 -0.208 4 11 20 0.179 4 12 19 -0.195 4 13 3 0.000 4 14 28 0.104 4 15 22 0.399 4 16 22 -0.053 4 17 26 0.569 4 18 23 0.115 4 19 25 -0.300 4 20 12 0.390 4 21 14 -0.601 4 22 29 -0.093 4 23 27 0.486 4 24 35 -0.338 4 25 40 0.103 4 26 12 0.130 4 27 21 0.098 4 28 24 0.000 4 29 15 0.168 4 30 40 0.300 4 31 15 0.000 4 32 23 -0.104 4 33 12 -0.260 4 34 26 -0.245 4 35 21 -0.257 4 36 23 0.271 4 37 16 0.325 4 38 19 0.298 4 39 NA 0.000 4 40 18 0.000 4 41 18 0.153 4 42 14 -0.214 4 43 21 -0.055 4 44 53 0.240 4 45 44 -0.475 4 46 13 0.000 4 47 17 0.121 4 48 31 0.521 4 49 19 -0.138 4 50 18 0.377 4 51 23 -0.209 4 52 16 0.631 4 53 14 0.000 5 1 NA 0.000 5 2 36 0.500 5 3 8 0.000 5 4 19 0.000 5 5 22 0.213 5 6 33 -0.026 5 7 26 0.000 5 8 29 0.000 5 9 17 0.000 5 10 29 0.119 5 11 15 0.103 5 12 20 0.000 5 13 29 0.214 5 14 42 0.855 5 15 21 -0.109 5 16 15 0.103 5 17 9 0.267 5 18 10 -1.028 5 19 10 -0.079 5 20 28 -0.094 5 21 13 0.000 5 22 12 0.000 5 23 6 0.000 5 24 9 -0.333 5 25 39 0.090 5 26 34 -0.043 5 27 20 0.000 5 28 14 0.000 5 29 28 -0.525 5 30 27 0.000 5 31 40 0.063 5 32 21 -0.175 5 33 4 -0.100 5 34 19 0.229 5 35 7 0.094 5 36 6 0.000 5 37 23 0.292 5 38 31 0.256 5 39 28 -0.189 5 40 18 0.000 5 41 17 0.000 5 42 32 0.354 6 1 NA 0.000 6 2 8 0.442 6 3 19 0.138 6 4 17 0.024 6 5 8 0.283 6 6 8 0.000 6 7 35 0.118 6 8 25 -0.522 6 9 14 0.267 6 10 13 -0.111 6 11 20 0.045 6 12 7 -0.189 6 13 8 0.088 6 14 4 0.000 6 15 6 0.102 6 16 6 -0.204 6 17 19 -0.023 6 18 49 0.129 6 19 9 0.267 6 20 33 -0.084 6 21 19 -0.057 6 22 14 -0.134 6 23 14 -0.027 6 24 10 0.000 6 25 25 -0.150 6 26 25 -0.050 6 27 27 0.106 6 28 4 0.000 6 29 30 0.571 6 30 29 0.167 6 31 20 -0.045 6 32 15 0.000 6 33 16 0.188 6 34 23 0.229 6 35 50 0.354 6 36 18 -0.265 6 37 23 -0.156 6 38 19 0.000 6 39 4 1.125 6 40 32 -0.272 6 41 26 0.245 6 42 22 0.213 6 43 18 0.424 6 44 6 0.000 6 45 12 -0.144 6 46 32 0.221 7 1 NA 0.000 7 2 3 0.433 7 3 22 0.213 7 4 19 0.156 7 5 34 0.454 7 6 15 0.000 7 7 19 0.000 7 8 10 0.032 7 9 30 0.037 7 10 12 0.217 7 11 26 -0.245 7 12 40 0.134 7 13 27 -0.096 7 14 12 0.361 7 15 15 0.000 7 16 7 0.000 7 17 13 0.000 7 18 19 0.115 7 19 7 0.000 7 20 12 -0.577 7 21 23 0.000 7 22 13 0.250 7 23 19 -0.138 7 24 36 0.000 7 25 30 0.301 7 26 18 0.648 7 27 15 0.000 7 28 26 -0.206 7 29 19 -0.041 7 30 35 0.237 7 31 21 -0.109 7 32 16 -0.312 7 33 29 0.262 7 34 14 0.000 7 35 8 0.000 boris.senti%&gt;% group_by(element_id) %&gt;% ggplot(aes(x=sentence_id,y=sentiment, col = element_id )) + geom_line() + facet_wrap(~element_id) + ggtitle(&quot;Johnson: Evolution of the sentiments \\nwithin speeches using valence shifters&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Sentences in speeches&quot;) ###################################################################################################################################### ########################################## Let&#39;s continue with the Macron&#39;s discourses ########################################### ###################################################################################################################################### macron.text&lt;-get_sentences(macron) macron.senti&lt;-sentiment(macron.text) sentiment(macron.text, question.weight = 0) element_id sentence_id word_count sentiment 1 1 3 0.346 1 2 8 0.177 1 3 26 -0.137 1 4 20 0.034 1 5 24 -0.131 1 6 41 -1.325 1 7 12 0.072 1 8 27 0.000 1 9 29 -0.371 1 10 19 0.115 1 11 52 0.653 1 12 10 0.395 1 13 24 -0.020 1 14 12 -0.447 1 15 24 0.388 1 16 7 0.189 1 17 60 0.400 1 18 7 0.000 1 19 10 0.111 1 20 8 -0.354 1 21 42 0.108 1 22 5 0.000 1 23 12 0.361 1 24 15 0.129 1 25 37 0.329 1 26 26 -0.098 1 27 8 -0.265 1 28 15 -0.426 1 29 11 0.754 1 30 37 0.214 1 31 34 -0.054 1 32 19 -0.133 1 33 37 -0.378 1 34 4 0.000 1 35 16 -0.125 1 36 8 0.230 1 37 31 -0.404 1 38 29 0.183 1 39 21 0.000 1 40 17 -0.424 1 41 26 0.255 1 42 17 0.094 1 43 16 0.250 1 44 17 0.061 1 45 34 0.120 1 46 28 0.546 1 47 18 0.377 1 48 8 -0.141 1 49 1 0.000 1 50 32 -0.194 1 51 36 0.108 1 52 61 0.045 1 53 17 0.061 1 54 49 0.507 1 55 21 -0.055 1 56 13 0.139 1 57 15 0.026 1 58 9 0.000 1 59 18 0.012 1 60 22 -0.128 1 61 13 -0.208 1 62 26 0.475 1 63 8 0.194 1 64 18 0.236 1 65 8 0.141 1 66 8 0.354 1 67 6 0.000 1 68 6 0.408 1 69 64 0.325 1 70 27 0.327 1 71 6 0.000 1 72 21 0.404 1 73 64 0.388 1 74 9 0.000 1 75 22 0.927 1 76 20 0.637 1 77 21 0.109 1 78 4 0.300 1 79 26 0.284 1 80 16 0.462 1 81 9 0.000 1 82 13 0.413 1 83 15 0.103 1 84 15 -0.697 1 85 33 -0.087 1 86 14 -0.267 1 87 18 -0.094 1 88 19 -0.344 1 89 7 0.076 1 90 16 -0.088 1 91 30 -0.037 1 92 19 0.344 1 93 15 0.207 1 94 8 0.000 1 95 7 0.000 1 96 10 -0.126 1 97 23 0.849 1 98 36 0.133 1 99 12 0.217 1 100 14 0.347 1 101 28 0.283 1 102 25 -0.550 1 103 11 0.075 1 104 20 0.000 1 105 10 0.237 1 106 7 0.283 1 107 29 0.121 1 108 7 0.302 1 109 10 0.253 1 110 4 0.000 1 111 5 0.000 1 112 11 0.000 1 113 23 0.344 1 114 6 -0.163 1 115 16 0.125 1 116 17 0.158 1 117 9 0.400 1 118 24 0.122 1 119 18 0.460 1 120 24 0.000 1 121 23 0.083 1 122 32 0.256 1 123 20 0.291 1 124 16 0.162 1 125 20 -0.078 1 126 12 0.000 1 127 30 0.228 1 128 18 0.059 1 129 15 0.129 1 130 7 -0.189 1 131 10 0.411 1 132 7 0.397 1 133 10 0.569 1 134 20 0.615 1 135 8 0.000 1 136 12 0.115 1 137 45 0.861 1 138 14 -0.748 1 139 9 0.133 1 140 24 -0.214 1 141 23 0.000 1 142 19 -0.252 1 143 6 0.000 1 144 25 -0.180 1 145 6 -0.408 1 146 13 -0.347 1 147 17 -0.121 1 148 9 0.000 1 149 7 -0.283 1 150 3 0.289 1 151 13 0.277 1 152 14 0.347 1 153 17 0.024 1 154 5 0.179 1 155 11 0.166 1 156 5 0.000 1 157 12 -0.014 1 158 39 0.328 1 159 17 0.061 1 160 15 0.491 1 161 18 0.059 1 162 14 0.388 1 163 7 0.189 1 164 8 0.000 1 165 19 0.344 1 166 16 0.000 1 167 15 -0.129 1 168 24 0.000 1 169 34 0.214 1 170 7 0.000 1 171 27 0.241 1 172 16 0.125 1 173 29 0.093 1 174 22 0.426 1 175 44 0.030 1 176 36 -0.067 1 177 22 -0.085 1 178 19 0.619 1 179 33 0.104 1 180 12 0.000 1 181 6 0.000 1 182 16 0.300 1 183 40 0.032 1 184 15 0.194 1 185 4 0.000 2 1 2 0.000 2 2 5 0.000 2 3 16 -0.188 2 4 23 -0.156 2 5 16 -0.188 2 6 12 0.173 2 7 22 -0.128 2 8 7 -0.302 2 9 17 0.121 2 10 26 0.559 2 11 35 0.355 2 12 7 0.302 2 13 32 0.318 2 14 34 0.086 2 15 21 0.218 2 16 13 0.111 2 17 42 -0.085 2 18 7 0.000 2 19 50 0.404 2 20 11 -0.030 2 21 26 0.029 2 22 24 0.245 2 23 40 0.245 2 24 18 0.000 2 25 41 0.472 2 26 9 0.027 2 27 43 -0.380 2 28 9 0.167 2 29 23 0.031 2 30 10 0.111 2 31 38 0.008 2 32 14 0.000 2 33 18 0.000 2 34 17 0.000 2 35 12 0.043 2 36 75 0.237 2 37 43 0.244 2 38 17 0.255 2 39 10 -0.079 2 40 38 0.105 2 41 6 0.551 2 42 18 0.071 2 43 14 0.167 2 44 12 0.289 2 45 34 -0.026 2 46 15 0.090 2 47 6 0.367 2 48 14 0.267 2 49 18 -0.247 2 50 48 0.707 2 51 10 0.000 2 52 11 0.226 2 53 12 -0.231 2 54 8 0.177 2 55 9 0.517 2 56 16 0.375 2 57 4 0.400 2 58 10 0.000 2 59 12 0.217 2 60 11 0.407 2 61 10 -0.142 2 62 15 0.478 2 63 22 -0.048 2 64 17 -0.049 2 65 12 -0.794 2 66 6 -0.102 2 67 4 -0.250 2 68 16 -0.250 2 69 10 0.000 2 70 20 0.101 2 71 33 -0.313 2 72 9 0.000 2 73 38 0.300 2 74 16 0.250 2 75 4 -0.250 2 76 35 0.127 2 77 4 -0.250 2 78 31 0.099 2 79 4 0.000 2 80 8 0.371 2 81 4 0.000 2 82 16 0.400 2 83 37 -0.033 2 84 23 0.000 2 85 15 0.000 2 86 14 0.294 2 87 26 0.000 2 88 8 0.265 2 89 17 0.194 2 90 9 0.067 2 91 18 0.118 2 92 37 0.205 2 93 15 0.000 2 94 23 0.000 2 95 28 -0.180 2 96 34 0.718 2 97 14 0.000 2 98 18 -0.141 2 99 19 0.184 2 100 26 0.147 2 101 35 0.355 2 102 19 -0.321 2 103 20 -0.369 2 104 22 -0.205 2 105 43 -0.290 2 106 23 -0.646 2 107 11 0.151 2 108 41 0.226 2 109 29 0.111 2 110 52 -0.125 2 111 30 0.256 2 112 11 0.090 2 113 20 -0.056 2 114 34 0.475 2 115 11 -0.121 2 116 7 -0.189 2 117 31 -0.180 2 118 42 0.548 2 119 5 0.000 2 120 41 0.372 2 121 48 0.217 2 122 13 0.485 2 123 52 0.582 2 124 13 0.607 2 125 15 0.129 2 126 7 -0.189 2 127 8 0.088 2 128 26 0.343 2 129 23 -0.021 2 130 9 0.033 2 131 8 0.177 3 1 4 0.000 3 2 3 0.289 3 3 6 -0.204 3 4 24 -0.704 3 5 10 -0.791 3 6 39 0.192 3 7 35 -0.279 3 8 11 0.407 3 9 38 0.260 3 10 3 0.000 3 11 15 0.194 3 12 21 0.262 3 13 50 0.219 3 14 6 0.163 3 15 48 -0.051 3 16 4 0.125 3 17 5 0.000 3 18 13 0.277 3 19 26 0.000 3 20 14 0.134 3 21 48 -0.260 3 22 15 0.000 3 23 28 -0.104 3 24 21 -0.098 3 25 23 0.349 3 26 4 0.000 3 27 55 0.034 3 28 8 0.530 3 29 20 0.224 3 30 18 -0.795 3 31 13 0.000 3 32 113 0.461 3 33 17 -0.121 3 34 15 0.349 3 35 14 0.000 3 36 37 0.041 3 37 13 0.839 3 38 13 0.139 3 39 21 0.055 3 40 12 0.231 3 41 11 0.814 3 42 12 -0.029 3 43 11 0.030 3 44 29 0.474 3 45 29 -0.078 3 46 12 0.260 3 47 17 0.218 3 48 15 -0.181 3 49 34 0.223 3 50 18 0.118 3 51 23 0.083 3 52 27 -0.500 3 53 20 -0.447 3 54 27 0.250 3 55 10 0.158 3 56 11 -0.404 3 57 29 -0.046 3 58 25 0.040 3 59 18 0.318 3 60 59 0.358 3 61 28 0.028 3 62 5 0.000 3 63 17 0.182 3 64 25 0.420 3 65 10 0.079 3 66 11 -0.965 3 67 18 0.236 3 68 29 0.251 3 69 20 -0.123 3 70 22 0.320 3 71 12 0.375 3 72 23 0.261 3 73 12 0.000 3 74 13 -0.180 3 75 33 -0.566 3 76 13 -0.555 3 77 13 0.000 3 78 26 0.078 3 79 13 0.000 3 80 14 0.067 3 81 29 0.046 3 82 23 0.188 3 83 4 0.300 3 84 19 0.092 3 85 20 0.089 3 86 21 0.109 3 87 29 -0.390 3 88 7 -0.151 3 89 25 0.150 3 90 17 0.303 3 91 18 0.306 3 92 19 0.241 3 93 20 0.201 3 94 10 0.000 3 95 23 -0.188 3 96 24 0.082 3 97 11 -0.121 3 98 15 0.000 3 99 10 -0.285 3 100 16 0.125 3 101 45 0.447 3 102 7 0.000 3 103 17 0.206 3 104 20 0.531 3 105 12 0.173 3 106 12 0.000 3 107 11 0.226 3 108 43 0.221 3 109 19 -0.206 3 110 28 0.227 3 111 26 0.029 3 112 25 0.160 3 113 9 0.000 3 114 12 0.000 3 115 7 0.189 3 116 3 0.462 3 117 13 -0.555 3 118 38 -0.032 3 119 33 -0.374 3 120 12 -0.217 3 121 11 0.377 3 122 29 0.511 3 123 33 0.174 3 124 5 -0.112 3 125 9 0.083 3 126 10 0.000 3 127 19 0.103 3 128 23 0.302 3 129 16 -0.125 3 130 16 0.000 3 131 10 0.000 3 132 9 -0.100 3 133 19 0.057 3 134 15 -0.161 3 135 11 0.226 3 136 30 -0.128 3 137 8 0.530 3 138 23 0.480 3 139 28 -0.333 3 140 41 0.078 3 141 12 0.260 3 142 19 0.258 3 143 11 0.226 3 144 13 0.347 3 145 7 0.189 3 146 24 -0.082 3 147 19 0.379 3 148 28 0.151 3 149 18 0.997 3 150 29 -0.416 3 151 21 -0.371 3 152 22 0.341 3 153 16 -0.100 3 154 26 0.353 3 155 19 0.578 3 156 15 0.000 3 157 24 -0.153 3 158 9 0.027 3 159 10 0.000 3 160 14 -0.160 3 161 32 -0.071 3 162 14 -0.274 3 163 24 -0.122 3 164 6 0.000 3 165 12 -0.289 3 166 16 0.000 3 167 1 0.000 3 168 16 0.150 3 169 26 0.392 3 170 10 0.126 3 171 23 0.209 3 172 8 0.460 3 173 4 0.000 3 174 27 0.308 3 175 3 0.577 macron.senti&lt;-as_tibble(macron.senti) macron.senti element_id sentence_id word_count sentiment 1 1 3 0.346 1 2 8 0.177 1 3 26 -0.137 1 4 20 0.034 1 5 24 -0.131 1 6 41 -1.325 1 7 12 0.072 1 8 27 0.000 1 9 29 -0.371 1 10 19 0.115 1 11 52 0.653 1 12 10 0.395 1 13 24 -0.020 1 14 12 -0.447 1 15 24 0.388 1 16 7 0.189 1 17 60 0.400 1 18 7 0.000 1 19 10 0.111 1 20 8 -0.354 1 21 42 0.108 1 22 5 0.000 1 23 12 0.361 1 24 15 0.129 1 25 37 0.329 1 26 26 -0.098 1 27 8 -0.265 1 28 15 -0.426 1 29 11 0.754 1 30 37 0.214 1 31 34 -0.054 1 32 19 -0.133 1 33 37 -0.378 1 34 4 0.000 1 35 16 -0.125 1 36 8 0.230 1 37 31 -0.404 1 38 29 0.183 1 39 21 0.000 1 40 17 -0.424 1 41 26 0.255 1 42 17 0.094 1 43 16 0.250 1 44 17 0.061 1 45 34 0.120 1 46 28 0.546 1 47 18 0.377 1 48 8 -0.141 1 49 1 0.000 1 50 32 -0.194 1 51 36 0.108 1 52 61 0.045 1 53 17 0.061 1 54 49 0.507 1 55 21 -0.055 1 56 13 0.139 1 57 15 0.026 1 58 9 0.000 1 59 18 0.012 1 60 22 -0.128 1 61 13 -0.208 1 62 26 0.475 1 63 8 0.194 1 64 18 0.236 1 65 8 0.141 1 66 8 0.354 1 67 6 0.000 1 68 6 0.408 1 69 64 0.325 1 70 27 0.327 1 71 6 0.000 1 72 21 0.404 1 73 64 0.388 1 74 9 0.000 1 75 22 0.927 1 76 20 0.637 1 77 21 0.109 1 78 4 0.300 1 79 26 0.284 1 80 16 0.462 1 81 9 0.000 1 82 13 0.413 1 83 15 0.103 1 84 15 -0.697 1 85 33 -0.087 1 86 14 -0.267 1 87 18 -0.094 1 88 19 -0.344 1 89 7 0.076 1 90 16 -0.088 1 91 30 -0.037 1 92 19 0.344 1 93 15 0.207 1 94 8 0.000 1 95 7 0.000 1 96 10 -0.126 1 97 23 0.849 1 98 36 0.133 1 99 12 0.217 1 100 14 0.347 1 101 28 0.283 1 102 25 -0.550 1 103 11 0.075 1 104 20 0.000 1 105 10 0.237 1 106 7 0.283 1 107 29 0.121 1 108 7 0.302 1 109 10 0.253 1 110 4 -0.125 1 111 5 0.000 1 112 11 0.000 1 113 23 0.344 1 114 6 -0.163 1 115 16 0.125 1 116 17 0.158 1 117 9 0.400 1 118 24 0.122 1 119 18 0.460 1 120 24 0.000 1 121 23 0.083 1 122 32 0.256 1 123 20 0.291 1 124 16 0.162 1 125 20 -0.078 1 126 12 0.000 1 127 30 0.228 1 128 18 0.059 1 129 15 0.129 1 130 7 -0.189 1 131 10 0.411 1 132 7 0.397 1 133 10 0.569 1 134 20 0.615 1 135 8 0.000 1 136 12 0.115 1 137 45 0.861 1 138 14 -0.748 1 139 9 0.133 1 140 24 -0.214 1 141 23 0.000 1 142 19 -0.252 1 143 6 0.000 1 144 25 -0.180 1 145 6 -0.408 1 146 13 -0.347 1 147 17 -0.121 1 148 9 0.000 1 149 7 -0.283 1 150 3 0.289 1 151 13 0.277 1 152 14 0.347 1 153 17 0.024 1 154 5 0.179 1 155 11 0.166 1 156 5 0.000 1 157 12 -0.014 1 158 39 0.328 1 159 17 0.061 1 160 15 0.491 1 161 18 0.059 1 162 14 0.388 1 163 7 0.189 1 164 8 0.000 1 165 19 0.344 1 166 16 0.000 1 167 15 -0.129 1 168 24 0.000 1 169 34 0.214 1 170 7 0.000 1 171 27 0.241 1 172 16 0.125 1 173 29 0.093 1 174 22 0.426 1 175 44 0.030 1 176 36 -0.067 1 177 22 -0.085 1 178 19 0.619 1 179 33 0.104 1 180 12 0.000 1 181 6 0.000 1 182 16 0.300 1 183 40 0.032 1 184 15 0.194 1 185 4 0.000 2 1 2 0.000 2 2 5 0.000 2 3 16 -0.188 2 4 23 -0.156 2 5 16 -0.188 2 6 12 0.173 2 7 22 -0.128 2 8 7 -0.302 2 9 17 0.121 2 10 26 0.559 2 11 35 0.355 2 12 7 0.302 2 13 32 0.318 2 14 34 0.086 2 15 21 0.218 2 16 13 0.111 2 17 42 -0.085 2 18 7 0.000 2 19 50 0.404 2 20 11 -0.030 2 21 26 0.029 2 22 24 0.245 2 23 40 0.245 2 24 18 0.000 2 25 41 0.472 2 26 9 0.027 2 27 43 -0.380 2 28 9 0.167 2 29 23 0.031 2 30 10 0.111 2 31 38 0.008 2 32 14 0.000 2 33 18 0.000 2 34 17 0.000 2 35 12 0.043 2 36 75 0.237 2 37 43 0.244 2 38 17 0.255 2 39 10 -0.079 2 40 38 0.105 2 41 6 0.551 2 42 18 0.071 2 43 14 0.167 2 44 12 0.289 2 45 34 -0.026 2 46 15 0.090 2 47 6 0.367 2 48 14 0.267 2 49 18 -0.247 2 50 48 0.707 2 51 10 0.000 2 52 11 0.226 2 53 12 -0.231 2 54 8 0.177 2 55 9 0.517 2 56 16 0.375 2 57 4 0.400 2 58 10 0.000 2 59 12 0.217 2 60 11 0.407 2 61 10 -0.142 2 62 15 0.478 2 63 22 -0.048 2 64 17 -0.049 2 65 12 -0.794 2 66 6 -0.102 2 67 4 -0.250 2 68 16 -0.250 2 69 10 0.000 2 70 20 0.101 2 71 33 -0.313 2 72 9 0.000 2 73 38 0.300 2 74 16 0.250 2 75 4 -0.250 2 76 35 0.127 2 77 4 -0.250 2 78 31 0.099 2 79 4 0.000 2 80 8 0.371 2 81 4 0.000 2 82 16 0.400 2 83 37 -0.033 2 84 23 0.000 2 85 15 0.000 2 86 14 0.294 2 87 26 0.000 2 88 8 0.265 2 89 17 0.194 2 90 9 0.067 2 91 18 0.118 2 92 37 0.205 2 93 15 0.000 2 94 23 0.000 2 95 28 -0.180 2 96 34 0.718 2 97 14 0.000 2 98 18 -0.141 2 99 19 0.184 2 100 26 0.147 2 101 35 0.355 2 102 19 -0.321 2 103 20 -0.369 2 104 22 -0.205 2 105 43 -0.290 2 106 23 -0.646 2 107 11 0.151 2 108 41 0.226 2 109 29 0.111 2 110 52 -0.125 2 111 30 0.256 2 112 11 0.090 2 113 20 -0.056 2 114 34 0.475 2 115 11 -0.121 2 116 7 -0.189 2 117 31 -0.180 2 118 42 0.548 2 119 5 0.000 2 120 41 0.372 2 121 48 0.217 2 122 13 0.485 2 123 52 0.582 2 124 13 0.607 2 125 15 0.129 2 126 7 -0.189 2 127 8 0.088 2 128 26 0.343 2 129 23 -0.021 2 130 9 0.033 2 131 8 0.177 3 1 4 0.000 3 2 3 0.289 3 3 6 -0.204 3 4 24 -0.704 3 5 10 -0.791 3 6 39 0.192 3 7 35 -0.279 3 8 11 0.407 3 9 38 0.260 3 10 3 0.000 3 11 15 0.194 3 12 21 0.262 3 13 50 0.219 3 14 6 0.163 3 15 48 -0.051 3 16 4 0.125 3 17 5 0.000 3 18 13 0.277 3 19 26 0.824 3 20 14 0.134 3 21 48 -0.260 3 22 15 0.000 3 23 28 -0.104 3 24 21 -0.098 3 25 23 0.349 3 26 4 0.000 3 27 55 0.034 3 28 8 0.530 3 29 20 0.224 3 30 18 -0.795 3 31 13 0.000 3 32 113 0.461 3 33 17 -0.121 3 34 15 0.349 3 35 14 0.000 3 36 37 0.041 3 37 13 0.839 3 38 13 0.139 3 39 21 0.055 3 40 12 0.231 3 41 11 0.814 3 42 12 -0.029 3 43 11 0.030 3 44 29 0.474 3 45 29 -0.078 3 46 12 0.260 3 47 17 0.218 3 48 15 -0.181 3 49 34 0.223 3 50 18 0.118 3 51 23 0.083 3 52 27 -0.500 3 53 20 -0.447 3 54 27 0.250 3 55 10 0.158 3 56 11 -0.404 3 57 29 -0.046 3 58 25 0.040 3 59 18 0.318 3 60 59 0.358 3 61 28 0.028 3 62 5 0.000 3 63 17 0.182 3 64 25 0.420 3 65 10 0.079 3 66 11 -0.965 3 67 18 0.236 3 68 29 0.251 3 69 20 -0.123 3 70 22 0.320 3 71 12 0.375 3 72 23 0.261 3 73 12 0.000 3 74 13 -0.180 3 75 33 -0.566 3 76 13 -0.555 3 77 13 0.000 3 78 26 0.078 3 79 13 0.000 3 80 14 0.067 3 81 29 0.046 3 82 23 0.188 3 83 4 0.300 3 84 19 0.092 3 85 20 0.089 3 86 21 0.109 3 87 29 -0.390 3 88 7 -0.151 3 89 25 0.150 3 90 17 0.303 3 91 18 0.306 3 92 19 0.241 3 93 20 0.201 3 94 10 0.000 3 95 23 -0.188 3 96 24 0.082 3 97 11 -0.121 3 98 15 0.000 3 99 10 -0.285 3 100 16 0.125 3 101 45 0.447 3 102 7 0.000 3 103 17 0.206 3 104 20 0.531 3 105 12 0.173 3 106 12 0.000 3 107 11 0.226 3 108 43 0.221 3 109 19 -0.206 3 110 28 0.227 3 111 26 0.029 3 112 25 0.160 3 113 9 -0.117 3 114 12 0.000 3 115 7 0.189 3 116 3 0.462 3 117 13 -0.555 3 118 38 -0.032 3 119 33 -0.374 3 120 12 -0.217 3 121 11 0.377 3 122 29 0.511 3 123 33 0.174 3 124 5 -0.112 3 125 9 0.083 3 126 10 0.000 3 127 19 0.103 3 128 23 0.302 3 129 16 -0.125 3 130 16 0.000 3 131 10 0.000 3 132 9 -0.100 3 133 19 0.057 3 134 15 -0.161 3 135 11 0.226 3 136 30 -0.128 3 137 8 0.530 3 138 23 0.480 3 139 28 -0.333 3 140 41 0.078 3 141 12 0.260 3 142 19 0.258 3 143 11 0.226 3 144 13 0.347 3 145 7 0.189 3 146 24 -0.082 3 147 19 0.379 3 148 28 0.151 3 149 18 0.997 3 150 29 -0.416 3 151 21 -0.371 3 152 22 0.341 3 153 16 -0.100 3 154 26 0.353 3 155 19 0.578 3 156 15 0.000 3 157 24 -0.153 3 158 9 0.027 3 159 10 0.000 3 160 14 -0.160 3 161 32 -0.071 3 162 14 -0.274 3 163 24 -0.122 3 164 6 0.000 3 165 12 -0.289 3 166 16 0.000 3 167 1 0.000 3 168 16 0.150 3 169 26 0.392 3 170 10 0.126 3 171 23 0.209 3 172 8 0.460 3 173 4 0.000 3 174 27 0.308 3 175 3 0.577 macron.senti%&gt;% group_by(element_id) %&gt;% ggplot(aes(x=sentence_id,y=sentiment, col = element_id )) + geom_line() + facet_wrap(~element_id) + ggtitle(&quot;Macron: Evolution of the sentiments \\nwithin speeches using valence shifters&quot;) + ylab(&quot;Document&quot;) + xlab(&quot;Sentences in speeches&quot;) Figure 4.4: Evolution of the sentiments within speeches using valence shifters "],
["similarities.html", "Chapter 5 Similarities 5.1 Boris 5.2 Macron", " Chapter 5 Similarities The aim of this part of the project is to compute the similarities and dissimilarities between the different doscuments for both Johnson and Macron. We will use here the previously cleaned and tokenised corpuses and the two TF-IDF matrices computed when performing the exploratory data analysis. The three metrics used are the following; the Jaccard Similarity (similarity measure) 5.1, the Cosine Similarity (similarity measure) 5.2 and the Euclidean Distance 5.3 (dissimilarity measure, bounded by the largest distance that is present in the corpus, can therefore be rescaled to a similarity measure between 0 and 1, 1 being the largest distance in the corpus). In order to get a better visualisation of the three metrics, we used a heatmap representation (similarity = 0 –&gt; yellow and similarity = 1 –&gt; red). Actually, when looking at the various heatmaps drawn when running the code, all those similarity measures show the same results, there is not any large similarity between the different documents for Boris Johnson. The only cases on the heatmap that are red are the ones that are on the diagonal, which corresponds to the similarity of a given document and itself, which is equal to 1. 5.1 Boris ## Jaccard Similarity boris.jac &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity boris.cos &lt;- textstat_simil(corpus_boris.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance boris.euc &lt;- textstat_dist(corpus_boris.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix boris.jac.mat &lt;- melt(as.matrix(boris.jac)) ggplot(data=boris.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() Figure 5.1: Jaccard Similarity - Boris Johnson ## Cosine Matrix boris.cos.mat &lt;- melt(as.matrix(boris.cos)) ggplot(data=boris.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() Figure 5.2: Cosine Similarity - Boris Johnson ## Euclidean Matrix boris.euc.mat &lt;- melt(as.matrix(boris.euc)) M &lt;- max(boris.euc.mat$value) boris.euc.mat$value.std &lt;- (M-boris.euc.mat$value)/M ggplot(data=boris.euc.mat, aes(x=Var1, y=Var2, fill=boris.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() Figure 5.3: Euclidean Distance - Boris Johnson We then used two different clustering methods, hierarchical clustering (dendrogram) 5.4 and partitioning (K-means method) ??. We see that the results are quite similar. When looking at the 10 most common words per cluster, there are some words that appear when using the first method and the second one. ## Clustering ## Jaccard Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.jac)) plot(boris.hc) ## Cosine Method boris.hc &lt;- hclust(dist(boris.euc)) boris.hc &lt;- hclust(dist(1 - boris.cos)) plot(boris.hc) ## Dendrogram = Hierarchical Clustering boris.clust &lt;- cutree(boris.hc, k=3) boris.clust #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 1 2 2 2 3 3 3 Figure 5.4: Dendrogram - Hierarchical Clustering ## K-means Method = Partitionning boris.km &lt;- kmeans(corpus_boris.tfidf, centers=3) boris.km$cluster #&gt; text1 text2 text3 text4 text5 text6 text7 #&gt; 1 1 2 3 1 1 1 ### Extracting the 10 most used words - Dendrogram data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.clust==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.clust==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.clust==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 outbreak school already chris child see four mass progress manage parent robert tackle period jenrick minister public behind phase ensure thousand thing important bite patrick dangerous virus delay london huge ### Extracting the 10 most used words - K-Means data.frame( clust1 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==1,],2,sum), decreasing = TRUE)[1:10]), clust2 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==2,],2,sum), decreasing = TRUE)[1:10]), clust3 = names(sort(apply(corpus_boris.tfidf[boris.km$cluster==3,],2,sum), decreasing = TRUE)[1:10]) ) clust1 clust2 clust3 outbreak mass school already london child see contact parent chris ensure pupil dangerous gathering teacher progress fight already robert without fightback jenrick stop update virus non-essential judgment public rather downward When computing document similarities for Macron, we also observe that the only elements on the heatmap that are represented by the red colour are situated on the digonal. However, we can notice a slight difference here compared to Johnson. We do observe that there is a small similarity between document 1 and document 2 for Macron. This is to say that he used the same tokens both in the first and the second document. 5.2 Macron ## Jaccard Similarity macron.jac &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;jaccard&quot;, margin = &quot;documents&quot;) ## Cosine Similarity macron.cos &lt;- textstat_simil(corpus_macron.tfidf, method = &quot;cosine&quot;, margin = &quot;documents&quot;) ## Euclidean Distance macron.euc &lt;- textstat_dist(corpus_macron.tfidf, method = &quot;euclidean&quot;, margin = &quot;documents&quot;) ## Jaccard Matrix macron.jac.mat &lt;- melt(as.matrix(macron.jac)) ggplot(data=macron.jac.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint =0.5, limit=c(0,1), name=&quot;Jaccard&quot;) + geom_tile() Figure 5.5: Jaccard Similarity - Emmanuel Macron ## Cosine Matrix macron.cos.mat &lt;- melt(as.matrix(macron.cos)) ggplot(data=macron.cos.mat, aes(x=Var1, y=Var2, fill=value)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1), name=&quot;Cosine&quot;) + geom_tile() Figure 5.6: Cosine Similarity - Emmanuel Macron ## Euclidean Matrix macron.euc.mat &lt;- melt(as.matrix(macron.euc)) M &lt;- max(macron.euc.mat$value) macron.euc.mat$value.std &lt;- (M-macron.euc.mat$value)/M ggplot(data=macron.euc.mat, aes(x=Var1, y=Var2, fill=macron.euc.mat$value.std)) + scale_fill_gradient2(low=&quot;yellow&quot;, high=&quot;red&quot;, mid=&quot;orange&quot;, midpoint=0.5, limit=c(0,1),name =&quot;Euclidean&quot;) + geom_tile() Figure 5.7: Euclidean Distance - Emmanuel Macron "],
["topic-modelling.html", "Chapter 6 Topic Modelling 6.1 Boris Johnson 6.2 Macron", " Chapter 6 Topic Modelling In this chapter, we analzye the topics of the speechs of Boris Jonhson and Macron using : LSA(Latent Semantic analysis). The core idea is to take a matrix of what we have — documents and terms — and decompose it into a separate document-topic matrix and a topic-term matrix. LDA(Latent Dirichlet Allocation).It uses dirichlet priors for the document-topic and word-topic distributions, lending itself to better generalization. And then we will combine the two dataset and do the same analysis. 6.1 Boris Johnson 6.1.1 LSA First, we make the DTM matrix. We are goin to use 3 dimensions, it means 3 differents topics. bmod&lt;-textmodel_lsa(corpus_boris.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition. In the firs table, each components measures the link between the document and the topic.6.1 In the second table, each component measure the link between the document and the term. 6.2 LSA is typical a reduction technique. Instead of have N documents or M term, it is represented by K documents. lsa_docs_boris&lt;-head(bmod$docs) lsa_docs_boris&lt;-data.frame(lsa_docs_boris) lsa_docs_boris%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.1: Link between document and topic X1 X2 X3 text1 -0.175 0.053 -0.148 text2 -0.519 0.052 -0.813 text3 -0.358 -0.737 0.101 text4 -0.525 0.626 0.342 text5 -0.299 -0.210 0.314 text6 -0.356 -0.121 0.226 lsa_features_boris&lt;-head(bmod$features) lsa_features_boris&lt;-data.frame(lsa_features_boris) lsa_features_boris%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.2: Link between document and terms X1 X2 X3 morning -0.002 0.002 -0.005 government’s -0.011 0.005 -0.039 cobr -0.007 -0.023 -0.002 emergency -0.030 -0.050 -0.011 committee -0.009 0.004 -0.034 coronavirus -0.047 0.015 -0.026 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. As we observe in the figure 6.1, the dimension 1 is negatively correlated with the document lenght. Therefore the dimension 1 bring us not a lot of informations that we have already. ns&lt;-apply(corpus_boris.dfm,1,sum) plot(ns~bmod$docs [,1]) Figure 6.1: First dimension of the LSA - Boris Johnson We clearly observe that the dimension 1 is negatively correlated with the document lenght. Now in order to make the link between the topics and the documents and the topics with term, we use biplot. We represent the dimension 2 and 3, beacause often the first component bring often little information. Reminders: The seven speech are class by chronological order: - 09 March (text1) - 12 March (text2) - 16 March (text3) - 18 March (text5) - 19 March (text6) - 20 March (text7) - 22 March (text8) It is notable that the texts that are brought together over time are grouped together.And that the first speeches are the opposite of the last ones as we observe in the figure 6.2 biplot(y=bmod$docs[,2:3],x=bmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) Figure 6.2: Biplot - Boris Johnson We repeat the same analysis with TF-IDF. The influence of small weighted-frequent tokens is reduced. In the firs table 6.3, each components measures the link between the document and the topic. In the second table 6.4 each component measure the link between the document and the term. bmod_2&lt;- textmodel_lsa(corpus_boris.tfidf, nd=3) lsa_docs_boris_2&lt;-head(bmod_2$docs) lsa_docs_boris_2&lt;-data.frame(lsa_docs_boris_2) lsa_docs_boris_2%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.3: Link between document and topic X1 X2 X3 text1 -0.147 -0.063 -0.318 text2 -0.380 -0.187 -0.752 text3 -0.444 -0.750 0.463 text4 -0.735 0.615 0.256 text5 -0.180 -0.132 -0.173 text6 -0.200 -0.040 -0.153 lsa_features_boris_2&lt;-head(bmod_2$features) lsa_features_boris_2&lt;-data.frame(lsa_features_boris_2) lsa_features_boris_2%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.4: Link between document and terms X1 X2 X3 morning -0.008 -0.004 -0.025 government’s -0.024 -0.014 -0.070 cobr -0.021 -0.037 0.007 emergency -0.042 -0.038 0.013 committee -0.019 -0.011 -0.054 coronavirus -0.016 -0.001 -0.011 6.1.2 LDA We now turn to the LDA. For illustration, we will make K=3 topis. K&lt;-3 corpus_boris.dtm&lt;- convert(corpus_boris.dfm, to=&quot;topicmodels&quot;) lda_boris&lt;- LDA(corpus_boris.dtm ,k=K) In the table 6.5, it is the list of the six most frequent term in each topic terms&lt;-terms(lda_boris,6) terms&lt;-data.frame(terms) terms %&gt;% kable(caption=&quot;List of the terms present in each topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.5: List of the terms present in each topic Topic.1 Topic.2 Topic.3 will will will much now people school can want disease go go go say measure now disease can In the table 6.6, you can observe which text is related to which topic. ## To see the topics related to each document topics&lt;-(topics(lda_boris,1)) topics&lt;-data.frame(topics) topics%&gt;% kable(caption=&quot;Topics&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.6: Topics topics text1 2 text2 1 text3 2 text4 1 text5 2 text6 3 text7 3 We now build the bar plot to inspect the per-topic-per-word probabilities (beta’s). We take the 10 top terms and rearrange the beta per topic according to this order. We observe in the figure 6.3 that the topic 1 that the 2 first terms are “now” and “go”. We can image that the topic 1 is more focous on the urgency and on the keep going. beta.td.boris&lt;-tidy(lda_boris,matrix=&quot;beta&quot;) beta.top.term.boris&lt;-beta.td.boris %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.boris %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + geom_col(show.legend = FALSE)+ facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() Figure 6.3: Beta - Boris Johnson Now, we compute the gamma, it shows the proportion of each topic within each document, as you can observe in the figure 6.4. We note that each document represented a text.The texts are very distinctive. gamma.td.boris&lt;- tidy(lda_boris,matrix=&quot;gamma&quot;) gamma.td.boris %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() Figure 6.4: Gamma - Boris Johnson 6.2 Macron 6.2.1 LSA mmod&lt;-textmodel_lsa(corpus_macron.dfm,nd=3) To inspect the results, we can extract the matrices involved in the LSA decomposition 6.7 6.8 lsa_docs_macron&lt;-head(mmod$docs) lsa_docs_macron&lt;-data.frame(lsa_docs_macron) lsa_docs_macron%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.7: Link between document and topic X1 X2 X3 text1 -0.662 0.495 0.563 text2 -0.426 0.370 -0.825 text3 -0.617 -0.786 -0.034 lsa_features_macron&lt;-head(mmod$features) lsa_features_macron&lt;-data.frame(lsa_features_macron) lsa_features_macron%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.8: Link between document and terms X1 X2 X3 check -0.004 0.010 0.013 delivery -0.011 -0.021 0.012 france -0.069 0.031 0.017 dear -0.054 0.018 -0.021 past -0.025 -0.010 0.018 country -0.084 -0.077 -0.065 Often the first dimension in LSA is associated with the document lenght. To see if it is true, we build a scatter-plot between the document lengt and Dimension 1. ns_macron&lt;-apply(corpus_macron.dfm,1,sum) plot(ns_macron~mmod$docs [,1]) Figure 6.5: First dimension of the LSA - Emmanuel Macron We clearly observe that the dimension 1 is negatively correlated with the document lenght, as we observe in the figure 6.5. Now in order to make the link between the topics and the documents and the topics with term, we use biplot 6.6. biplot(y=mmod$docs[,2:3],x=mmod$features[,2:3], col=c(&quot;grey&quot;,&quot;red&quot;), xlab = &quot;Dimension 2&quot;, ylab=&quot;Dimension 3&quot;) Figure 6.6: Biplot - Emmanuel Macron We repeat the same analysis with TF-IDF 6.9 6.10 mmod_2&lt;- textmodel_lsa(corpus_macron.tfidf, nd=3) lsa_docs_macron_2&lt;-head(mmod_2$docs) lsa_docs_macron_2&lt;-data.frame(lsa_docs_macron_2) lsa_docs_macron_2%&gt;% kable(caption=&quot; Link between document and topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.9: Link between document and topic X1 X2 X3 text1 -0.318 0.930 0.187 text2 -0.115 0.158 -0.981 text3 -0.941 -0.333 0.056 lsa_features_macron_2&lt;-head(mmod_2$features) lsa_features_macron_2&lt;-data.frame(lsa_features_macron_2) lsa_features_macron_2%&gt;% kable(caption=&quot; Link between document and terms&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.10: Link between document and terms X1 X2 X3 check -0.010 0.035 0.008 delivery -0.026 0.004 0.005 france 0.000 0.000 0.000 dear 0.000 0.000 0.000 past 0.000 0.000 0.000 country 0.000 0.000 0.000 6.2.2 LDA We now turn to the LDA. For illustration, we will make K=5 topis. K&lt;-5 corpus_macron.dtm&lt;- convert(corpus_macron.dfm, to=&quot;topicmodels&quot;) lda_macron&lt;- LDA(corpus_macron.dtm ,k=K) Top terms per topic and top topic per document can be easily obtained. Belo, the six top terms and the top topic are extracted. 6.11 6.12 terms_macron&lt;-terms(lda_macron,6) terms_macron&lt;-data.frame(terms_macron) terms_macron %&gt;% kable(caption=&quot;List of the terms present in each topic&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.11: List of the terms present in each topic Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 will will will will will work also much virus much even must can also also hospital much come us health must day work week time protect us many government take topics_macron&lt;-(topics(lda_macron,1)) topics_macron&lt;-data.frame(topics_macron) topics_macron%&gt;% kable(caption=&quot;Topics&quot;) %&gt;% kable_styling(bootstrap_options = &quot;striped&quot;) Table 6.12: Topics topics_macron text1 5 text2 2 text3 4 We now build the bar plot to inspect the per-topic-per-word probabilities (beta’s). We take the 10 top terms and rearrange the beta per topic according to this order. 6.7 beta.td.macron&lt;-tidy(lda_macron,matrix=&quot;beta&quot;) beta.top.term.macron&lt;-beta.td.macron %&gt;% group_by(topic) %&gt;% top_n(10, beta) %&gt;% ungroup() %&gt;% arrange(topic, -beta) beta.top.term.macron %&gt;% mutate(term=reorder_within(term, beta, topic)) %&gt;% ggplot(aes(term,beta,fill=factor(topic))) + geom_col(show.legend = FALSE)+ facet_wrap(~topic, scales = &quot;free&quot;) + coord_flip()+ scale_x_reordered() Figure 6.7: Beta - Emmanuel Macron Now, we compute the gamma, it shows the proportion of each topic within each document. We note that text 1 is related to the topic 1 4 and 5. Th test 3 is related to the topic 2. And the text 2 is related to the topic 3. 6.8 gamma.td.macron&lt;- tidy(lda_macron,matrix=&quot;gamma&quot;) gamma.td.macron %&gt;% ggplot(aes(document,gamma,fill=factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~topic,scales = &quot;free&quot;)+ coord_flip()+ scale_x_reordered() Figure 6.8: Gamma - Emmanuel Macron "],
["word-embedding.html", "Chapter 7 Word Embedding 7.1 Boris Johnson 7.2 Macron 7.3 Comparison", " Chapter 7 Word Embedding A word embedding is a learned representation for text where words that have the same meaning have a similar representation. This is a method of learning a representation of words used in particular in automatic language processing. The term should rather be rendered by vectorisation of words in order to correspond more neatly to this method. 7.1 Boris Johnson Here, we compute the co-occurence matrix. We use the fcm function from quanteda. We use a window lenght 5. speech.coo.boris&lt;-fcm(corpus_boris,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.boris&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.boris&lt;-speech.glove.boris$fit_transform(speech.coo.boris) #&gt; INFO [23:30:31.553] epoch 1, loss 0.0335 #&gt; INFO [23:30:31.623] epoch 2, loss 0.0243 #&gt; INFO [23:30:31.648] epoch 3, loss 0.0223 #&gt; INFO [23:30:31.652] epoch 4, loss 0.0212 #&gt; INFO [23:30:31.656] epoch 5, loss 0.0205 #&gt; INFO [23:30:31.659] epoch 6, loss 0.0198 #&gt; INFO [23:30:31.663] epoch 7, loss 0.0191 #&gt; INFO [23:30:31.667] epoch 8, loss 0.0184 #&gt; INFO [23:30:31.670] epoch 9, loss 0.0178 #&gt; INFO [23:30:31.675] epoch 10, loss 0.0172 For illustration purpose, we now plot the 50 most used terms as you can observe in the figure 7.1. More the words are close, more they are similar. Two word are similar if they are often use in the same context. n.w.boris&lt;-apply(corpus_boris.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.boris,decreasing = TRUE)[1:50] plot(speech.weC.boris[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.boris[index,],labels = rownames(speech.weC.boris[index,])) Figure 7.1: The 50 most used terms In the figure 7.2 speech.dtm &lt;-corpus_boris.dfm speech.rwmd.model.boris&lt;-RelaxedWordMoversDistance$new(corpus_boris.dfm,speech.weC.boris) speech.rwms.boris&lt;-speech.rwmd.model.boris$sim2(corpus_boris.dfm) speech.rwmd.boris&lt;-speech.rwmd.model.boris$dist2(corpus_boris.dfm) speech.hc.boris&lt;-hclust(as.dist(speech.rwmd.boris)) plot(speech.hc.boris,cex=0.8) Figure 7.2: Cluster Dendogram We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.boris&lt;- cutree(speech.hc.boris,k=4) corpus_boris.dfm[speech.cl.boris==1,] #&gt; Document-feature matrix of: 1 document, 797 features (78.2% sparse). #&gt; features #&gt; docs morning government&#39;s cobr emergency committee coronavirus #&gt; text1 1 2 1 1 1 3 #&gt; features #&gt; docs outbreak first scotland minister #&gt; text1 5 4 1 3 #&gt; [ reached max_nfeat ... 787 more features ] 7.2 Macron speech.coo.macron&lt;-fcm(corpus_macron,context=&quot;window&quot;,window=5, tri=FALSE) p&lt;-2 #word embedding dimension speech.glove.macron&lt;-GlobalVectors$new(rank = p,x_max = 10) #xmas is a neede technical option speech.weC.macron&lt;-speech.glove.macron$fit_transform(speech.coo.macron) #&gt; INFO [23:30:32.962] epoch 1, loss 0.0250 #&gt; INFO [23:30:32.970] epoch 2, loss 0.0178 #&gt; INFO [23:30:32.980] epoch 3, loss 0.0161 #&gt; INFO [23:30:32.995] epoch 4, loss 0.0149 #&gt; INFO [23:30:33.007] epoch 5, loss 0.0141 #&gt; INFO [23:30:33.017] epoch 6, loss 0.0133 #&gt; INFO [23:30:33.027] epoch 7, loss 0.0126 #&gt; INFO [23:30:33.036] epoch 8, loss 0.0120 #&gt; INFO [23:30:33.045] epoch 9, loss 0.0116 #&gt; INFO [23:30:33.061] epoch 10, loss 0.0112 For illustration purpose, we now plot the 50 most used terms 7.3 n.w.macron&lt;-apply(corpus_macron.dfm,2,sum) #compute the number of times each term is used index&lt;-order(n.w.macron,decreasing = TRUE)[1:50] plot(speech.weC.macron[index,],type = &quot;n&quot;,xlab = &quot;Dimension 1&quot;, ylab = &quot;Dimendion 2&quot;) text(x=speech.weC.macron[index,],labels = rownames(speech.weC.macron[index,])) Figure 7.3: The 50 most used terms 7.4 speech.dtm.macron &lt;- corpus_macron.dfm speech.rwmd.model.macron&lt;-RelaxedWordMoversDistance$new(corpus_macron.dfm,speech.weC.macron) speech.rwms.macron&lt;-speech.rwmd.model.macron$sim2(corpus_macron.dfm) speech.rwmd.macron&lt;-speech.rwmd.model.macron$dist2(corpus_macron.dfm) speech.hc.macron&lt;-hclust(as.dist(speech.rwmd.macron)) plot(speech.hc.macron,cex=0.8) Figure 7.4: Cluser Dendogram We can observe that there is some coherence within the groups in terms the date of the speech. speech.cl.macron&lt;- cutree(speech.hc.macron,k=2) corpus_macron.dfm[speech.cl.macron==1,] #&gt; Document-feature matrix of: 2 documents, 1,366 features (45.8% sparse). #&gt; features #&gt; docs check delivery france dear past country spread virus #&gt; text1 1 1 9 6 3 6 8 13 #&gt; text3 0 2 6 5 3 12 2 12 #&gt; features #&gt; docs covid-19 several #&gt; text1 4 5 #&gt; text3 1 5 #&gt; [ reached max_nfeat ... 1,356 more features ] 7.3 Comparison "],
["supervised-learning.html", "Chapter 8 Supervised learning 8.1 LSA 8.2 Random forest 8.3 Improving the features", " Chapter 8 Supervised learning In this section, we use a supervised learner to develop a classifier of the Politicans’ speeches. The aim of this section is to have a classification model able to correctly attribute a random speech to Boris Johnson or Emmanuel Macron. To do so, we first combine the dataframe of Boris Johnson with the dataframe of Emmanuel Macron. Since those dataframes differ in number of speechs and in length, we divide the speeches into sentences, which would smooth difference between our two dependent outcome possibilities. ##Boris Johnson boris_2&lt;-as_tibble(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) %&gt;% rename( text=value) author=&quot;Boris Johnson&quot; boris_supervised&lt;- cbind(boris_2, author) boris_2_sentence&lt;-get_sentences(boris_supervised) ##Emmanuel Macron Macron_2&lt;-as_tibble(c(macron12march,macron16march,macron13april)) %&gt;% rename( text = value) author=&quot;Macron&quot; macron_supervised&lt;- cbind(Macron_2, author) macron_2_sentence&lt;-get_sentences(macron_supervised) ##Combine the 2 dataframes combine &lt;- rbind(boris_2_sentence, macron_2_sentence) ## Tokenization combine_corpus&lt;-corpus(combine) combine_tokens&lt;- tokens(combine_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE) ##combi Lemmatization combine_tokens &lt;- tokens_replace(combine_tokens, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma) ## Cleaning combine_tokens = combine_tokens %&gt;% tokens_tolower() %&gt;% tokens_remove(stopwords(&quot;english&quot;)) y&lt;-factor(docvars(combine_tokens,&quot;author&quot;)) Then, we build the featues. To this aim, we first compute the DTM matrix. combine.dfm&lt;-dfm(combine_tokens) combine.dfm #&gt; Document-feature matrix of: 771 documents, 1,691 features (99.4% sparse) and 3 docvars. #&gt; features #&gt; docs morning meeting government&#39;s cobr emergency committee #&gt; text1 0 0 0 0 0 0 #&gt; text2 1 0 1 1 1 1 #&gt; text3 0 0 0 0 0 0 #&gt; text4 0 0 0 0 0 0 #&gt; text5 0 0 0 0 0 0 #&gt; text6 0 0 0 0 0 0 #&gt; features #&gt; docs coronavirus outbreak first scotland #&gt; text1 0 0 0 0 #&gt; text2 1 1 0 0 #&gt; text3 0 0 3 1 #&gt; text4 0 0 0 0 #&gt; text5 0 0 0 0 #&gt; text6 1 0 0 0 #&gt; [ reached max_ndoc ... 765 more documents, reached max_nfeat ... 1,681 more features ] 8.1 LSA Because of the huge number of tokens, the feature matrix obtained may be too big to train a model in a reasonable amount of time. We thus apply a reduction dimension technque in order to obtain less features while keeping the relevant information. LSA is the perfect technique to achieve this. We target 30 dimensions (30 subjects) combine_corpus.dfm &lt;- dfm(combine_corpus) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=30) 8.2 Random forest After preparing our data to be used by the learner, we decide to run a random forest, which is a robust method to find the best classification model by computing a large set of classification tree to obtain the most pertinent values of classification criterias. After building our model, we create a training and a test sets. In this simple context, in order to illustrate the concepts without too long computation times, we will limit ourselves to just one training set and one test set by applying the Pareto law 80-20. set.seed(782) df&lt;-data.frame(Class=y, x=cmod$docs) index.tr&lt;-sample(size = round(0.8*length(y)),x=c(1:length(y)),replace = FALSE) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(Class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) In order to see the prediction quality of the model, we call the confusionMatrix function in the caret package: confusionMatrix&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$Class) confusionMatrix #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Boris Johnson Macron #&gt; Boris Johnson 23 6 #&gt; Macron 29 96 #&gt; #&gt; Accuracy : 0.773 #&gt; 95% CI : (0.698, 0.836) #&gt; No Information Rate : 0.662 #&gt; P-Value [Acc &gt; NIR] : 0.00191 #&gt; #&gt; Kappa : 0.43 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00020 #&gt; #&gt; Sensitivity : 0.442 #&gt; Specificity : 0.941 #&gt; Pos Pred Value : 0.793 #&gt; Neg Pred Value : 0.768 #&gt; Prevalence : 0.338 #&gt; Detection Rate : 0.149 #&gt; Detection Prevalence : 0.188 #&gt; Balanced Accuracy : 0.692 #&gt; #&gt; &#39;Positive&#39; Class : Boris Johnson #&gt; The model has an accuracy of 80.5%, which is being satisfying: a model is said to be satisfactory enough when reaching 80% of accuracy. Boris Johnson being the positive class, the model overestimates the sentences belonging to Emmanuel Macrons’s speeches (specificity: 95.1%), while greatly underestimate the sentences of Boris Jonhson (sensitivity: 51.9%). The prediction quality is not balanced between the two classes and the model is not able to classify correctly. One of the solutions would be to attribute weights to the sentences in order to mitigate the unaccurate results. 8.3 Improving the features 8.1 nd.vec&lt;-c(2,5,25,50,100,500,1000) acc.vec&lt;-numeric(length(nd.vec)) for (j in 1:length(nd.vec)) { cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=nd.vec[j]) df&lt;-data.frame(class=y,x=cmod$docs) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) acc.vec[j]&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$class)$overall[1] } acc.vec #&gt; [1] 0.740 0.779 0.779 0.805 0.786 0.727 0.708 plot(acc.vec~nd.vec,type=&quot;b&quot;) Figure 8.1: Accuracy We can see that 100 is the best choice among the ones we tried. set.seed(788) combine_corpus.dfm &lt;- dfm(combine_corpus) cmod&lt;-textmodel_lsa(combine_corpus.dfm,nd=100) df&lt;-data.frame(class=y, x=cmod$docs) index.tr&lt;-sample(size = round(0.8*length(y)),x=c(1:length(y)),replace = FALSE) df.tr&lt;-df[index.tr,] df.te&lt;-df[-index.tr,] combine.fit&lt;-ranger(class~., data = df.tr) pred.te&lt;-predict(combine.fit,df.te) confusionmatrix_2&lt;-confusionMatrix(data=pred.te$predictions,reference = df.te$class) confusionmatrix_2 #&gt; Confusion Matrix and Statistics #&gt; #&gt; Reference #&gt; Prediction Boris Johnson Macron #&gt; Boris Johnson 32 4 #&gt; Macron 19 99 #&gt; #&gt; Accuracy : 0.851 #&gt; 95% CI : (0.784, 0.903) #&gt; No Information Rate : 0.669 #&gt; P-Value [Acc &gt; NIR] : 2.57e-07 #&gt; #&gt; Kappa : 0.636 #&gt; #&gt; Mcnemar&#39;s Test P-Value : 0.00351 #&gt; #&gt; Sensitivity : 0.627 #&gt; Specificity : 0.961 #&gt; Pos Pred Value : 0.889 #&gt; Neg Pred Value : 0.839 #&gt; Prevalence : 0.331 #&gt; Detection Rate : 0.208 #&gt; Detection Prevalence : 0.234 #&gt; Balanced Accuracy : 0.794 #&gt; #&gt; &#39;Positive&#39; Class : Boris Johnson #&gt; By re-running the same model but by increasing dimensions rather than weighting the sentences, we observe an increase in accuracy (+3.97%), however the over-/underestimation of the speeches remains. Sensitivity has improved by 18.50% but the learner has a still a poor prediction ability, in spite of the increase of the dimensions.The improvement by weighting the independent variables would be potentially more efficient. We would keep it as being part of further investigations. "],
["conclusion.html", "Chapter 9 Conclusion", " Chapter 9 Conclusion We focused on the beginning of pandemic period by focusing on the speeches of both politicians in March. After having downloaded 3 speeches of Emmanuel Macron and 7 speeches of Boris Johnson, using Xpath, our Exploratory Data Analysis points out specificities among the UK and French supervised. In spite of the shortness of the Boris Johnson’s words, the vocabulary used by the Brititish Price Minister is richer than the allocutions of the French President. The French President speaks more about the future, by using at a high frequency the auxiliary «will», whereas the Prime Minister is more balanced in the words used, linking present and future and integrating social terms like family and friends. The Sentiment Analysis outlines more positive discourses coming from across the Channel, the speech on March 12th being the more positive than the others. The specificity of Emmanuel Macron’s structure is its stability: his allocutions follow an almost similar distribution of the sentiment, and this trend is not affected by the length of his speeches. By using the «nrc», «afinn» and «LSD2015», the qualitative ad quantitative-based results differs, but 3 stages out of the 4 are convergent, fortifying those results. The Similarity analysis was not significant: the results highlight no similarities among speeches of the same president. Some investigations might be needed in order to understand the roots of this outcome. The Word Embedding presents clusters in a seemingly chronological order for Boris Johnson, grouping step by step the 1rst speech to the later, while the respective Macron’s dendogram groups the 1rst and 3rd discourses together, joining ultimately the second one. Topic Modelling’s insights for the texts per politician has no added value: speaking of the same subject and in the same way, the discourses overlap each other by sharing certain similar topics. However, when combining French and British discourses, the Topic Modelling gives a clearer vision on the difference of topics mentioned. We note that the joint analysis leads to better quality results. Finally, after identifying some discriminatory features among speeches, we built a supervised learner to categorize random speeches as belonging to Emmanuel Macron ou Boris Johnson, using a random forest model. It is based on the division of the speeches by sentence and on their tokenization in order to get the sentiment per sentence as independent variable. Our classification model is biased as predicting too much False Negative (sentences erroneously categorized as belonging to Emmanuel Macron). An attempt for improvement by increasing the number of features was not satisfactory, increasing slightly the accuracy while keeping the overestimation of President Macron’s sentences. Further improvements might be operated by weighting the independent variables in order to reduce the bias. To conclude, our basis assumption of similarity of the construction of sentences and of the goal of such discourses might be correct. In addition to the similar subject (pandemic measures), the basis assumption could be a cause of the lukewarm results in Topic Modeling, Word Embedding and Similarity, limiting the potentiality of discriminatory features. The speeches differ greatly in length, vocabulary and sentiments, but those differences are not as strong as needed for building a satisfactory random forest classification learner. The project might be leveled up by adding new speeches (taking a larger period of time) and rethinking our supervised learner to be based on more accurate independent variables, assessed by a statistical analysis and investigation on the p-values. "],
["references.html", "References", " References "]
]
