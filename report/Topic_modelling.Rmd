# Topic Modelling    

For this chapter, we analyzed the different topics used in the speeches. We used :
- the Latent Semantic Analysis, which is going to decompose the DTM Matrix into the document-topic similarity, the topic strength and the term-to-topic similarity
- and the Latent Dirichlet Allocation, that is a typical dimension reduction technique, which uses dirichlet priors for the document-topic and the word-topic distribution
We have first analyzed the speeches of Johnson and Macron separately and we have then combined them together.

## Boris Johnson

### LSA

We first computed the DTM matrix. We used 3 dimensions, which means 3 differents topics.

```{r,warning=FALSE}
bmod<-textmodel_lsa(corpus_boris.dfm,nd=3)
```

To inspect the results, we can extract the matrices involved in the LSA decomposition.
In the first table, each component measures the link between the document and the topic.\@ref(tab:table-docs-boris)
In the second table, each component measures the link between the document and the term. \@ref(tab:table-features-boris)

LSA is typical a reduction technique. Instead of having a M (documents) x N (terms) matrix, we represented it by K topics.

```{r table-docs-boris,warning=FALSE}
lsa_docs_boris<-head(bmod$docs)
lsa_docs_boris<-data.frame(lsa_docs_boris)

lsa_docs_boris%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")
```

```{r table-features-boris,warning=FALSE}
lsa_features_boris<-head(bmod$features)
lsa_features_boris<-data.frame(lsa_features_boris)

lsa_features_boris%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")
```

Often, the first dimension in LSA is associated with the document length. To see if it is true, we build a scatter-plot between the document length and dimension 1.
As we observe in the figure \@ref(fig:lsaboris), dimension 1 is negatively correlated with the document length.

```{r lsaboris,fig.cap="First dimension of the LSA - Boris Johnson"}
ns<-apply(corpus_boris.dfm,1,sum) 
plot(ns~bmod$docs [,1])
```

We clearly observe that dimension 1 is negatively correlated with the document length.

Now in order to make the link between the topics and the documents and the topics with terms, we use a biplot. We represent the dimensions 2 and 3, because the first component often brings little information. 

Reminder:

The seven speeches are ordered by chronological order:
  - 09 March (text1)
  - 12 March (text2)
  - 16 March (text3)
  - 18 March (text5)
  - 19 March (text6)
  - 20 March (text7)
  - 22 March (text8)
  
It is  noticeable that the texts that are closer in time are grouped together. And that the first speeches go in the opposite direction of the one of the last speeches, as we observe in the figure \@ref(fig:biplotboris).

```{r biplotboris,fig.cap="Biplot - Boris Johnson"}
biplot(y=bmod$docs[,2:3],x=bmod$features[,2:3],
       col=c("grey","red"),
       xlab = "Dimension 2",
       ylab="Dimension 3")
```

We repeated the same analysis with TF-IDF. The influence of small weighted-frequent tokens is reduced.

In the first table \@ref(tab:table-docs-boris-2), each component measures the link between the documents and the topics.
In the second table \@ref(tab:table-features-boris-2), each component measures the link between the documents and the terms.


```{r table-docs-boris-2,warning=FALSE}
bmod_2<- textmodel_lsa(corpus_boris.tfidf, nd=3)

lsa_docs_boris_2<-head(bmod_2$docs)
lsa_docs_boris_2<-data.frame(lsa_docs_boris_2)

lsa_docs_boris_2%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")
```

```{r table-features-boris-2,warning=FALSE}

lsa_features_boris_2<-head(bmod_2$features)
lsa_features_boris_2<-data.frame(lsa_features_boris_2)

lsa_features_boris_2%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")
```

### LDA

We then computed the LDA. For illustration, we used K=3 topics. 

```{r,warning=FALSE}
K<-3
corpus_boris.dtm<- convert(corpus_boris.dfm, to="topicmodels")
lda_boris<- LDA(corpus_boris.dtm ,k=K)

```

In the table \@ref(tab:table-term-boris), we can see the list of the six most frequent terms per topic.

```{r table-term-boris,warning=FALSE}
terms<-terms(lda_boris,6)
terms<-data.frame(terms)

terms %>%
  kable(caption="List of the terms present in each topic") %>%
  kable_styling(bootstrap_options = "striped")

```

In the table \@ref(tab:table-topic-boris), we can observe which text is related to which topic.

```{r table-topic-boris,warning=FALSE}
## To see the topics related to each document

topics<-(topics(lda_boris,1))
topics<-data.frame(topics)

topics%>%
  kable(caption="Topics") %>%
  kable_styling(bootstrap_options = "striped")


```

We then built the bar plot to inspect the per-topic-per-word probabilities (beta's). We took the top 10 terms and rearranged the betas per topic according to this order. We observe in the figure \@ref(fig:betaboris) that topic 1 is correlated to the tokens "will", "go" and "want", topic 2 to the token "school" and topic 3 to the tokens "now", "can" and "will".

```{r betaboris,fig.cap="Beta - Boris Johnson"}
beta.td.boris<-tidy(lda_boris,matrix="beta")

beta.top.term.boris<-beta.td.boris %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


beta.top.term.boris %>%
  mutate(term=reorder_within(term, beta, topic)) %>%
  ggplot(aes(term,beta,fill=factor(topic))) +
   geom_col(show.legend = FALSE)+
  facet_wrap(~topic, scales = "free") +
  coord_flip()+
  scale_x_reordered()

```

Now, we calculated the gammas, which show the proportions of each topic within each document, as we can observe in the figure \@ref(fig:gammaboris). We see that the first speeches are related to topic 3 ("now", "can", "will"). This makes sense as those speeches were made in the begining of the pandemic and are linked to a sense of urgency. The speeches in the middle correspond to the second topic and all the measures implented for schools. The three last speeches are represented by the first topic only, and are correlated to the tokens "will", "go" and "want". We can conlude from this that the British Prime Minster wanted his citizens to keep up the efforts.

```{r gammaboris,fig.cap="Gamma - Boris Johnson"}
gamma.td.boris<- tidy(lda_boris,matrix="gamma")


gamma.td.boris %>%
  ggplot(aes(document,gamma,fill=factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~topic,scales = "free")+
  coord_flip()+
  scale_x_reordered()
```

## Macron

### LSA

```{r,warning=FALSE}
mmod<-textmodel_lsa(corpus_macron.dfm,nd=3)

```

To inspect the results, we can extract the matrices involved in the LSA decomposition.

\@ref(tab:table-document-macron)
\@ref(tab:table-features-macron)

```{r table-document-macron,warning=FALSE}

lsa_docs_macron<-head(mmod$docs)
lsa_docs_macron<-data.frame(lsa_docs_macron)

lsa_docs_macron%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")

```

```{r table-features-macron,warning=FALSE}

lsa_features_macron<-head(mmod$features)
lsa_features_macron<-data.frame(lsa_features_macron)

lsa_features_macron%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")


```

Often the first dimension in LSA is associated with the document length. To see if it is true, we build a scatter-plot between the document length and dimension 1.

```{r lsamacron,fig.cap="First dimension of the LSA - Emmanuel Macron"}
ns_macron<-apply(corpus_macron.dfm,1,sum) 
plot(ns_macron~mmod$docs [,1])
```

We clearly observe that the dimension 1 is negatively correlated with the document length.

Now in order to make the link between the topics and the documents and the topics with term, we used a biplot, composed by the second dimension on the x-axis and the third dimension on the y-axis.

```{r biplotmacron,fig.cap="Biplot - Emmanuel Macron"}
biplot(y=mmod$docs[,2:3],x=mmod$features[,2:3],
       col=c("grey","red"),
       xlab = "Dimension 2",
       ylab="Dimension 3")
```

We repeated the same analysis with TF-IDF, \@ref(tab:table-document-macron-2),\@ref(tab:table-term-macron-2)

```{r table-document-macron-2}
mmod_2<- textmodel_lsa(corpus_macron.tfidf, nd=3)

lsa_docs_macron_2<-head(mmod_2$docs)
lsa_docs_macron_2<-data.frame(lsa_docs_macron_2)

lsa_docs_macron_2%>%
  kable(caption=" Link between document and topic") %>%
  kable_styling(bootstrap_options = "striped")
```

```{r table-term-macron-2}
lsa_features_macron_2<-head(mmod_2$features)
lsa_features_macron_2<-data.frame(lsa_features_macron_2)

lsa_features_macron_2%>%
  kable(caption=" Link between document and terms") %>%
  kable_styling(bootstrap_options = "striped")
```

## Combine

We combined the speeches of both authors in the same dataframe, that will have a total of ten speeches.

```{r,warning=FALSE}
##Boris Johnson
boris_2<-as_tibble(c(boris9mars,boris12mars,boris16mars,boris18mars,boris19mars,boris20mars,boris22mars)) %>%
  rename(
  text=value)
author="Boris Johnson"
boris_2<- cbind(boris_2, author)

##Emmanuel Macron
Macron_2<-as_tibble(c(macron12march,macron16march,macron13april)) %>% 
  rename(
    text = value)

author="Macron"
macron_2<- cbind(Macron_2, author)

##Combine the 2 dataframes
combine <- rbind(boris_2, macron_2)


## Tokenization
combine_corpus<-corpus(combine)
combine_tokens<- tokens(combine_corpus, remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE)

##combi Lemmatization

combine_tokens <- tokens_replace(combine_tokens, pattern=hash_lemmas$token, replacement = hash_lemmas$lemma)

## Cleaning
combine_tokens = combine_tokens %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("english"))
```

### LSA

```{r,warning=FALSE}
combine_corpus.dfm <- dfm(combine_tokens)
cmod<-textmodel_lsa(combine_corpus.dfm,nd=6)
```

Often the first dimension in the LSA is associated with the document length. To see if it is true, we built a scatter-plot between the document length and dimension 1. We clearly observe that the three last documents are the longest ones and they all correspond to Macron's speeches.

```{r,warning=FALSE}
ns_combine<-apply(combine_corpus.dfm,1,sum) 
plot(ns_combine~cmod$docs [,1])
```

We then decide to proceed to the analysis using the second and third dimensions, as we have observed that the first dimension is negatively correlated to the document length and therefore does not  bring us a lot of information. We see that the speeches of Macron and Johnson are represented by different dimensions. Macron's speeches correspond better to the second dimension (documents 8, 9 and 10), whereas the third dimension is better represented by Johnson. This tells us that without building any further model and just by looking at the biplot corresponding to the second and third dimensions, we can guess whether a new corpus would belong to Johnson or to Macron.

```{r biplot-combine,warning=FALSE}
biplot(y=cmod$docs[,2:3],x=cmod$features[,2:3],
       col=c("grey","red"),
       xlab = "Dimension 2",
       ylab="Dimension 3")
```